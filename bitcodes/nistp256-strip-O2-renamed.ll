; ModuleID = 'nistp256-strip-O2-renamed.bc'
source_filename = "nistp256.c"
target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

%0 = type { i8*, [1 x %1], [1 x %1], [1 x %1], [1 x %1], [1 x %1], [1 x %1], void (%2*, %1*, %1*, %1*, %0*, %1*, %1*, %1*)*, void (%2*, %1*, %1*, %1*, %0*, %1*, %1*, %1*, %1*, %1*, %1*)*, void (%1*, %1*, %1*, %0*, %1*, %1*, %1*, %1*)*, void (%1*, %1*, %1*, %0*, [1 x %1]*, [1 x %1]*, [1 x %1]*, [1 x %1]*, i64)*, %3* (%0*, %1*, %1*, %1*, i64)*, void (%1*, %1*, %1*, %0*, %3*, %1*)*, void (%3*)*, i64 (i32, %1*, %1*)*, i64 (i32, %1*, %1*)* }
%1 = type { i32, i32, i64* }
%2 = type { [1 x %1], [1 x %1], [1 x %1], [1 x %1], [1 x %1], [1 x %1], [1 x %1], [1 x %1], [1 x %1] }
%3 = type opaque
%4 = type { %0*, i64, i64, i64, [4 x i64]**, [4 x i64]**, [4 x i64]** }
%5 = type { [1 x %4], i64 }

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jmul_nistp256_inner(i128* %0, i128* nocapture %1, i128* nocapture %2, %0* nocapture readnone %3, i64* nocapture readonly %4, i64* nocapture readonly %5, i64* nocapture readonly %6, %1* %7) local_unnamed_addr #0 {
  %9 = bitcast i128* %0 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 16 %9, i8 0, i64 64, i1 false)
  %10 = bitcast i128* %1 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 16 %10, i8 0, i64 64, i1 false)
  %11 = bitcast i128* %2 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 16 %11, i8 0, i64 64, i1 false)
  %12 = tail call i64 @__gmpz_sizeinbase(%1* %7, i32 2) #6
  %13 = trunc i64 %12 to i32
  %14 = icmp sgt i32 %13, -1
  br i1 %14, label %15, label %26

15:                                               ; preds = %8
  %16 = shl i64 %12, 32
  %17 = ashr exact i64 %16, 32
  br label %18

18:                                               ; preds = %15, %23
  %19 = phi i64 [ %17, %15 ], [ %24, %23 ]
  tail call fastcc void @0(i128* %0, i128* %1, i128* %2, i128* %0, i128* %1, i128* %2)
  %20 = tail call i32 @__gmpz_tstbit(%1* %7, i64 %19) #6
  %21 = icmp eq i32 %20, 0
  br i1 %21, label %23, label %22

22:                                               ; preds = %18
  tail call fastcc void @1(i128* %0, i128* %1, i128* %2, i128* %0, i128* %1, i128* %2, i64* %4, i64* %5, i64* %6)
  br label %23

23:                                               ; preds = %18, %22
  %24 = add nsw i64 %19, -1
  %25 = icmp sgt i64 %19, 0
  br i1 %25, label %18, label %26

26:                                               ; preds = %23, %8
  ret void
}

; Function Attrs: argmemonly nounwind willreturn
declare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #1

; Function Attrs: argmemonly nounwind willreturn writeonly
declare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i1 immarg) #2

; Function Attrs: nounwind readonly
declare dso_local i64 @__gmpz_sizeinbase(%1*, i32) local_unnamed_addr #3

; Function Attrs: nounwind sspstrong uwtable
define internal fastcc void @0(i128* %0, i128* nocapture %1, i128* nocapture %2, i128* nocapture readonly %3, i128* nocapture readonly %4, i128* nocapture readonly %5) unnamed_addr #0 {
  %7 = load i128, i128* %3, align 16
  %8 = getelementptr inbounds i128, i128* %3, i64 1
  %9 = load i128, i128* %8, align 16
  %10 = getelementptr inbounds i128, i128* %3, i64 2
  %11 = load i128, i128* %10, align 16
  %12 = getelementptr inbounds i128, i128* %3, i64 3
  %13 = load i128, i128* %12, align 16
  %14 = getelementptr inbounds i128, i128* %5, i64 3
  %15 = load i128, i128* %14, align 16
  %16 = add i128 %15, 18446744069414584320
  %17 = getelementptr inbounds i128, i128* %5, i64 2
  %18 = load i128, i128* %17, align 16
  %19 = lshr i128 %18, 64
  %20 = add i128 %16, %19
  %21 = and i128 %18, 18446744073709551615
  %22 = add nuw nsw i128 %21, 18446673704965373952
  %23 = load i128, i128* %5, align 16
  %24 = add i128 %23, 18446744073709551615
  %25 = getelementptr inbounds i128, i128* %5, i64 1
  %26 = load i128, i128* %25, align 16
  %27 = add i128 %26, 1298074214633706907132628377272319
  %28 = lshr i128 %20, 64
  %29 = trunc i128 %28 to i64
  %30 = and i128 %20, 18446744073709551615
  %31 = sub nsw i128 %30, %28
  %32 = shl nuw nsw i128 %28, 32
  %33 = add nsw i128 %31, %32
  %34 = lshr i128 %33, 64
  %35 = trunc i128 %34 to i64
  %36 = add i64 %35, %29
  %37 = and i128 %33, 18446744073709551615
  %38 = sub nsw i128 %37, %34
  %39 = shl nuw nsw i128 %34, 32
  %40 = add nsw i128 %38, %39
  %41 = zext i64 %36 to i128
  %42 = add i128 %24, %41
  %43 = shl nuw nsw i128 %41, 32
  %44 = sub i128 %27, %43
  %45 = lshr i128 %40, 1
  %46 = trunc i128 %45 to i64
  %47 = trunc i128 %40 to i64
  %48 = and i64 %47, 9223372036854775807
  %49 = sub nsw i64 9223372032559808512, %48
  %50 = and i64 %49, %47
  %51 = or i64 %50, %46
  %52 = ashr i64 %51, 63
  %53 = zext i64 %52 to i128
  %54 = sub i128 %42, %53
  %55 = and i64 %52, 4294967295
  %56 = zext i64 %55 to i128
  %57 = sub i128 %44, %56
  %58 = and i64 %52, -4294967295
  %59 = zext i64 %58 to i128
  %60 = sub nsw i128 %40, %59
  %61 = lshr i128 %54, 64
  %62 = add i128 %57, %61
  %63 = lshr i128 %62, 64
  %64 = add nuw nsw i128 %22, %63
  %65 = lshr i128 %64, 64
  %66 = add nsw i128 %60, %65
  %67 = and i128 %54, 18446744073709551615
  %68 = mul nuw i128 %67, %67
  %69 = lshr i128 %68, 64
  %70 = and i128 %68, 18446744073709551615
  %71 = and i128 %62, 18446744073709551615
  %72 = mul nuw i128 %71, %67
  %73 = lshr i128 %72, 64
  %74 = shl i128 %72, 1
  %75 = and i128 %74, 36893488147419103230
  %76 = add nuw nsw i128 %75, %69
  %77 = and i128 %64, 18446744073709551615
  %78 = mul nuw i128 %77, %67
  %79 = lshr i128 %78, 64
  %80 = and i128 %78, 18446744073709551615
  %81 = add nuw nsw i128 %80, %73
  %82 = and i128 %66, 18446744073709551615
  %83 = mul nuw i128 %82, %67
  %84 = lshr i128 %83, 64
  %85 = and i128 %83, 18446744073709551615
  %86 = mul nuw i128 %77, %71
  %87 = lshr i128 %86, 64
  %88 = and i128 %86, 18446744073709551615
  %89 = add nuw nsw i128 %88, %79
  %90 = add nuw nsw i128 %89, %85
  %91 = shl nuw nsw i128 %90, 1
  %92 = add nuw nsw i128 %84, %87
  %93 = mul nuw i128 %71, %71
  %94 = lshr i128 %93, 64
  %95 = and i128 %93, 18446744073709551615
  %96 = mul nuw i128 %82, %71
  %97 = lshr i128 %96, 64
  %98 = and i128 %96, 18446744073709551615
  %99 = add nuw nsw i128 %92, %98
  %100 = shl nuw nsw i128 %99, 1
  %101 = mul nuw i128 %82, %77
  %102 = lshr i128 %101, 64
  %103 = and i128 %101, 18446744073709551615
  %104 = add nuw nsw i128 %97, %103
  %105 = shl nuw nsw i128 %104, 1
  %106 = shl nuw nsw i128 %102, 1
  %107 = mul nuw i128 %77, %77
  %108 = lshr i128 %107, 64
  %109 = and i128 %107, 18446744073709551615
  %110 = add nuw nsw i128 %100, %109
  %111 = add nuw nsw i128 %105, %108
  %112 = mul nuw i128 %82, %82
  %113 = lshr i128 %112, 64
  %114 = and i128 %112, 18446744073709551615
  %115 = add nuw nsw i128 %106, %114
  %116 = add nuw nsw i128 %70, 1267650600228229401427983728624
  %117 = or i128 %76, 1267650600228229401496703205376
  %118 = shl nuw nsw i128 %111, 32
  %119 = add nuw nsw i128 %118, %110
  %120 = sub nsw i128 %111, %113
  %121 = shl nuw nsw i128 %110, 32
  %122 = shl nuw nsw i128 %115, 32
  %123 = shl nuw nsw i128 %115, 33
  %124 = add nuw nsw i128 %123, %117
  %125 = add nsw i128 %124, %120
  %126 = sub nsw i128 %125, %121
  %127 = shl nuw nsw i128 %113, 32
  %128 = sub nsw i128 %116, %113
  %129 = sub nsw i128 %128, %127
  %130 = sub nsw i128 %129, %115
  %131 = sub nsw i128 %130, %122
  %132 = add nsw i128 %131, %119
  %133 = shl nuw nsw i128 %113, 33
  %134 = add nuw nsw i128 %81, %115
  %135 = shl nuw nsw i128 %134, 1
  %136 = add nuw nsw i128 %95, 1267650600228229401427983728656
  %137 = add nuw nsw i128 %136, %133
  %138 = sub nsw i128 %137, %120
  %139 = sub nsw i128 %138, %118
  %140 = add i128 %139, %135
  %141 = mul nuw nsw i128 %113, 3
  %142 = add nuw nsw i128 %94, 1267650600228229401427983728656
  %143 = add nuw nsw i128 %142, %141
  %144 = add nuw nsw i128 %143, %91
  %145 = sub nsw i128 %144, %122
  %146 = sub nsw i128 %145, %119
  %147 = add i128 %146, %121
  %148 = getelementptr inbounds i128, i128* %4, i64 3
  %149 = load i128, i128* %148, align 16
  %150 = add i128 %149, 18446744069414584320
  %151 = getelementptr inbounds i128, i128* %4, i64 2
  %152 = load i128, i128* %151, align 16
  %153 = lshr i128 %152, 64
  %154 = add i128 %150, %153
  %155 = and i128 %152, 18446744073709551615
  %156 = add nuw nsw i128 %155, 18446673704965373952
  %157 = load i128, i128* %4, align 16
  %158 = add i128 %157, 18446744073709551615
  %159 = getelementptr inbounds i128, i128* %4, i64 1
  %160 = load i128, i128* %159, align 16
  %161 = add i128 %160, 1298074214633706907132628377272319
  %162 = lshr i128 %154, 64
  %163 = trunc i128 %162 to i64
  %164 = and i128 %154, 18446744073709551615
  %165 = sub nsw i128 %164, %162
  %166 = shl nuw nsw i128 %162, 32
  %167 = add nsw i128 %165, %166
  %168 = lshr i128 %167, 64
  %169 = trunc i128 %168 to i64
  %170 = add i64 %169, %163
  %171 = and i128 %167, 18446744073709551615
  %172 = sub nsw i128 %171, %168
  %173 = shl nuw nsw i128 %168, 32
  %174 = add nsw i128 %172, %173
  %175 = zext i64 %170 to i128
  %176 = add i128 %158, %175
  %177 = shl nuw nsw i128 %175, 32
  %178 = sub i128 %161, %177
  %179 = lshr i128 %174, 1
  %180 = trunc i128 %179 to i64
  %181 = trunc i128 %174 to i64
  %182 = and i64 %181, 9223372036854775807
  %183 = sub nsw i64 9223372032559808512, %182
  %184 = and i64 %183, %181
  %185 = or i64 %184, %180
  %186 = ashr i64 %185, 63
  %187 = zext i64 %186 to i128
  %188 = sub i128 %176, %187
  %189 = and i64 %186, 4294967295
  %190 = zext i64 %189 to i128
  %191 = sub i128 %178, %190
  %192 = and i64 %186, -4294967295
  %193 = zext i64 %192 to i128
  %194 = sub nsw i128 %174, %193
  %195 = lshr i128 %188, 64
  %196 = add i128 %191, %195
  %197 = lshr i128 %196, 64
  %198 = add nuw nsw i128 %156, %197
  %199 = lshr i128 %198, 64
  %200 = add nsw i128 %194, %199
  %201 = and i128 %188, 18446744073709551615
  %202 = mul nuw i128 %201, %201
  %203 = lshr i128 %202, 64
  %204 = and i128 %202, 18446744073709551615
  %205 = and i128 %196, 18446744073709551615
  %206 = mul nuw i128 %205, %201
  %207 = lshr i128 %206, 64
  %208 = shl i128 %206, 1
  %209 = and i128 %208, 36893488147419103230
  %210 = add nuw nsw i128 %209, %203
  %211 = and i128 %198, 18446744073709551615
  %212 = mul nuw i128 %211, %201
  %213 = lshr i128 %212, 64
  %214 = and i128 %212, 18446744073709551615
  %215 = add nuw nsw i128 %214, %207
  %216 = and i128 %200, 18446744073709551615
  %217 = mul nuw i128 %216, %201
  %218 = lshr i128 %217, 64
  %219 = and i128 %217, 18446744073709551615
  %220 = mul nuw i128 %211, %205
  %221 = lshr i128 %220, 64
  %222 = and i128 %220, 18446744073709551615
  %223 = add nuw nsw i128 %222, %213
  %224 = add nuw nsw i128 %223, %219
  %225 = shl nuw nsw i128 %224, 1
  %226 = add nuw nsw i128 %218, %221
  %227 = mul nuw i128 %205, %205
  %228 = lshr i128 %227, 64
  %229 = and i128 %227, 18446744073709551615
  %230 = mul nuw i128 %216, %205
  %231 = lshr i128 %230, 64
  %232 = and i128 %230, 18446744073709551615
  %233 = add nuw nsw i128 %226, %232
  %234 = shl nuw nsw i128 %233, 1
  %235 = mul nuw i128 %216, %211
  %236 = lshr i128 %235, 64
  %237 = and i128 %235, 18446744073709551615
  %238 = add nuw nsw i128 %231, %237
  %239 = shl nuw nsw i128 %238, 1
  %240 = shl nuw nsw i128 %236, 1
  %241 = mul nuw i128 %211, %211
  %242 = lshr i128 %241, 64
  %243 = and i128 %241, 18446744073709551615
  %244 = add nuw nsw i128 %234, %243
  %245 = add nuw nsw i128 %239, %242
  %246 = mul nuw i128 %216, %216
  %247 = lshr i128 %246, 64
  %248 = and i128 %246, 18446744073709551615
  %249 = add nuw nsw i128 %240, %248
  %250 = add nuw nsw i128 %204, 1267650600228229401427983728624
  %251 = or i128 %210, 1267650600228229401496703205376
  %252 = shl nuw nsw i128 %245, 32
  %253 = add nuw nsw i128 %252, %244
  %254 = sub nsw i128 %245, %247
  %255 = shl nuw nsw i128 %244, 32
  %256 = shl nuw nsw i128 %249, 32
  %257 = shl nuw nsw i128 %249, 33
  %258 = add nuw nsw i128 %257, %251
  %259 = add nsw i128 %258, %254
  %260 = sub nsw i128 %259, %255
  %261 = shl nuw nsw i128 %247, 32
  %262 = sub nsw i128 %250, %247
  %263 = sub nsw i128 %262, %261
  %264 = sub nsw i128 %263, %249
  %265 = sub nsw i128 %264, %256
  %266 = add nsw i128 %265, %253
  %267 = shl nuw nsw i128 %247, 33
  %268 = add nuw nsw i128 %215, %249
  %269 = shl nuw nsw i128 %268, 1
  %270 = add nuw nsw i128 %229, 1267650600228229401427983728656
  %271 = add nuw nsw i128 %270, %267
  %272 = sub nsw i128 %271, %254
  %273 = sub nsw i128 %272, %252
  %274 = add i128 %273, %269
  %275 = mul nuw nsw i128 %247, 3
  %276 = add nuw nsw i128 %228, 1267650600228229401427983728656
  %277 = add nuw nsw i128 %276, %275
  %278 = add nuw nsw i128 %277, %225
  %279 = sub nsw i128 %278, %256
  %280 = sub nsw i128 %279, %253
  %281 = add i128 %280, %255
  %282 = add i128 %281, 18446744069414584320
  %283 = lshr i128 %274, 64
  %284 = add i128 %282, %283
  %285 = and i128 %274, 18446744073709551615
  %286 = add nuw nsw i128 %285, 18446673704965373952
  %287 = add i128 %266, 18446744073709551615
  %288 = add i128 %260, 1298074214633706907132628377272319
  %289 = lshr i128 %284, 64
  %290 = trunc i128 %289 to i64
  %291 = and i128 %284, 18446744073709551615
  %292 = sub nsw i128 %291, %289
  %293 = shl nuw nsw i128 %289, 32
  %294 = add nsw i128 %292, %293
  %295 = lshr i128 %294, 64
  %296 = trunc i128 %295 to i64
  %297 = add i64 %296, %290
  %298 = and i128 %294, 18446744073709551615
  %299 = sub nsw i128 %298, %295
  %300 = shl nuw nsw i128 %295, 32
  %301 = add nsw i128 %299, %300
  %302 = zext i64 %297 to i128
  %303 = add i128 %287, %302
  %304 = shl nuw nsw i128 %302, 32
  %305 = sub i128 %288, %304
  %306 = lshr i128 %301, 1
  %307 = trunc i128 %306 to i64
  %308 = trunc i128 %301 to i64
  %309 = and i64 %308, 9223372036854775807
  %310 = sub nsw i64 9223372032559808512, %309
  %311 = and i64 %310, %308
  %312 = or i64 %311, %307
  %313 = ashr i64 %312, 63
  %314 = zext i64 %313 to i128
  %315 = sub i128 %303, %314
  %316 = and i64 %313, 4294967295
  %317 = zext i64 %316 to i128
  %318 = sub i128 %305, %317
  %319 = and i64 %313, -4294967295
  %320 = zext i64 %319 to i128
  %321 = sub nsw i128 %301, %320
  %322 = lshr i128 %315, 64
  %323 = add i128 %318, %322
  %324 = lshr i128 %323, 64
  %325 = add nuw nsw i128 %286, %324
  %326 = lshr i128 %325, 64
  %327 = add nsw i128 %321, %326
  %328 = add i128 %13, 18446744069414584320
  %329 = lshr i128 %11, 64
  %330 = add i128 %328, %329
  %331 = and i128 %11, 18446744073709551615
  %332 = add nuw nsw i128 %331, 18446673704965373952
  %333 = add i128 %7, 18446744073709551615
  %334 = add i128 %9, 1298074214633706907132628377272319
  %335 = lshr i128 %330, 64
  %336 = trunc i128 %335 to i64
  %337 = and i128 %330, 18446744073709551615
  %338 = sub nsw i128 %337, %335
  %339 = shl nuw nsw i128 %335, 32
  %340 = add nsw i128 %338, %339
  %341 = lshr i128 %340, 64
  %342 = trunc i128 %341 to i64
  %343 = add i64 %342, %336
  %344 = and i128 %340, 18446744073709551615
  %345 = sub nsw i128 %344, %341
  %346 = shl nuw nsw i128 %341, 32
  %347 = add nsw i128 %345, %346
  %348 = zext i64 %343 to i128
  %349 = add i128 %333, %348
  %350 = shl nuw nsw i128 %348, 32
  %351 = sub i128 %334, %350
  %352 = lshr i128 %347, 1
  %353 = trunc i128 %352 to i64
  %354 = trunc i128 %347 to i64
  %355 = and i64 %354, 9223372036854775807
  %356 = sub nsw i64 9223372032559808512, %355
  %357 = and i64 %356, %354
  %358 = or i64 %357, %353
  %359 = ashr i64 %358, 63
  %360 = zext i64 %359 to i128
  %361 = sub i128 %349, %360
  %362 = and i64 %359, 4294967295
  %363 = zext i64 %362 to i128
  %364 = sub i128 %351, %363
  %365 = and i64 %359, -4294967295
  %366 = zext i64 %365 to i128
  %367 = sub nsw i128 %347, %366
  %368 = lshr i128 %361, 64
  %369 = add i128 %364, %368
  %370 = lshr i128 %369, 64
  %371 = add nuw nsw i128 %332, %370
  %372 = lshr i128 %371, 64
  %373 = add nsw i128 %367, %372
  %374 = and i128 %315, 18446744073709551615
  %375 = and i128 %361, 18446744073709551615
  %376 = mul nuw i128 %374, %375
  %377 = lshr i128 %376, 64
  %378 = and i128 %376, 18446744073709551615
  %379 = and i128 %369, 18446744073709551615
  %380 = mul nuw i128 %374, %379
  %381 = lshr i128 %380, 64
  %382 = and i128 %380, 18446744073709551615
  %383 = add nuw nsw i128 %382, %377
  %384 = and i128 %323, 18446744073709551615
  %385 = mul nuw i128 %384, %375
  %386 = lshr i128 %385, 64
  %387 = and i128 %385, 18446744073709551615
  %388 = add nuw nsw i128 %383, %387
  %389 = and i128 %371, 18446744073709551615
  %390 = mul nuw i128 %374, %389
  %391 = lshr i128 %390, 64
  %392 = and i128 %390, 18446744073709551615
  %393 = mul nuw i128 %384, %379
  %394 = lshr i128 %393, 64
  %395 = and i128 %393, 18446744073709551615
  %396 = and i128 %325, 18446744073709551615
  %397 = mul nuw i128 %396, %375
  %398 = lshr i128 %397, 64
  %399 = and i128 %397, 18446744073709551615
  %400 = and i128 %373, 18446744073709551615
  %401 = mul nuw i128 %374, %400
  %402 = lshr i128 %401, 64
  %403 = and i128 %401, 18446744073709551615
  %404 = mul nuw i128 %384, %389
  %405 = lshr i128 %404, 64
  %406 = and i128 %404, 18446744073709551615
  %407 = add nuw nsw i128 %405, %402
  %408 = mul nuw i128 %396, %379
  %409 = lshr i128 %408, 64
  %410 = and i128 %408, 18446744073709551615
  %411 = and i128 %327, 18446744073709551615
  %412 = mul nuw i128 %411, %375
  %413 = lshr i128 %412, 64
  %414 = and i128 %412, 18446744073709551615
  %415 = mul nuw i128 %384, %400
  %416 = lshr i128 %415, 64
  %417 = and i128 %415, 18446744073709551615
  %418 = mul nuw i128 %396, %389
  %419 = lshr i128 %418, 64
  %420 = and i128 %418, 18446744073709551615
  %421 = add nuw nsw i128 %419, %416
  %422 = mul nuw i128 %411, %379
  %423 = lshr i128 %422, 64
  %424 = and i128 %422, 18446744073709551615
  %425 = add nuw nsw i128 %407, %417
  %426 = add nuw nsw i128 %425, %409
  %427 = add nuw nsw i128 %426, %420
  %428 = add nuw nsw i128 %427, %413
  %429 = add nuw nsw i128 %428, %424
  %430 = mul nuw i128 %396, %400
  %431 = lshr i128 %430, 64
  %432 = and i128 %430, 18446744073709551615
  %433 = add nuw nsw i128 %421, %432
  %434 = add nuw nsw i128 %433, %423
  %435 = mul nuw i128 %411, %389
  %436 = lshr i128 %435, 64
  %437 = and i128 %435, 18446744073709551615
  %438 = add nuw nsw i128 %434, %437
  %439 = add nuw nsw i128 %436, %431
  %440 = mul nuw i128 %411, %400
  %441 = lshr i128 %440, 64
  %442 = and i128 %440, 18446744073709551615
  %443 = add nuw nsw i128 %439, %442
  %444 = add nuw nsw i128 %378, 1267650600228229401427983728624
  %445 = or i128 %388, 1267650600228229401496703205376
  %446 = shl nuw nsw i128 %438, 32
  %447 = add i128 %446, %429
  %448 = sub nsw i128 %438, %441
  %449 = add nsw i128 %448, %445
  %450 = shl i128 %429, 32
  %451 = shl nuw nsw i128 %443, 32
  %452 = shl nuw nsw i128 %443, 33
  %453 = add i128 %449, %452
  %454 = sub i128 %453, %450
  %455 = shl nuw nsw i128 %443, 1
  %456 = shl nuw nsw i128 %441, 32
  %457 = sub nsw i128 %444, %441
  %458 = sub nsw i128 %457, %456
  %459 = sub nsw i128 %458, %443
  %460 = sub nsw i128 %459, %451
  %461 = add i128 %460, %447
  %462 = shl nuw nsw i128 %441, 33
  %463 = add nuw nsw i128 %381, 1267650600228229401427983728656
  %464 = add nuw nsw i128 %463, %392
  %465 = add nuw nsw i128 %464, %386
  %466 = add nuw nsw i128 %465, %395
  %467 = add nuw nsw i128 %466, %399
  %468 = add nuw nsw i128 %467, %462
  %469 = sub i128 %468, %448
  %470 = sub i128 %469, %446
  %471 = add i128 %470, %455
  %472 = mul nuw nsw i128 %441, 3
  %473 = add nuw nsw i128 %391, 1267650600228229401427983728656
  %474 = add nuw nsw i128 %473, %403
  %475 = add nuw nsw i128 %474, %394
  %476 = add nuw nsw i128 %475, %406
  %477 = add nuw nsw i128 %476, %398
  %478 = add nuw nsw i128 %477, %410
  %479 = add i128 %478, %414
  %480 = add i128 %479, %472
  %481 = sub i128 %480, %451
  %482 = sub i128 %481, %447
  %483 = add i128 %482, %450
  %484 = add i128 %11, 40564819207303340845695479316992
  %485 = sub i128 %484, %140
  %486 = add i128 %132, %7
  %487 = add i128 %126, %9
  %488 = add i128 %140, %11
  %489 = add i128 %147, %13
  %490 = mul i128 %486, 3
  %491 = mul i128 %487, 3
  %492 = mul i128 %488, 3
  %493 = mul i128 %489, 3
  %494 = lshr i128 %485, 64
  %495 = add i128 %13, 40564819207321787589764893901312
  %496 = sub i128 %495, %147
  %497 = add i128 %496, %494
  %498 = and i128 %485, 18446744073709551615
  %499 = add nuw nsw i128 %498, 18446673704965373952
  %500 = lshr i128 %497, 64
  %501 = trunc i128 %500 to i64
  %502 = and i128 %497, 18446744073709551615
  %503 = sub nsw i128 %502, %500
  %504 = shl nuw nsw i128 %500, 32
  %505 = add nsw i128 %503, %504
  %506 = lshr i128 %505, 64
  %507 = trunc i128 %506 to i64
  %508 = add i64 %507, %501
  %509 = and i128 %505, 18446744073709551615
  %510 = sub nsw i128 %509, %506
  %511 = shl nuw nsw i128 %506, 32
  %512 = add nsw i128 %510, %511
  %513 = zext i64 %508 to i128
  %514 = shl nuw nsw i128 %513, 32
  %515 = lshr i128 %512, 1
  %516 = trunc i128 %515 to i64
  %517 = trunc i128 %512 to i64
  %518 = and i64 %517, 9223372036854775807
  %519 = sub nsw i64 9223372032559808512, %518
  %520 = and i64 %519, %517
  %521 = or i64 %520, %516
  %522 = ashr i64 %521, 63
  %523 = zext i64 %522 to i128
  %524 = add i128 %7, 40564819207321787589769188867583
  %525 = sub i128 %524, %132
  %526 = add i128 %525, %513
  %527 = sub i128 %526, %523
  %528 = and i64 %522, 4294967295
  %529 = zext i64 %528 to i128
  %530 = and i64 %522, -4294967295
  %531 = zext i64 %530 to i128
  %532 = sub nsw i128 %512, %531
  %533 = lshr i128 %527, 64
  %534 = add i128 %9, 1338639033841010247980522879844351
  %535 = sub i128 %534, %126
  %536 = sub i128 %535, %514
  %537 = sub i128 %536, %529
  %538 = add i128 %537, %533
  %539 = lshr i128 %538, 64
  %540 = add nuw nsw i128 %499, %539
  %541 = lshr i128 %540, 64
  %542 = add nsw i128 %532, %541
  %543 = add i128 %493, 18446744069414584320
  %544 = lshr i128 %492, 64
  %545 = add i128 %543, %544
  %546 = and i128 %492, 18446744073709551615
  %547 = add nuw nsw i128 %546, 18446673704965373952
  %548 = add i128 %490, 18446744073709551615
  %549 = add i128 %491, 1298074214633706907132628377272319
  %550 = lshr i128 %545, 64
  %551 = trunc i128 %550 to i64
  %552 = and i128 %545, 18446744073709551615
  %553 = sub nsw i128 %552, %550
  %554 = shl nuw nsw i128 %550, 32
  %555 = add nsw i128 %553, %554
  %556 = lshr i128 %555, 64
  %557 = trunc i128 %556 to i64
  %558 = add i64 %557, %551
  %559 = and i128 %555, 18446744073709551615
  %560 = sub nsw i128 %559, %556
  %561 = shl nuw nsw i128 %556, 32
  %562 = add nsw i128 %560, %561
  %563 = zext i64 %558 to i128
  %564 = add i128 %548, %563
  %565 = shl nuw nsw i128 %563, 32
  %566 = sub i128 %549, %565
  %567 = lshr i128 %562, 1
  %568 = trunc i128 %567 to i64
  %569 = trunc i128 %562 to i64
  %570 = and i64 %569, 9223372036854775807
  %571 = sub nsw i64 9223372032559808512, %570
  %572 = and i64 %571, %569
  %573 = or i64 %572, %568
  %574 = ashr i64 %573, 63
  %575 = zext i64 %574 to i128
  %576 = sub i128 %564, %575
  %577 = and i64 %574, 4294967295
  %578 = zext i64 %577 to i128
  %579 = sub i128 %566, %578
  %580 = and i64 %574, -4294967295
  %581 = zext i64 %580 to i128
  %582 = sub nsw i128 %562, %581
  %583 = lshr i128 %576, 64
  %584 = add i128 %579, %583
  %585 = lshr i128 %584, 64
  %586 = add nuw nsw i128 %547, %585
  %587 = lshr i128 %586, 64
  %588 = add nsw i128 %582, %587
  %589 = and i128 %527, 18446744073709551615
  %590 = and i128 %576, 18446744073709551615
  %591 = mul nuw i128 %590, %589
  %592 = lshr i128 %591, 64
  %593 = and i128 %591, 18446744073709551615
  %594 = and i128 %584, 18446744073709551615
  %595 = mul nuw i128 %594, %589
  %596 = lshr i128 %595, 64
  %597 = and i128 %595, 18446744073709551615
  %598 = and i128 %538, 18446744073709551615
  %599 = mul nuw i128 %598, %590
  %600 = lshr i128 %599, 64
  %601 = and i128 %599, 18446744073709551615
  %602 = add nuw nsw i128 %601, %592
  %603 = add nuw nsw i128 %602, %597
  %604 = and i128 %586, 18446744073709551615
  %605 = mul nuw i128 %604, %589
  %606 = lshr i128 %605, 64
  %607 = and i128 %605, 18446744073709551615
  %608 = mul nuw i128 %594, %598
  %609 = lshr i128 %608, 64
  %610 = and i128 %608, 18446744073709551615
  %611 = and i128 %540, 18446744073709551615
  %612 = mul nuw i128 %611, %590
  %613 = lshr i128 %612, 64
  %614 = and i128 %612, 18446744073709551615
  %615 = and i128 %588, 18446744073709551615
  %616 = mul nuw i128 %615, %589
  %617 = lshr i128 %616, 64
  %618 = and i128 %616, 18446744073709551615
  %619 = mul nuw i128 %604, %598
  %620 = lshr i128 %619, 64
  %621 = and i128 %619, 18446744073709551615
  %622 = mul nuw i128 %611, %594
  %623 = lshr i128 %622, 64
  %624 = and i128 %622, 18446744073709551615
  %625 = and i128 %542, 18446744073709551615
  %626 = mul nuw i128 %625, %590
  %627 = lshr i128 %626, 64
  %628 = and i128 %626, 18446744073709551615
  %629 = mul nuw i128 %615, %598
  %630 = lshr i128 %629, 64
  %631 = and i128 %629, 18446744073709551615
  %632 = mul nuw i128 %604, %611
  %633 = lshr i128 %632, 64
  %634 = and i128 %632, 18446744073709551615
  %635 = mul nuw i128 %625, %594
  %636 = lshr i128 %635, 64
  %637 = and i128 %635, 18446744073709551615
  %638 = add nuw nsw i128 %620, %623
  %639 = add nuw nsw i128 %638, %634
  %640 = add nuw nsw i128 %639, %627
  %641 = add nuw nsw i128 %640, %637
  %642 = add nuw nsw i128 %641, %617
  %643 = add nuw nsw i128 %642, %631
  %644 = mul nuw i128 %615, %611
  %645 = lshr i128 %644, 64
  %646 = and i128 %644, 18446744073709551615
  %647 = mul nuw i128 %625, %604
  %648 = lshr i128 %647, 64
  %649 = and i128 %647, 18446744073709551615
  %650 = add nuw nsw i128 %636, %633
  %651 = add nuw nsw i128 %650, %649
  %652 = add nuw nsw i128 %651, %630
  %653 = add nuw nsw i128 %652, %646
  %654 = add nuw nsw i128 %645, %648
  %655 = mul nuw i128 %615, %625
  %656 = lshr i128 %655, 64
  %657 = and i128 %655, 18446744073709551615
  %658 = add nuw nsw i128 %654, %657
  %659 = or i128 %603, 1267650600228229401496703205376
  %660 = shl nuw nsw i128 %653, 32
  %661 = add i128 %643, %660
  %662 = sub nsw i128 %653, %656
  %663 = shl i128 %643, 32
  %664 = shl nuw nsw i128 %658, 32
  %665 = shl nuw nsw i128 %658, 33
  %666 = shl nuw nsw i128 %658, 1
  %667 = shl nuw nsw i128 %656, 32
  %668 = shl nuw nsw i128 %656, 33
  %669 = add nuw nsw i128 %600, 1267650600228229401427983728656
  %670 = add nuw nsw i128 %669, %596
  %671 = add nuw nsw i128 %670, %610
  %672 = add nuw nsw i128 %671, %614
  %673 = add nuw nsw i128 %672, %607
  %674 = add nuw nsw i128 %673, %668
  %675 = add i128 %674, %666
  %676 = sub i128 %675, %662
  %677 = sub i128 %676, %660
  %678 = mul nuw nsw i128 %656, 3
  %679 = lshr i128 %677, 64
  %680 = add nuw nsw i128 %609, 1267650600246676145497398312976
  %681 = add nuw nsw i128 %680, %613
  %682 = add nuw nsw i128 %681, %624
  %683 = add nuw nsw i128 %682, %606
  %684 = add nuw nsw i128 %683, %621
  %685 = add nuw nsw i128 %684, %628
  %686 = add i128 %685, %618
  %687 = add i128 %686, %678
  %688 = sub i128 %687, %664
  %689 = sub i128 %688, %661
  %690 = add i128 %689, %663
  %691 = add i128 %690, %679
  %692 = and i128 %677, 18446744073709551615
  %693 = add nuw nsw i128 %692, 18446673704965373952
  %694 = lshr i128 %691, 64
  %695 = trunc i128 %694 to i64
  %696 = and i128 %691, 18446744073709551615
  %697 = sub nsw i128 %696, %694
  %698 = shl nuw nsw i128 %694, 32
  %699 = add nsw i128 %697, %698
  %700 = lshr i128 %699, 64
  %701 = trunc i128 %700 to i64
  %702 = add i64 %701, %695
  %703 = and i128 %699, 18446744073709551615
  %704 = sub nsw i128 %703, %700
  %705 = shl nuw nsw i128 %700, 32
  %706 = add nsw i128 %704, %705
  %707 = zext i64 %702 to i128
  %708 = shl nuw nsw i128 %707, 32
  %709 = lshr i128 %706, 1
  %710 = trunc i128 %709 to i64
  %711 = trunc i128 %706 to i64
  %712 = and i64 %711, 9223372036854775807
  %713 = sub nsw i64 9223372032559808512, %712
  %714 = and i64 %713, %711
  %715 = or i64 %714, %710
  %716 = ashr i64 %715, 63
  %717 = zext i64 %716 to i128
  %718 = add nuw nsw i128 %593, 1267650600246676145501693280239
  %719 = sub nsw i128 %718, %656
  %720 = sub nsw i128 %719, %667
  %721 = sub nsw i128 %720, %658
  %722 = sub nsw i128 %721, %664
  %723 = add i128 %722, %661
  %724 = add i128 %723, %707
  %725 = sub i128 %724, %717
  %726 = and i64 %716, 4294967295
  %727 = zext i64 %726 to i128
  %728 = and i64 %716, -4294967295
  %729 = zext i64 %728 to i128
  %730 = sub nsw i128 %706, %729
  %731 = lshr i128 %725, 64
  %732 = add nuw nsw i128 %659, 1298074214633706907132628377272319
  %733 = add nuw nsw i128 %732, %665
  %734 = add nsw i128 %733, %662
  %735 = sub i128 %734, %663
  %736 = sub i128 %735, %708
  %737 = sub i128 %736, %727
  %738 = add i128 %737, %731
  %739 = lshr i128 %738, 64
  %740 = add nuw nsw i128 %693, %739
  %741 = lshr i128 %740, 64
  %742 = add nsw i128 %730, %741
  %743 = and i128 %725, 18446744073709551615
  %744 = mul nuw i128 %743, %743
  %745 = lshr i128 %744, 64
  %746 = and i128 %744, 18446744073709551615
  %747 = and i128 %738, 18446744073709551615
  %748 = mul nuw i128 %747, %743
  %749 = lshr i128 %748, 64
  %750 = shl i128 %748, 1
  %751 = and i128 %750, 36893488147419103230
  %752 = add nuw nsw i128 %751, %745
  %753 = and i128 %740, 18446744073709551615
  %754 = mul nuw i128 %753, %743
  %755 = lshr i128 %754, 64
  %756 = and i128 %754, 18446744073709551615
  %757 = add nuw nsw i128 %756, %749
  %758 = shl nuw nsw i128 %757, 1
  %759 = and i128 %742, 18446744073709551615
  %760 = mul nuw i128 %759, %743
  %761 = lshr i128 %760, 64
  %762 = and i128 %760, 18446744073709551615
  %763 = mul nuw i128 %753, %747
  %764 = lshr i128 %763, 64
  %765 = and i128 %763, 18446744073709551615
  %766 = add nuw nsw i128 %765, %755
  %767 = add nuw nsw i128 %766, %762
  %768 = shl nuw nsw i128 %767, 1
  %769 = add nuw nsw i128 %761, %764
  %770 = mul nuw i128 %747, %747
  %771 = lshr i128 %770, 64
  %772 = and i128 %770, 18446744073709551615
  %773 = mul nuw i128 %759, %747
  %774 = lshr i128 %773, 64
  %775 = and i128 %773, 18446744073709551615
  %776 = add nuw nsw i128 %769, %775
  %777 = shl nuw nsw i128 %776, 1
  %778 = mul nuw i128 %759, %753
  %779 = lshr i128 %778, 64
  %780 = and i128 %778, 18446744073709551615
  %781 = add nuw nsw i128 %774, %780
  %782 = shl nuw nsw i128 %781, 1
  %783 = shl nuw nsw i128 %779, 1
  %784 = mul nuw i128 %753, %753
  %785 = lshr i128 %784, 64
  %786 = and i128 %784, 18446744073709551615
  %787 = add nuw nsw i128 %777, %786
  %788 = add nuw nsw i128 %782, %785
  %789 = mul nuw i128 %759, %759
  %790 = lshr i128 %789, 64
  %791 = and i128 %789, 18446744073709551615
  %792 = add nuw nsw i128 %783, %791
  %793 = add nuw nsw i128 %746, 1267650600228229401427983728624
  %794 = or i128 %752, 1267650600228229401496703205376
  %795 = getelementptr inbounds i128, i128* %0, i64 1
  %796 = add nuw nsw i128 %772, 1267650600228229401427983728656
  %797 = add nuw nsw i128 %796, %758
  %798 = getelementptr inbounds i128, i128* %0, i64 2
  %799 = add nuw nsw i128 %771, 1267650600228229401427983728656
  %800 = add nuw nsw i128 %799, %768
  %801 = getelementptr inbounds i128, i128* %0, i64 3
  %802 = shl nuw nsw i128 %788, 32
  %803 = add nuw nsw i128 %802, %787
  %804 = add nuw nsw i128 %803, %793
  %805 = sub nsw i128 %800, %803
  %806 = sub nsw i128 %788, %790
  %807 = add nsw i128 %806, %794
  %808 = sub nsw i128 %797, %806
  %809 = shl nuw nsw i128 %787, 32
  %810 = sub nsw i128 %807, %809
  %811 = add i128 %805, %809
  %812 = sub nsw i128 %808, %802
  %813 = sub i128 %804, %792
  %814 = shl nuw nsw i128 %792, 32
  %815 = sub i128 %813, %814
  %816 = shl nuw nsw i128 %792, 33
  %817 = add i128 %810, %816
  %818 = shl nuw nsw i128 %792, 1
  %819 = add i128 %812, %818
  %820 = sub i128 %811, %814
  %821 = sub i128 %815, %790
  %822 = shl nuw nsw i128 %790, 32
  %823 = sub i128 %821, %822
  %824 = shl nuw nsw i128 %790, 33
  %825 = add i128 %819, %824
  %826 = mul nuw nsw i128 %790, 3
  %827 = add i128 %820, %826
  %828 = shl i128 %461, 3
  %829 = shl i128 %454, 3
  %830 = shl i128 %471, 3
  %831 = shl i128 %483, 3
  %832 = add i128 %823, 40564819207303340845695479315968
  %833 = add i128 %817, 40564819207303340847894502572032
  %834 = add i128 %825, 40564819207303340845695479316992
  %835 = add i128 %827, 40564819207303340845695479316992
  %836 = sub i128 %832, %828
  store i128 %836, i128* %0, align 16
  %837 = sub i128 %833, %829
  store i128 %837, i128* %795, align 16
  %838 = sub i128 %834, %830
  store i128 %838, i128* %798, align 16
  %839 = sub i128 %835, %831
  store i128 %839, i128* %801, align 16
  %840 = load i128, i128* %4, align 16
  %841 = load i128, i128* %159, align 16
  %842 = load i128, i128* %151, align 16
  %843 = load i128, i128* %148, align 16
  %844 = load i128, i128* %5, align 16
  %845 = load i128, i128* %25, align 16
  %846 = load i128, i128* %17, align 16
  %847 = add i128 %846, %842
  %848 = load i128, i128* %14, align 16
  %849 = lshr i128 %847, 64
  %850 = add i128 %843, 18446744069414584320
  %851 = add i128 %850, %848
  %852 = add i128 %851, %849
  %853 = and i128 %847, 18446744073709551615
  %854 = add nuw nsw i128 %853, 18446673704965373952
  %855 = lshr i128 %852, 64
  %856 = trunc i128 %855 to i64
  %857 = and i128 %852, 18446744073709551615
  %858 = sub nsw i128 %857, %855
  %859 = shl nuw nsw i128 %855, 32
  %860 = add nsw i128 %858, %859
  %861 = lshr i128 %860, 64
  %862 = trunc i128 %861 to i64
  %863 = add i64 %862, %856
  %864 = and i128 %860, 18446744073709551615
  %865 = sub nsw i128 %864, %861
  %866 = shl nuw nsw i128 %861, 32
  %867 = add nsw i128 %865, %866
  %868 = zext i64 %863 to i128
  %869 = shl nuw nsw i128 %868, 32
  %870 = lshr i128 %867, 1
  %871 = trunc i128 %870 to i64
  %872 = trunc i128 %867 to i64
  %873 = and i64 %872, 9223372036854775807
  %874 = sub nsw i64 9223372032559808512, %873
  %875 = and i64 %874, %872
  %876 = or i64 %875, %871
  %877 = ashr i64 %876, 63
  %878 = zext i64 %877 to i128
  %879 = add i128 %840, 18446744073709551615
  %880 = add i128 %879, %844
  %881 = add i128 %880, %868
  %882 = sub i128 %881, %878
  %883 = and i64 %877, 4294967295
  %884 = zext i64 %883 to i128
  %885 = and i64 %877, -4294967295
  %886 = zext i64 %885 to i128
  %887 = sub nsw i128 %867, %886
  %888 = lshr i128 %882, 64
  %889 = add i128 %841, 1298074214633706907132628377272319
  %890 = add i128 %889, %845
  %891 = sub i128 %890, %869
  %892 = sub i128 %891, %884
  %893 = add i128 %892, %888
  %894 = lshr i128 %893, 64
  %895 = add nuw nsw i128 %854, %894
  %896 = lshr i128 %895, 64
  %897 = add nsw i128 %887, %896
  %898 = and i128 %882, 18446744073709551615
  %899 = mul nuw i128 %898, %898
  %900 = lshr i128 %899, 64
  %901 = and i128 %899, 18446744073709551615
  %902 = and i128 %893, 18446744073709551615
  %903 = mul nuw i128 %902, %898
  %904 = lshr i128 %903, 64
  %905 = shl i128 %903, 1
  %906 = and i128 %905, 36893488147419103230
  %907 = add nuw nsw i128 %906, %900
  %908 = and i128 %895, 18446744073709551615
  %909 = mul nuw i128 %908, %898
  %910 = lshr i128 %909, 64
  %911 = and i128 %909, 18446744073709551615
  %912 = add nuw nsw i128 %911, %904
  %913 = shl nuw nsw i128 %912, 1
  %914 = and i128 %897, 18446744073709551615
  %915 = mul nuw i128 %914, %898
  %916 = lshr i128 %915, 64
  %917 = and i128 %915, 18446744073709551615
  %918 = mul nuw i128 %908, %902
  %919 = lshr i128 %918, 64
  %920 = and i128 %918, 18446744073709551615
  %921 = add nuw nsw i128 %920, %910
  %922 = add nuw nsw i128 %921, %917
  %923 = shl nuw nsw i128 %922, 1
  %924 = add nuw nsw i128 %916, %919
  %925 = mul nuw i128 %902, %902
  %926 = lshr i128 %925, 64
  %927 = and i128 %925, 18446744073709551615
  %928 = mul nuw i128 %914, %902
  %929 = lshr i128 %928, 64
  %930 = and i128 %928, 18446744073709551615
  %931 = add nuw nsw i128 %924, %930
  %932 = shl nuw nsw i128 %931, 1
  %933 = mul nuw i128 %914, %908
  %934 = lshr i128 %933, 64
  %935 = and i128 %933, 18446744073709551615
  %936 = add nuw nsw i128 %929, %935
  %937 = shl nuw nsw i128 %936, 1
  %938 = shl nuw nsw i128 %934, 1
  %939 = mul nuw i128 %908, %908
  %940 = lshr i128 %939, 64
  %941 = and i128 %939, 18446744073709551615
  %942 = add nuw nsw i128 %932, %941
  %943 = add nuw nsw i128 %937, %940
  %944 = mul nuw i128 %914, %914
  %945 = lshr i128 %944, 64
  %946 = and i128 %944, 18446744073709551615
  %947 = add nuw nsw i128 %938, %946
  %948 = add nuw nsw i128 %901, 1267650600228229401427983728624
  %949 = or i128 %907, 1267650600228229401496703205376
  %950 = getelementptr inbounds i128, i128* %2, i64 1
  %951 = add nuw nsw i128 %927, 1267650600228229401427983728656
  %952 = add nuw nsw i128 %951, %913
  %953 = getelementptr inbounds i128, i128* %2, i64 2
  %954 = add nuw nsw i128 %926, 1267650600228229401427983728656
  %955 = add nuw nsw i128 %954, %923
  %956 = getelementptr inbounds i128, i128* %2, i64 3
  %957 = shl nuw nsw i128 %943, 32
  %958 = add nuw nsw i128 %957, %942
  %959 = add nuw nsw i128 %958, %948
  %960 = sub nsw i128 %955, %958
  %961 = sub nsw i128 %943, %945
  %962 = add nsw i128 %961, %949
  %963 = sub nsw i128 %952, %961
  %964 = shl nuw nsw i128 %942, 32
  %965 = sub nsw i128 %962, %964
  %966 = add i128 %960, %964
  %967 = sub nsw i128 %963, %957
  %968 = sub i128 %959, %947
  %969 = shl nuw nsw i128 %947, 32
  %970 = sub i128 %968, %969
  %971 = shl nuw nsw i128 %947, 33
  %972 = add i128 %965, %971
  %973 = shl nuw nsw i128 %947, 1
  %974 = add i128 %967, %973
  %975 = sub i128 %966, %969
  %976 = sub i128 %970, %945
  %977 = shl nuw nsw i128 %945, 32
  %978 = sub i128 %976, %977
  %979 = shl nuw nsw i128 %945, 33
  %980 = add i128 %974, %979
  %981 = mul nuw nsw i128 %945, 3
  %982 = add i128 %975, %981
  %983 = add i128 %978, 40564819207303340845695479315968
  %984 = add i128 %972, 40564819207303340847894502572032
  %985 = add i128 %980, 40564819207303340845695479316992
  %986 = add i128 %982, 40564819207303340845695479316992
  %987 = add i128 %266, %132
  %988 = sub i128 %983, %987
  store i128 %988, i128* %2, align 16
  %989 = add i128 %260, %126
  %990 = sub i128 %984, %989
  store i128 %990, i128* %950, align 16
  %991 = add i128 %274, %140
  %992 = sub i128 %985, %991
  store i128 %992, i128* %953, align 16
  %993 = add i128 %281, %147
  %994 = sub i128 %986, %993
  store i128 %994, i128* %956, align 16
  %995 = shl i128 %461, 2
  %996 = shl i128 %454, 2
  %997 = shl i128 %471, 2
  %998 = shl i128 %483, 2
  %999 = load i128, i128* %0, align 16
  %1000 = load i128, i128* %795, align 16
  %1001 = load i128, i128* %798, align 16
  %1002 = sub i128 162259276829213363382781917267968, %1001
  %1003 = add i128 %1002, %997
  %1004 = load i128, i128* %801, align 16
  %1005 = lshr i128 %1003, 64
  %1006 = sub i128 162259276829231810126851331852288, %1004
  %1007 = add i128 %1006, %998
  %1008 = add i128 %1007, %1005
  %1009 = and i128 %1003, 18446744073709551615
  %1010 = add nuw nsw i128 %1009, 18446673704965373952
  %1011 = lshr i128 %1008, 64
  %1012 = trunc i128 %1011 to i64
  %1013 = and i128 %1008, 18446744073709551615
  %1014 = sub nsw i128 %1013, %1011
  %1015 = shl nuw nsw i128 %1011, 32
  %1016 = add nsw i128 %1014, %1015
  %1017 = lshr i128 %1016, 64
  %1018 = trunc i128 %1017 to i64
  %1019 = add i64 %1018, %1012
  %1020 = and i128 %1016, 18446744073709551615
  %1021 = sub nsw i128 %1020, %1017
  %1022 = shl nuw nsw i128 %1017, 32
  %1023 = add nsw i128 %1021, %1022
  %1024 = zext i64 %1019 to i128
  %1025 = shl nuw nsw i128 %1024, 32
  %1026 = lshr i128 %1023, 1
  %1027 = trunc i128 %1026 to i64
  %1028 = trunc i128 %1023 to i64
  %1029 = and i64 %1028, 9223372036854775807
  %1030 = sub nsw i64 9223372032559808512, %1029
  %1031 = and i64 %1030, %1028
  %1032 = or i64 %1031, %1027
  %1033 = ashr i64 %1032, 63
  %1034 = zext i64 %1033 to i128
  %1035 = sub i128 162259276829231810126855626815487, %999
  %1036 = add i128 %1035, %995
  %1037 = add i128 %1036, %1024
  %1038 = sub i128 %1037, %1034
  %1039 = and i64 %1033, 4294967295
  %1040 = zext i64 %1039 to i128
  %1041 = and i64 %1033, -4294967295
  %1042 = zext i64 %1041 to i128
  %1043 = sub nsw i128 %1023, %1042
  %1044 = lshr i128 %1038, 64
  %1045 = sub i128 1460333491462920270524206387560447, %1000
  %1046 = add i128 %1045, %996
  %1047 = sub i128 %1046, %1025
  %1048 = sub i128 %1047, %1040
  %1049 = add i128 %1048, %1044
  %1050 = lshr i128 %1049, 64
  %1051 = add nuw nsw i128 %1010, %1050
  %1052 = lshr i128 %1051, 64
  %1053 = add nsw i128 %1043, %1052
  %1054 = and i128 %1038, 18446744073709551615
  %1055 = mul nuw i128 %1054, %743
  %1056 = lshr i128 %1055, 64
  %1057 = and i128 %1055, 18446744073709551615
  %1058 = and i128 %1049, 18446744073709551615
  %1059 = mul nuw i128 %1058, %743
  %1060 = lshr i128 %1059, 64
  %1061 = and i128 %1059, 18446744073709551615
  %1062 = mul nuw i128 %747, %1054
  %1063 = lshr i128 %1062, 64
  %1064 = and i128 %1062, 18446744073709551615
  %1065 = add nuw nsw i128 %1060, %1063
  %1066 = and i128 %1051, 18446744073709551615
  %1067 = mul nuw i128 %1066, %743
  %1068 = lshr i128 %1067, 64
  %1069 = and i128 %1067, 18446744073709551615
  %1070 = mul nuw i128 %1058, %747
  %1071 = lshr i128 %1070, 64
  %1072 = and i128 %1070, 18446744073709551615
  %1073 = mul nuw i128 %753, %1054
  %1074 = lshr i128 %1073, 64
  %1075 = and i128 %1073, 18446744073709551615
  %1076 = add nuw nsw i128 %1065, %1072
  %1077 = add nuw nsw i128 %1076, %1075
  %1078 = add nuw nsw i128 %1077, %1069
  %1079 = and i128 %1053, 18446744073709551615
  %1080 = mul nuw i128 %1079, %743
  %1081 = lshr i128 %1080, 64
  %1082 = and i128 %1080, 18446744073709551615
  %1083 = mul nuw i128 %1066, %747
  %1084 = lshr i128 %1083, 64
  %1085 = and i128 %1083, 18446744073709551615
  %1086 = mul nuw i128 %753, %1058
  %1087 = lshr i128 %1086, 64
  %1088 = and i128 %1086, 18446744073709551615
  %1089 = mul nuw i128 %759, %1054
  %1090 = lshr i128 %1089, 64
  %1091 = and i128 %1089, 18446744073709551615
  %1092 = mul nuw i128 %1079, %747
  %1093 = lshr i128 %1092, 64
  %1094 = and i128 %1092, 18446744073709551615
  %1095 = mul nuw i128 %1066, %753
  %1096 = lshr i128 %1095, 64
  %1097 = and i128 %1095, 18446744073709551615
  %1098 = mul nuw i128 %759, %1058
  %1099 = lshr i128 %1098, 64
  %1100 = and i128 %1098, 18446744073709551615
  %1101 = mul nuw i128 %1079, %753
  %1102 = lshr i128 %1101, 64
  %1103 = and i128 %1101, 18446744073709551615
  %1104 = mul nuw i128 %759, %1066
  %1105 = lshr i128 %1104, 64
  %1106 = and i128 %1104, 18446744073709551615
  %1107 = mul nuw i128 %1079, %759
  %1108 = lshr i128 %1107, 64
  %1109 = and i128 %1107, 18446744073709551615
  %1110 = mul nuw i128 %374, %374
  %1111 = lshr i128 %1110, 64
  %1112 = mul nuw i128 %384, %374
  %1113 = lshr i128 %1112, 64
  %1114 = shl i128 %1112, 1
  %1115 = and i128 %1114, 36893488147419103230
  %1116 = add nuw nsw i128 %1115, %1111
  %1117 = mul nuw i128 %396, %374
  %1118 = lshr i128 %1117, 64
  %1119 = and i128 %1117, 18446744073709551615
  %1120 = add nuw nsw i128 %1119, %1113
  %1121 = shl nuw nsw i128 %1120, 1
  %1122 = mul nuw i128 %411, %374
  %1123 = lshr i128 %1122, 64
  %1124 = and i128 %1122, 18446744073709551615
  %1125 = mul nuw i128 %396, %384
  %1126 = lshr i128 %1125, 64
  %1127 = and i128 %1125, 18446744073709551615
  %1128 = add nuw nsw i128 %1127, %1118
  %1129 = add nuw nsw i128 %1128, %1124
  %1130 = shl nuw nsw i128 %1129, 1
  %1131 = add nuw nsw i128 %1123, %1126
  %1132 = mul nuw i128 %384, %384
  %1133 = lshr i128 %1132, 64
  %1134 = and i128 %1132, 18446744073709551615
  %1135 = add nuw nsw i128 %1121, %1134
  %1136 = add nuw nsw i128 %1130, %1133
  %1137 = mul nuw i128 %411, %384
  %1138 = lshr i128 %1137, 64
  %1139 = and i128 %1137, 18446744073709551615
  %1140 = add nuw nsw i128 %1131, %1139
  %1141 = shl nuw nsw i128 %1140, 1
  %1142 = mul nuw i128 %411, %396
  %1143 = lshr i128 %1142, 64
  %1144 = and i128 %1142, 18446744073709551615
  %1145 = add nuw nsw i128 %1138, %1144
  %1146 = shl nuw nsw i128 %1145, 1
  %1147 = shl nuw nsw i128 %1143, 1
  %1148 = mul nuw i128 %396, %396
  %1149 = lshr i128 %1148, 64
  %1150 = and i128 %1148, 18446744073709551615
  %1151 = add nuw nsw i128 %1141, %1150
  %1152 = add nuw nsw i128 %1146, %1149
  %1153 = mul nuw i128 %411, %411
  %1154 = lshr i128 %1153, 64
  %1155 = and i128 %1153, 18446744073709551615
  %1156 = add nuw nsw i128 %1147, %1155
  %1157 = shl i128 %1110, 3
  %1158 = and i128 %1157, 147573952589676412920
  %1159 = shl nuw nsw i128 %1116, 3
  %1160 = shl nuw nsw i128 %1135, 3
  %1161 = shl nuw nsw i128 %1136, 3
  %1162 = shl nuw nsw i128 %1151, 3
  %1163 = shl nuw nsw i128 %1152, 3
  %1164 = shl nuw nsw i128 %1156, 3
  %1165 = shl nuw nsw i128 %1154, 3
  %1166 = or i128 %1078, 1180591620717411303424
  %1167 = sub nsw i128 1180591621816922931200, %1159
  %1168 = add nuw nsw i128 %1167, %1056
  %1169 = add nuw nsw i128 %1168, %1064
  %1170 = add nuw nsw i128 %1169, %1061
  %1171 = sub nsw i128 %1166, %1160
  %1172 = sub nsw i128 1180591620717411303360, %1162
  %1173 = add i128 %1172, %1087
  %1174 = add i128 %1173, %1084
  %1175 = add i128 %1174, %1090
  %1176 = add i128 %1175, %1097
  %1177 = add i128 %1176, %1100
  %1178 = add i128 %1177, %1081
  %1179 = add i128 %1178, %1094
  %1180 = sub nsw i128 1180591620717411303360, %1163
  %1181 = add nsw i128 %1180, %1096
  %1182 = add i128 %1181, %1099
  %1183 = add i128 %1182, %1106
  %1184 = add i128 %1183, %1093
  %1185 = add i128 %1184, %1103
  %1186 = sub nsw i128 1180591620717411303360, %1164
  %1187 = add nuw nsw i128 %1186, %1105
  %1188 = add nuw nsw i128 %1187, %1102
  %1189 = add i128 %1188, %1109
  %1190 = sub nsw i128 1180591620717411303360, %1165
  %1191 = add nuw nsw i128 %1190, %1108
  %1192 = sub nsw i128 40564819208483932466412890619200, %1158
  %1193 = add nuw nsw i128 %1192, %1057
  %1194 = or i128 %1170, 40564819207303340847894502572032
  %1195 = getelementptr inbounds i128, i128* %1, i64 1
  %1196 = add i128 %1171, 40564819207303340845695479316992
  %1197 = getelementptr inbounds i128, i128* %1, i64 2
  %1198 = sub nsw i128 40564819208483932465038501085760, %1161
  %1199 = add i128 %1198, %1071
  %1200 = add i128 %1199, %1074
  %1201 = add i128 %1200, %1088
  %1202 = add i128 %1201, %1068
  %1203 = add i128 %1202, %1085
  %1204 = add i128 %1203, %1091
  %1205 = add i128 %1204, %1082
  %1206 = getelementptr inbounds i128, i128* %1, i64 3
  %1207 = shl i128 %1185, 32
  %1208 = add i128 %1179, %1207
  %1209 = add i128 %1208, %1193
  %1210 = sub i128 %1205, %1208
  %1211 = sub i128 %1185, %1191
  %1212 = add i128 %1211, %1194
  %1213 = sub i128 %1196, %1211
  %1214 = shl i128 %1179, 32
  %1215 = sub i128 %1212, %1214
  %1216 = add i128 %1210, %1214
  %1217 = sub i128 %1213, %1207
  %1218 = sub i128 %1209, %1189
  %1219 = shl nsw i128 %1189, 32
  %1220 = sub i128 %1218, %1219
  %1221 = shl i128 %1189, 33
  %1222 = add i128 %1215, %1221
  store i128 %1222, i128* %1195, align 16
  %1223 = shl nsw i128 %1189, 1
  %1224 = add i128 %1217, %1223
  %1225 = sub i128 %1216, %1219
  %1226 = mul nsw i128 %1191, -4294967297
  %1227 = add i128 %1226, %1220
  store i128 %1227, i128* %1, align 16
  %1228 = shl nuw nsw i128 %1191, 33
  %1229 = add i128 %1224, %1228
  store i128 %1229, i128* %1197, align 16
  %1230 = mul nuw nsw i128 %1191, 3
  %1231 = add i128 %1225, %1230
  store i128 %1231, i128* %1206, align 16
  ret void
}

; Function Attrs: nounwind readonly
declare dso_local i32 @__gmpz_tstbit(%1*, i64) local_unnamed_addr #3

; Function Attrs: nounwind sspstrong uwtable
define internal fastcc void @1(i128* %0, i128* nocapture %1, i128* nocapture %2, i128* nocapture readonly %3, i128* nocapture readonly %4, i128* nocapture readonly %5, i64* nocapture readonly %6, i64* nocapture readonly %7, i64* nocapture readonly %8) unnamed_addr #0 {
  %10 = getelementptr inbounds i128, i128* %5, i64 3
  %11 = load i128, i128* %10, align 16
  %12 = add i128 %11, 18446744069414584320
  %13 = getelementptr inbounds i128, i128* %5, i64 2
  %14 = load i128, i128* %13, align 16
  %15 = lshr i128 %14, 64
  %16 = add i128 %12, %15
  %17 = and i128 %14, 18446744073709551615
  %18 = add nuw nsw i128 %17, 18446673704965373952
  %19 = load i128, i128* %5, align 16
  %20 = add i128 %19, 18446744073709551615
  %21 = getelementptr inbounds i128, i128* %5, i64 1
  %22 = load i128, i128* %21, align 16
  %23 = add i128 %22, 1298074214633706907132628377272319
  %24 = lshr i128 %16, 64
  %25 = trunc i128 %24 to i64
  %26 = and i128 %16, 18446744073709551615
  %27 = sub nsw i128 %26, %24
  %28 = shl nuw nsw i128 %24, 32
  %29 = add nsw i128 %27, %28
  %30 = lshr i128 %29, 64
  %31 = trunc i128 %30 to i64
  %32 = add i64 %31, %25
  %33 = and i128 %29, 18446744073709551615
  %34 = sub nsw i128 %33, %30
  %35 = shl nuw nsw i128 %30, 32
  %36 = add nsw i128 %34, %35
  %37 = zext i64 %32 to i128
  %38 = add i128 %20, %37
  %39 = shl nuw nsw i128 %37, 32
  %40 = sub i128 %23, %39
  %41 = lshr i128 %36, 1
  %42 = trunc i128 %41 to i64
  %43 = trunc i128 %36 to i64
  %44 = and i64 %43, 9223372036854775807
  %45 = sub nsw i64 9223372032559808512, %44
  %46 = and i64 %45, %43
  %47 = or i64 %46, %42
  %48 = ashr i64 %47, 63
  %49 = zext i64 %48 to i128
  %50 = sub i128 %38, %49
  %51 = and i64 %48, 4294967295
  %52 = zext i64 %51 to i128
  %53 = sub i128 %40, %52
  %54 = and i64 %48, -4294967295
  %55 = zext i64 %54 to i128
  %56 = sub nsw i128 %36, %55
  %57 = lshr i128 %50, 64
  %58 = add i128 %53, %57
  %59 = trunc i128 %50 to i64
  %60 = lshr i128 %58, 64
  %61 = add nuw nsw i128 %18, %60
  %62 = trunc i128 %58 to i64
  %63 = lshr i128 %61, 64
  %64 = add nsw i128 %63, %56
  %65 = trunc i128 %61 to i64
  %66 = trunc i128 %64 to i64
  %67 = or i128 %58, %50
  %68 = or i128 %67, %61
  %69 = or i128 %68, %64
  %70 = trunc i128 %69 to i64
  %71 = load i64, i64* %8, align 8
  %72 = getelementptr inbounds i64, i64* %8, i64 1
  %73 = load i64, i64* %72, align 8
  %74 = getelementptr inbounds i64, i64* %8, i64 2
  %75 = load i64, i64* %74, align 8
  %76 = getelementptr inbounds i64, i64* %8, i64 3
  %77 = load i64, i64* %76, align 8
  %78 = or i64 %73, %71
  %79 = or i64 %78, %75
  %80 = or i64 %79, %77
  %81 = insertelement <2 x i64> undef, i64 %70, i32 0
  %82 = insertelement <2 x i64> %81, i64 %80, i32 1
  %83 = add <2 x i64> %82, <i64 -1, i64 -1>
  %84 = shl <2 x i64> %83, <i64 32, i64 32>
  %85 = and <2 x i64> %84, %83
  %86 = shl <2 x i64> %85, <i64 16, i64 16>
  %87 = and <2 x i64> %86, %85
  %88 = insertelement <2 x i64> undef, i64 %59, i32 0
  %89 = insertelement <2 x i64> %88, i64 %71, i32 1
  %90 = xor <2 x i64> %89, <i64 -1, i64 -1>
  %91 = insertelement <2 x i64> undef, i64 %62, i32 0
  %92 = insertelement <2 x i64> %91, i64 %73, i32 1
  %93 = xor <2 x i64> %92, <i64 4294967295, i64 4294967295>
  %94 = or <2 x i64> %93, %90
  %95 = insertelement <2 x i64> undef, i64 %65, i32 0
  %96 = insertelement <2 x i64> %95, i64 %75, i32 1
  %97 = or <2 x i64> %94, %96
  %98 = insertelement <2 x i64> undef, i64 %66, i32 0
  %99 = insertelement <2 x i64> %98, i64 %77, i32 1
  %100 = xor <2 x i64> %99, <i64 -4294967295, i64 -4294967295>
  %101 = or <2 x i64> %97, %100
  %102 = add <2 x i64> %101, <i64 -1, i64 -1>
  %103 = shl <2 x i64> %102, <i64 32, i64 32>
  %104 = and <2 x i64> %103, %102
  %105 = shl <2 x i64> %104, <i64 16, i64 16>
  %106 = and <2 x i64> %105, %104
  %107 = shl <2 x i64> %87, <i64 8, i64 8>
  %108 = and <2 x i64> %107, %87
  %109 = shl <2 x i64> %108, <i64 4, i64 4>
  %110 = and <2 x i64> %109, %108
  %111 = shl <2 x i64> %110, <i64 2, i64 2>
  %112 = and <2 x i64> %111, %110
  %113 = shl <2 x i64> %112, <i64 1, i64 1>
  %114 = and <2 x i64> %113, %112
  %115 = shl <2 x i64> %106, <i64 8, i64 8>
  %116 = and <2 x i64> %115, %106
  %117 = shl <2 x i64> %116, <i64 4, i64 4>
  %118 = and <2 x i64> %117, %116
  %119 = shl <2 x i64> %118, <i64 2, i64 2>
  %120 = and <2 x i64> %119, %118
  %121 = shl <2 x i64> %120, <i64 1, i64 1>
  %122 = and <2 x i64> %121, %120
  %123 = or <2 x i64> %122, %114
  %124 = ashr <2 x i64> %123, <i64 63, i64 63>
  %125 = zext <2 x i64> %124 to <2 x i128>
  %126 = shl nuw <2 x i128> %125, <i128 64, i128 64>
  %127 = or <2 x i128> %126, %125
  %128 = and i128 %50, 18446744073709551615
  %129 = mul nuw i128 %128, %128
  %130 = lshr i128 %129, 64
  %131 = and i128 %129, 18446744073709551615
  %132 = and i128 %58, 18446744073709551615
  %133 = mul nuw i128 %132, %128
  %134 = lshr i128 %133, 64
  %135 = shl i128 %133, 1
  %136 = and i128 %135, 36893488147419103230
  %137 = add nuw nsw i128 %136, %130
  %138 = and i128 %61, 18446744073709551615
  %139 = mul nuw i128 %138, %128
  %140 = lshr i128 %139, 64
  %141 = and i128 %139, 18446744073709551615
  %142 = add nuw nsw i128 %141, %134
  %143 = and i128 %64, 18446744073709551615
  %144 = mul nuw i128 %143, %128
  %145 = lshr i128 %144, 64
  %146 = and i128 %144, 18446744073709551615
  %147 = mul nuw i128 %138, %132
  %148 = lshr i128 %147, 64
  %149 = and i128 %147, 18446744073709551615
  %150 = add nuw nsw i128 %149, %140
  %151 = add nuw nsw i128 %150, %146
  %152 = shl nuw nsw i128 %151, 1
  %153 = add nuw nsw i128 %145, %148
  %154 = mul nuw i128 %132, %132
  %155 = lshr i128 %154, 64
  %156 = and i128 %154, 18446744073709551615
  %157 = mul nuw i128 %143, %132
  %158 = lshr i128 %157, 64
  %159 = and i128 %157, 18446744073709551615
  %160 = add nuw nsw i128 %153, %159
  %161 = shl nuw nsw i128 %160, 1
  %162 = mul nuw i128 %143, %138
  %163 = lshr i128 %162, 64
  %164 = and i128 %162, 18446744073709551615
  %165 = add nuw nsw i128 %158, %164
  %166 = shl nuw nsw i128 %165, 1
  %167 = shl nuw nsw i128 %163, 1
  %168 = mul nuw i128 %138, %138
  %169 = lshr i128 %168, 64
  %170 = and i128 %168, 18446744073709551615
  %171 = add nuw nsw i128 %161, %170
  %172 = add nuw nsw i128 %166, %169
  %173 = mul nuw i128 %143, %143
  %174 = lshr i128 %173, 64
  %175 = and i128 %173, 18446744073709551615
  %176 = add nuw nsw i128 %167, %175
  %177 = add nuw nsw i128 %131, 1267650600228229401427983728624
  %178 = or i128 %137, 1267650600228229401496703205376
  %179 = shl nuw nsw i128 %172, 32
  %180 = add nuw nsw i128 %179, %171
  %181 = sub nsw i128 %172, %174
  %182 = shl nuw nsw i128 %171, 32
  %183 = shl nuw nsw i128 %176, 32
  %184 = shl nuw nsw i128 %176, 33
  %185 = add nuw nsw i128 %184, %178
  %186 = add nsw i128 %185, %181
  %187 = sub nsw i128 %186, %182
  %188 = shl nuw nsw i128 %174, 32
  %189 = sub nsw i128 %177, %174
  %190 = sub nsw i128 %189, %188
  %191 = sub nsw i128 %190, %176
  %192 = sub nsw i128 %191, %183
  %193 = add nsw i128 %192, %180
  %194 = shl nuw nsw i128 %174, 33
  %195 = add nuw nsw i128 %142, %176
  %196 = shl nuw nsw i128 %195, 1
  %197 = add nuw nsw i128 %156, 1267650600228229401427983728656
  %198 = add nuw nsw i128 %197, %194
  %199 = sub nsw i128 %198, %181
  %200 = sub nsw i128 %199, %179
  %201 = add i128 %200, %196
  %202 = mul nuw nsw i128 %174, 3
  %203 = add nuw nsw i128 %155, 1267650600228229401427983728656
  %204 = add nuw nsw i128 %203, %202
  %205 = add nuw nsw i128 %204, %152
  %206 = sub nsw i128 %205, %183
  %207 = sub nsw i128 %206, %180
  %208 = add i128 %207, %182
  %209 = add i128 %208, 18446744069414584320
  %210 = lshr i128 %201, 64
  %211 = add i128 %209, %210
  %212 = and i128 %201, 18446744073709551615
  %213 = add nuw nsw i128 %212, 18446673704965373952
  %214 = add i128 %193, 18446744073709551615
  %215 = add i128 %187, 1298074214633706907132628377272319
  %216 = lshr i128 %211, 64
  %217 = trunc i128 %216 to i64
  %218 = and i128 %211, 18446744073709551615
  %219 = sub nsw i128 %218, %216
  %220 = shl nuw nsw i128 %216, 32
  %221 = add nsw i128 %219, %220
  %222 = lshr i128 %221, 64
  %223 = trunc i128 %222 to i64
  %224 = add i64 %223, %217
  %225 = and i128 %221, 18446744073709551615
  %226 = sub nsw i128 %225, %222
  %227 = shl nuw nsw i128 %222, 32
  %228 = add nsw i128 %226, %227
  %229 = zext i64 %224 to i128
  %230 = add i128 %214, %229
  %231 = shl nuw nsw i128 %229, 32
  %232 = sub i128 %215, %231
  %233 = lshr i128 %228, 1
  %234 = trunc i128 %233 to i64
  %235 = trunc i128 %228 to i64
  %236 = and i64 %235, 9223372036854775807
  %237 = sub nsw i64 9223372032559808512, %236
  %238 = and i64 %237, %235
  %239 = or i64 %238, %234
  %240 = ashr i64 %239, 63
  %241 = zext i64 %240 to i128
  %242 = sub i128 %230, %241
  %243 = and i64 %240, 4294967295
  %244 = zext i64 %243 to i128
  %245 = sub i128 %232, %244
  %246 = and i64 %240, -4294967295
  %247 = zext i64 %246 to i128
  %248 = sub nsw i128 %228, %247
  %249 = lshr i128 %242, 64
  %250 = add i128 %245, %249
  %251 = lshr i128 %250, 64
  %252 = add nuw nsw i128 %213, %251
  %253 = lshr i128 %252, 64
  %254 = add nsw i128 %248, %253
  %255 = zext i64 %71 to i128
  %256 = mul nuw i128 %255, %255
  %257 = lshr i128 %256, 64
  %258 = and i128 %256, 18446744073709551615
  %259 = zext i64 %73 to i128
  %260 = mul nuw i128 %259, %255
  %261 = lshr i128 %260, 64
  %262 = shl i128 %260, 1
  %263 = and i128 %262, 36893488147419103230
  %264 = add nuw nsw i128 %263, %257
  %265 = zext i64 %75 to i128
  %266 = mul nuw i128 %265, %255
  %267 = lshr i128 %266, 64
  %268 = and i128 %266, 18446744073709551615
  %269 = add nuw nsw i128 %268, %261
  %270 = zext i64 %77 to i128
  %271 = mul nuw i128 %270, %255
  %272 = lshr i128 %271, 64
  %273 = and i128 %271, 18446744073709551615
  %274 = mul nuw i128 %265, %259
  %275 = lshr i128 %274, 64
  %276 = and i128 %274, 18446744073709551615
  %277 = add nuw nsw i128 %276, %267
  %278 = add nuw nsw i128 %277, %273
  %279 = shl nuw nsw i128 %278, 1
  %280 = add nuw nsw i128 %272, %275
  %281 = mul nuw i128 %259, %259
  %282 = lshr i128 %281, 64
  %283 = and i128 %281, 18446744073709551615
  %284 = mul nuw i128 %270, %259
  %285 = lshr i128 %284, 64
  %286 = and i128 %284, 18446744073709551615
  %287 = add nuw nsw i128 %280, %286
  %288 = shl nuw nsw i128 %287, 1
  %289 = mul nuw i128 %270, %265
  %290 = lshr i128 %289, 64
  %291 = and i128 %289, 18446744073709551615
  %292 = add nuw nsw i128 %285, %291
  %293 = shl nuw nsw i128 %292, 1
  %294 = shl nuw nsw i128 %290, 1
  %295 = mul nuw i128 %265, %265
  %296 = lshr i128 %295, 64
  %297 = and i128 %295, 18446744073709551615
  %298 = add nuw nsw i128 %288, %297
  %299 = add nuw nsw i128 %293, %296
  %300 = mul nuw i128 %270, %270
  %301 = lshr i128 %300, 64
  %302 = and i128 %300, 18446744073709551615
  %303 = add nuw nsw i128 %294, %302
  %304 = add nuw nsw i128 %258, 1267650600228229401427983728624
  %305 = or i128 %264, 1267650600228229401496703205376
  %306 = shl nuw nsw i128 %299, 32
  %307 = add nuw nsw i128 %306, %298
  %308 = sub nsw i128 %299, %301
  %309 = shl nuw nsw i128 %298, 32
  %310 = shl nuw nsw i128 %303, 32
  %311 = shl nuw nsw i128 %303, 33
  %312 = add nuw nsw i128 %311, %305
  %313 = add nsw i128 %312, %308
  %314 = sub nsw i128 %313, %309
  %315 = shl nuw nsw i128 %301, 32
  %316 = sub nsw i128 %304, %301
  %317 = sub nsw i128 %316, %315
  %318 = sub nsw i128 %317, %303
  %319 = sub nsw i128 %318, %310
  %320 = add nsw i128 %319, %307
  %321 = shl nuw nsw i128 %301, 33
  %322 = add nuw nsw i128 %269, %303
  %323 = shl nuw nsw i128 %322, 1
  %324 = add nuw nsw i128 %283, 1267650600228229401427983728656
  %325 = add nuw nsw i128 %324, %321
  %326 = sub nsw i128 %325, %308
  %327 = sub nsw i128 %326, %306
  %328 = add i128 %327, %323
  %329 = mul nuw nsw i128 %301, 3
  %330 = add nuw nsw i128 %282, 1267650600228229401427983728656
  %331 = add nuw nsw i128 %330, %329
  %332 = add nuw nsw i128 %331, %279
  %333 = sub nsw i128 %332, %310
  %334 = sub nsw i128 %333, %307
  %335 = add i128 %334, %309
  %336 = add i128 %335, 18446744069414584320
  %337 = lshr i128 %328, 64
  %338 = add i128 %336, %337
  %339 = and i128 %328, 18446744073709551615
  %340 = add nuw nsw i128 %339, 18446673704965373952
  %341 = add i128 %320, 18446744073709551615
  %342 = add i128 %314, 1298074214633706907132628377272319
  %343 = lshr i128 %338, 64
  %344 = trunc i128 %343 to i64
  %345 = and i128 %338, 18446744073709551615
  %346 = sub nsw i128 %345, %343
  %347 = shl nuw nsw i128 %343, 32
  %348 = add nsw i128 %346, %347
  %349 = lshr i128 %348, 64
  %350 = trunc i128 %349 to i64
  %351 = add i64 %350, %344
  %352 = and i128 %348, 18446744073709551615
  %353 = sub nsw i128 %352, %349
  %354 = shl nuw nsw i128 %349, 32
  %355 = add nsw i128 %353, %354
  %356 = zext i64 %351 to i128
  %357 = add i128 %341, %356
  %358 = shl nuw nsw i128 %356, 32
  %359 = sub i128 %342, %358
  %360 = lshr i128 %355, 1
  %361 = trunc i128 %360 to i64
  %362 = trunc i128 %355 to i64
  %363 = and i64 %362, 9223372036854775807
  %364 = sub nsw i64 9223372032559808512, %363
  %365 = and i64 %364, %362
  %366 = or i64 %365, %361
  %367 = ashr i64 %366, 63
  %368 = zext i64 %367 to i128
  %369 = sub i128 %357, %368
  %370 = and i64 %367, 4294967295
  %371 = zext i64 %370 to i128
  %372 = sub i128 %359, %371
  %373 = and i64 %367, -4294967295
  %374 = zext i64 %373 to i128
  %375 = sub nsw i128 %355, %374
  %376 = lshr i128 %369, 64
  %377 = add i128 %372, %376
  %378 = lshr i128 %377, 64
  %379 = add nuw nsw i128 %340, %378
  %380 = lshr i128 %379, 64
  %381 = add nsw i128 %375, %380
  %382 = getelementptr inbounds i128, i128* %3, i64 3
  %383 = load i128, i128* %382, align 16
  %384 = add i128 %383, 18446744069414584320
  %385 = getelementptr inbounds i128, i128* %3, i64 2
  %386 = load i128, i128* %385, align 16
  %387 = lshr i128 %386, 64
  %388 = add i128 %384, %387
  %389 = and i128 %386, 18446744073709551615
  %390 = add nuw nsw i128 %389, 18446673704965373952
  %391 = load i128, i128* %3, align 16
  %392 = add i128 %391, 18446744073709551615
  %393 = getelementptr inbounds i128, i128* %3, i64 1
  %394 = load i128, i128* %393, align 16
  %395 = add i128 %394, 1298074214633706907132628377272319
  %396 = lshr i128 %388, 64
  %397 = trunc i128 %396 to i64
  %398 = and i128 %388, 18446744073709551615
  %399 = sub nsw i128 %398, %396
  %400 = shl nuw nsw i128 %396, 32
  %401 = add nsw i128 %399, %400
  %402 = lshr i128 %401, 64
  %403 = trunc i128 %402 to i64
  %404 = add i64 %403, %397
  %405 = and i128 %401, 18446744073709551615
  %406 = sub nsw i128 %405, %402
  %407 = shl nuw nsw i128 %402, 32
  %408 = add nsw i128 %406, %407
  %409 = zext i64 %404 to i128
  %410 = add i128 %392, %409
  %411 = shl nuw nsw i128 %409, 32
  %412 = sub i128 %395, %411
  %413 = lshr i128 %408, 1
  %414 = trunc i128 %413 to i64
  %415 = trunc i128 %408 to i64
  %416 = and i64 %415, 9223372036854775807
  %417 = sub nsw i64 9223372032559808512, %416
  %418 = and i64 %417, %415
  %419 = or i64 %418, %414
  %420 = ashr i64 %419, 63
  %421 = zext i64 %420 to i128
  %422 = sub i128 %410, %421
  %423 = and i64 %420, 4294967295
  %424 = zext i64 %423 to i128
  %425 = sub i128 %412, %424
  %426 = and i64 %420, -4294967295
  %427 = zext i64 %426 to i128
  %428 = sub nsw i128 %408, %427
  %429 = lshr i128 %422, 64
  %430 = add i128 %425, %429
  %431 = lshr i128 %430, 64
  %432 = add nuw nsw i128 %390, %431
  %433 = lshr i128 %432, 64
  %434 = add nsw i128 %428, %433
  %435 = and i128 %422, 18446744073709551615
  %436 = and i128 %369, 18446744073709551615
  %437 = mul nuw i128 %436, %435
  %438 = lshr i128 %437, 64
  %439 = and i128 %437, 18446744073709551615
  %440 = and i128 %377, 18446744073709551615
  %441 = mul nuw i128 %440, %435
  %442 = lshr i128 %441, 64
  %443 = and i128 %441, 18446744073709551615
  %444 = and i128 %430, 18446744073709551615
  %445 = mul nuw i128 %436, %444
  %446 = lshr i128 %445, 64
  %447 = and i128 %445, 18446744073709551615
  %448 = add nuw nsw i128 %447, %438
  %449 = add nuw nsw i128 %448, %443
  %450 = and i128 %379, 18446744073709551615
  %451 = mul nuw i128 %450, %435
  %452 = lshr i128 %451, 64
  %453 = and i128 %451, 18446744073709551615
  %454 = mul nuw i128 %440, %444
  %455 = lshr i128 %454, 64
  %456 = and i128 %454, 18446744073709551615
  %457 = and i128 %432, 18446744073709551615
  %458 = mul nuw i128 %436, %457
  %459 = lshr i128 %458, 64
  %460 = and i128 %458, 18446744073709551615
  %461 = and i128 %381, 18446744073709551615
  %462 = mul nuw i128 %461, %435
  %463 = lshr i128 %462, 64
  %464 = and i128 %462, 18446744073709551615
  %465 = mul nuw i128 %450, %444
  %466 = lshr i128 %465, 64
  %467 = and i128 %465, 18446744073709551615
  %468 = mul nuw i128 %440, %457
  %469 = lshr i128 %468, 64
  %470 = and i128 %468, 18446744073709551615
  %471 = and i128 %434, 18446744073709551615
  %472 = mul nuw i128 %436, %471
  %473 = lshr i128 %472, 64
  %474 = and i128 %472, 18446744073709551615
  %475 = mul nuw i128 %461, %444
  %476 = lshr i128 %475, 64
  %477 = and i128 %475, 18446744073709551615
  %478 = mul nuw i128 %450, %457
  %479 = lshr i128 %478, 64
  %480 = and i128 %478, 18446744073709551615
  %481 = mul nuw i128 %440, %471
  %482 = lshr i128 %481, 64
  %483 = and i128 %481, 18446744073709551615
  %484 = add nuw nsw i128 %469, %473
  %485 = add nuw nsw i128 %484, %483
  %486 = add nuw nsw i128 %485, %466
  %487 = add nuw nsw i128 %486, %480
  %488 = add nuw nsw i128 %487, %463
  %489 = add nuw nsw i128 %488, %477
  %490 = mul nuw i128 %461, %457
  %491 = lshr i128 %490, 64
  %492 = and i128 %490, 18446744073709551615
  %493 = mul nuw i128 %450, %471
  %494 = lshr i128 %493, 64
  %495 = and i128 %493, 18446744073709551615
  %496 = add nuw nsw i128 %479, %482
  %497 = add nuw nsw i128 %496, %495
  %498 = add nuw nsw i128 %497, %476
  %499 = add nuw nsw i128 %498, %492
  %500 = add nuw nsw i128 %491, %494
  %501 = mul nuw i128 %461, %471
  %502 = lshr i128 %501, 64
  %503 = and i128 %501, 18446744073709551615
  %504 = add nuw nsw i128 %500, %503
  %505 = add nuw nsw i128 %439, 1267650600228229401427983728624
  %506 = or i128 %449, 1267650600228229401496703205376
  %507 = shl nuw nsw i128 %499, 32
  %508 = add i128 %489, %507
  %509 = sub nsw i128 %499, %502
  %510 = shl i128 %489, 32
  %511 = shl nuw nsw i128 %504, 32
  %512 = shl nuw nsw i128 %504, 33
  %513 = add nuw nsw i128 %512, %506
  %514 = add nsw i128 %513, %509
  %515 = sub i128 %514, %510
  %516 = shl nuw nsw i128 %504, 1
  %517 = shl nuw nsw i128 %502, 32
  %518 = sub nsw i128 %505, %502
  %519 = sub nsw i128 %518, %517
  %520 = sub nsw i128 %519, %504
  %521 = sub nsw i128 %520, %511
  %522 = add i128 %521, %508
  %523 = shl nuw nsw i128 %502, 33
  %524 = add nuw nsw i128 %446, 1267650600228229401427983728656
  %525 = add nuw nsw i128 %524, %460
  %526 = add nuw nsw i128 %525, %442
  %527 = add nuw nsw i128 %526, %456
  %528 = add nuw nsw i128 %527, %453
  %529 = add nuw nsw i128 %528, %523
  %530 = add i128 %529, %516
  %531 = sub i128 %530, %509
  %532 = sub i128 %531, %507
  %533 = mul nuw nsw i128 %502, 3
  %534 = add nuw nsw i128 %459, 1267650600228229401427983728656
  %535 = add nuw nsw i128 %534, %474
  %536 = add nuw nsw i128 %535, %455
  %537 = add nuw nsw i128 %536, %470
  %538 = add nuw nsw i128 %537, %452
  %539 = add nuw nsw i128 %538, %467
  %540 = add i128 %539, %464
  %541 = add i128 %540, %533
  %542 = sub i128 %541, %511
  %543 = sub i128 %542, %508
  %544 = add i128 %543, %510
  %545 = add i128 %14, %265
  %546 = lshr i128 %545, 64
  %547 = add i128 %12, %270
  %548 = add i128 %547, %546
  %549 = and i128 %545, 18446744073709551615
  %550 = add nuw nsw i128 %549, 18446673704965373952
  %551 = lshr i128 %548, 64
  %552 = trunc i128 %551 to i64
  %553 = and i128 %548, 18446744073709551615
  %554 = sub nsw i128 %553, %551
  %555 = shl nuw nsw i128 %551, 32
  %556 = add nsw i128 %554, %555
  %557 = lshr i128 %556, 64
  %558 = trunc i128 %557 to i64
  %559 = add i64 %558, %552
  %560 = and i128 %556, 18446744073709551615
  %561 = sub nsw i128 %560, %557
  %562 = shl nuw nsw i128 %557, 32
  %563 = add nsw i128 %561, %562
  %564 = zext i64 %559 to i128
  %565 = shl nuw nsw i128 %564, 32
  %566 = lshr i128 %563, 1
  %567 = trunc i128 %566 to i64
  %568 = trunc i128 %563 to i64
  %569 = and i64 %568, 9223372036854775807
  %570 = sub nsw i64 9223372032559808512, %569
  %571 = and i64 %570, %568
  %572 = or i64 %571, %567
  %573 = ashr i64 %572, 63
  %574 = zext i64 %573 to i128
  %575 = add i128 %20, %255
  %576 = add i128 %575, %564
  %577 = sub i128 %576, %574
  %578 = and i64 %573, 4294967295
  %579 = zext i64 %578 to i128
  %580 = and i64 %573, -4294967295
  %581 = zext i64 %580 to i128
  %582 = sub nsw i128 %563, %581
  %583 = lshr i128 %577, 64
  %584 = add i128 %23, %259
  %585 = sub i128 %584, %565
  %586 = sub i128 %585, %579
  %587 = add i128 %586, %583
  %588 = lshr i128 %587, 64
  %589 = add nuw nsw i128 %550, %588
  %590 = lshr i128 %589, 64
  %591 = add nsw i128 %582, %590
  %592 = and i128 %577, 18446744073709551615
  %593 = mul nuw i128 %592, %592
  %594 = lshr i128 %593, 64
  %595 = and i128 %593, 18446744073709551615
  %596 = and i128 %587, 18446744073709551615
  %597 = mul nuw i128 %596, %592
  %598 = lshr i128 %597, 64
  %599 = shl i128 %597, 1
  %600 = and i128 %599, 36893488147419103230
  %601 = add nuw nsw i128 %600, %594
  %602 = and i128 %589, 18446744073709551615
  %603 = mul nuw i128 %602, %592
  %604 = lshr i128 %603, 64
  %605 = and i128 %603, 18446744073709551615
  %606 = add nuw nsw i128 %605, %598
  %607 = and i128 %591, 18446744073709551615
  %608 = mul nuw i128 %607, %592
  %609 = lshr i128 %608, 64
  %610 = and i128 %608, 18446744073709551615
  %611 = mul nuw i128 %602, %596
  %612 = lshr i128 %611, 64
  %613 = and i128 %611, 18446744073709551615
  %614 = add nuw nsw i128 %613, %604
  %615 = add nuw nsw i128 %614, %610
  %616 = shl nuw nsw i128 %615, 1
  %617 = add nuw nsw i128 %609, %612
  %618 = mul nuw i128 %596, %596
  %619 = lshr i128 %618, 64
  %620 = and i128 %618, 18446744073709551615
  %621 = mul nuw i128 %607, %596
  %622 = lshr i128 %621, 64
  %623 = and i128 %621, 18446744073709551615
  %624 = add nuw nsw i128 %617, %623
  %625 = shl nuw nsw i128 %624, 1
  %626 = mul nuw i128 %607, %602
  %627 = lshr i128 %626, 64
  %628 = and i128 %626, 18446744073709551615
  %629 = add nuw nsw i128 %622, %628
  %630 = shl nuw nsw i128 %629, 1
  %631 = shl nuw nsw i128 %627, 1
  %632 = mul nuw i128 %602, %602
  %633 = lshr i128 %632, 64
  %634 = and i128 %632, 18446744073709551615
  %635 = add nuw nsw i128 %625, %634
  %636 = add nuw nsw i128 %630, %633
  %637 = mul nuw i128 %607, %607
  %638 = lshr i128 %637, 64
  %639 = and i128 %637, 18446744073709551615
  %640 = add nuw nsw i128 %631, %639
  %641 = or i128 %601, 1267650600228229401496703205376
  %642 = shl nuw nsw i128 %636, 32
  %643 = add nuw nsw i128 %642, %635
  %644 = sub nsw i128 %636, %638
  %645 = shl nuw nsw i128 %635, 32
  %646 = shl nuw nsw i128 %640, 32
  %647 = shl nuw nsw i128 %640, 33
  %648 = shl nuw nsw i128 %638, 32
  %649 = shl nuw nsw i128 %638, 33
  %650 = mul nuw nsw i128 %638, 3
  %651 = add nuw nsw i128 %606, %640
  %652 = shl nuw nsw i128 %651, 1
  %653 = sub i128 41832469807531570247123463045648, %328
  %654 = add i128 %653, %620
  %655 = sub i128 %654, %201
  %656 = add i128 %655, %649
  %657 = sub i128 %656, %644
  %658 = sub i128 %657, %642
  %659 = add i128 %658, %652
  %660 = mul nuw i128 %436, %255
  %661 = lshr i128 %660, 64
  %662 = and i128 %660, 18446744073709551615
  %663 = mul nuw i128 %436, %259
  %664 = lshr i128 %663, 64
  %665 = and i128 %663, 18446744073709551615
  %666 = add nuw nsw i128 %665, %661
  %667 = mul nuw i128 %440, %255
  %668 = lshr i128 %667, 64
  %669 = and i128 %667, 18446744073709551615
  %670 = add nuw nsw i128 %666, %669
  %671 = mul nuw i128 %436, %265
  %672 = lshr i128 %671, 64
  %673 = and i128 %671, 18446744073709551615
  %674 = mul nuw i128 %440, %259
  %675 = lshr i128 %674, 64
  %676 = and i128 %674, 18446744073709551615
  %677 = mul nuw i128 %450, %255
  %678 = lshr i128 %677, 64
  %679 = and i128 %677, 18446744073709551615
  %680 = mul nuw i128 %436, %270
  %681 = lshr i128 %680, 64
  %682 = and i128 %680, 18446744073709551615
  %683 = mul nuw i128 %440, %265
  %684 = lshr i128 %683, 64
  %685 = and i128 %683, 18446744073709551615
  %686 = add nuw nsw i128 %684, %681
  %687 = mul nuw i128 %450, %259
  %688 = lshr i128 %687, 64
  %689 = and i128 %687, 18446744073709551615
  %690 = mul nuw i128 %461, %255
  %691 = lshr i128 %690, 64
  %692 = and i128 %690, 18446744073709551615
  %693 = mul nuw i128 %440, %270
  %694 = lshr i128 %693, 64
  %695 = and i128 %693, 18446744073709551615
  %696 = mul nuw i128 %450, %265
  %697 = lshr i128 %696, 64
  %698 = and i128 %696, 18446744073709551615
  %699 = add nuw nsw i128 %697, %694
  %700 = mul nuw i128 %461, %259
  %701 = lshr i128 %700, 64
  %702 = and i128 %700, 18446744073709551615
  %703 = add nuw nsw i128 %686, %695
  %704 = add nuw nsw i128 %703, %688
  %705 = add nuw nsw i128 %704, %698
  %706 = add nuw nsw i128 %705, %691
  %707 = add nuw nsw i128 %706, %702
  %708 = mul nuw i128 %450, %270
  %709 = lshr i128 %708, 64
  %710 = and i128 %708, 18446744073709551615
  %711 = add nuw nsw i128 %699, %710
  %712 = add nuw nsw i128 %711, %701
  %713 = mul nuw i128 %461, %265
  %714 = lshr i128 %713, 64
  %715 = and i128 %713, 18446744073709551615
  %716 = add nuw nsw i128 %712, %715
  %717 = add nuw nsw i128 %714, %709
  %718 = mul nuw i128 %461, %270
  %719 = lshr i128 %718, 64
  %720 = and i128 %718, 18446744073709551615
  %721 = add nuw nsw i128 %717, %720
  %722 = or i128 %670, 1267650600228229401496703205376
  %723 = shl nuw nsw i128 %716, 32
  %724 = add i128 %723, %707
  %725 = sub nsw i128 %716, %719
  %726 = shl i128 %707, 32
  %727 = shl nuw nsw i128 %721, 32
  %728 = shl nuw nsw i128 %721, 33
  %729 = shl nuw nsw i128 %721, 1
  %730 = shl nuw nsw i128 %719, 32
  %731 = shl nuw nsw i128 %719, 33
  %732 = add nuw nsw i128 %664, 1267650600228229401427983728656
  %733 = add nuw nsw i128 %732, %673
  %734 = add nuw nsw i128 %733, %668
  %735 = add nuw nsw i128 %734, %676
  %736 = add nuw nsw i128 %735, %679
  %737 = add nuw nsw i128 %736, %731
  %738 = sub i128 %737, %725
  %739 = sub i128 %738, %723
  %740 = add i128 %739, %729
  %741 = mul nuw nsw i128 %719, 3
  %742 = getelementptr inbounds i128, i128* %4, i64 3
  %743 = load i128, i128* %742, align 16
  %744 = add i128 %743, 18446744069414584320
  %745 = getelementptr inbounds i128, i128* %4, i64 2
  %746 = load i128, i128* %745, align 16
  %747 = lshr i128 %746, 64
  %748 = add i128 %744, %747
  %749 = and i128 %746, 18446744073709551615
  %750 = add nuw nsw i128 %749, 18446673704965373952
  %751 = load i128, i128* %4, align 16
  %752 = add i128 %751, 18446744073709551615
  %753 = getelementptr inbounds i128, i128* %4, i64 1
  %754 = load i128, i128* %753, align 16
  %755 = add i128 %754, 1298074214633706907132628377272319
  %756 = lshr i128 %748, 64
  %757 = trunc i128 %756 to i64
  %758 = and i128 %748, 18446744073709551615
  %759 = sub nsw i128 %758, %756
  %760 = shl nuw nsw i128 %756, 32
  %761 = add nsw i128 %759, %760
  %762 = lshr i128 %761, 64
  %763 = trunc i128 %762 to i64
  %764 = add i64 %763, %757
  %765 = and i128 %761, 18446744073709551615
  %766 = sub nsw i128 %765, %762
  %767 = shl nuw nsw i128 %762, 32
  %768 = add nsw i128 %766, %767
  %769 = zext i64 %764 to i128
  %770 = add i128 %752, %769
  %771 = shl nuw nsw i128 %769, 32
  %772 = sub i128 %755, %771
  %773 = lshr i128 %768, 1
  %774 = trunc i128 %773 to i64
  %775 = trunc i128 %768 to i64
  %776 = and i64 %775, 9223372036854775807
  %777 = sub nsw i64 9223372032559808512, %776
  %778 = and i64 %777, %775
  %779 = or i64 %778, %774
  %780 = ashr i64 %779, 63
  %781 = zext i64 %780 to i128
  %782 = sub i128 %770, %781
  %783 = and i64 %780, 4294967295
  %784 = zext i64 %783 to i128
  %785 = sub i128 %772, %784
  %786 = and i64 %780, -4294967295
  %787 = zext i64 %786 to i128
  %788 = sub nsw i128 %768, %787
  %789 = lshr i128 %782, 64
  %790 = add i128 %785, %789
  %791 = lshr i128 %790, 64
  %792 = add nuw nsw i128 %750, %791
  %793 = lshr i128 %792, 64
  %794 = add nsw i128 %788, %793
  %795 = lshr i128 %740, 64
  %796 = add nuw nsw i128 %672, 1267650600246676145497398312976
  %797 = add nuw nsw i128 %796, %682
  %798 = add nuw nsw i128 %797, %675
  %799 = add nuw nsw i128 %798, %685
  %800 = add nuw nsw i128 %799, %678
  %801 = add nuw nsw i128 %800, %689
  %802 = add i128 %801, %692
  %803 = add i128 %802, %741
  %804 = sub i128 %803, %727
  %805 = sub i128 %804, %724
  %806 = add i128 %805, %726
  %807 = add i128 %806, %795
  %808 = and i128 %740, 18446744073709551615
  %809 = add nuw nsw i128 %808, 18446673704965373952
  %810 = lshr i128 %807, 64
  %811 = trunc i128 %810 to i64
  %812 = and i128 %807, 18446744073709551615
  %813 = sub nsw i128 %812, %810
  %814 = shl nuw nsw i128 %810, 32
  %815 = add nsw i128 %813, %814
  %816 = lshr i128 %815, 64
  %817 = trunc i128 %816 to i64
  %818 = add i64 %817, %811
  %819 = and i128 %815, 18446744073709551615
  %820 = sub nsw i128 %819, %816
  %821 = shl nuw nsw i128 %816, 32
  %822 = add nsw i128 %820, %821
  %823 = zext i64 %818 to i128
  %824 = shl nuw nsw i128 %823, 32
  %825 = lshr i128 %822, 1
  %826 = trunc i128 %825 to i64
  %827 = trunc i128 %822 to i64
  %828 = and i64 %827, 9223372036854775807
  %829 = sub nsw i64 9223372032559808512, %828
  %830 = and i64 %829, %827
  %831 = or i64 %830, %826
  %832 = ashr i64 %831, 63
  %833 = zext i64 %832 to i128
  %834 = add nuw nsw i128 %662, 1267650600246676145501693280239
  %835 = sub nsw i128 %834, %719
  %836 = sub nsw i128 %835, %730
  %837 = sub nsw i128 %836, %721
  %838 = sub nsw i128 %837, %727
  %839 = add i128 %838, %724
  %840 = add i128 %839, %823
  %841 = sub i128 %840, %833
  %842 = and i64 %832, 4294967295
  %843 = zext i64 %842 to i128
  %844 = and i64 %832, -4294967295
  %845 = zext i64 %844 to i128
  %846 = sub nsw i128 %822, %845
  %847 = lshr i128 %841, 64
  %848 = add nuw nsw i128 %722, 1298074214633706907132628377272319
  %849 = add nsw i128 %848, %725
  %850 = add i128 %849, %728
  %851 = sub i128 %850, %726
  %852 = sub i128 %851, %824
  %853 = sub i128 %852, %843
  %854 = add i128 %853, %847
  %855 = lshr i128 %854, 64
  %856 = add nuw nsw i128 %809, %855
  %857 = lshr i128 %856, 64
  %858 = add nsw i128 %846, %857
  %859 = and i128 %782, 18446744073709551615
  %860 = and i128 %841, 18446744073709551615
  %861 = mul nuw i128 %860, %859
  %862 = lshr i128 %861, 64
  %863 = and i128 %861, 18446744073709551615
  %864 = and i128 %854, 18446744073709551615
  %865 = mul nuw i128 %864, %859
  %866 = lshr i128 %865, 64
  %867 = and i128 %865, 18446744073709551615
  %868 = and i128 %790, 18446744073709551615
  %869 = mul nuw i128 %860, %868
  %870 = lshr i128 %869, 64
  %871 = and i128 %869, 18446744073709551615
  %872 = add nuw nsw i128 %871, %862
  %873 = add nuw nsw i128 %872, %867
  %874 = and i128 %856, 18446744073709551615
  %875 = mul nuw i128 %874, %859
  %876 = lshr i128 %875, 64
  %877 = and i128 %875, 18446744073709551615
  %878 = mul nuw i128 %864, %868
  %879 = lshr i128 %878, 64
  %880 = and i128 %878, 18446744073709551615
  %881 = and i128 %792, 18446744073709551615
  %882 = mul nuw i128 %860, %881
  %883 = lshr i128 %882, 64
  %884 = and i128 %882, 18446744073709551615
  %885 = and i128 %858, 18446744073709551615
  %886 = mul nuw i128 %885, %859
  %887 = lshr i128 %886, 64
  %888 = and i128 %886, 18446744073709551615
  %889 = mul nuw i128 %874, %868
  %890 = lshr i128 %889, 64
  %891 = and i128 %889, 18446744073709551615
  %892 = mul nuw i128 %864, %881
  %893 = lshr i128 %892, 64
  %894 = and i128 %892, 18446744073709551615
  %895 = and i128 %794, 18446744073709551615
  %896 = mul nuw i128 %860, %895
  %897 = lshr i128 %896, 64
  %898 = and i128 %896, 18446744073709551615
  %899 = mul nuw i128 %885, %868
  %900 = lshr i128 %899, 64
  %901 = and i128 %899, 18446744073709551615
  %902 = mul nuw i128 %874, %881
  %903 = lshr i128 %902, 64
  %904 = and i128 %902, 18446744073709551615
  %905 = mul nuw i128 %864, %895
  %906 = lshr i128 %905, 64
  %907 = and i128 %905, 18446744073709551615
  %908 = add nuw nsw i128 %893, %897
  %909 = add nuw nsw i128 %908, %907
  %910 = add nuw nsw i128 %909, %890
  %911 = add nuw nsw i128 %910, %904
  %912 = add nuw nsw i128 %911, %887
  %913 = add nuw nsw i128 %912, %901
  %914 = mul nuw i128 %885, %881
  %915 = lshr i128 %914, 64
  %916 = and i128 %914, 18446744073709551615
  %917 = mul nuw i128 %874, %895
  %918 = lshr i128 %917, 64
  %919 = and i128 %917, 18446744073709551615
  %920 = add nuw nsw i128 %903, %906
  %921 = add nuw nsw i128 %920, %919
  %922 = add nuw nsw i128 %921, %900
  %923 = add nuw nsw i128 %922, %916
  %924 = add nuw nsw i128 %915, %918
  %925 = mul nuw i128 %885, %895
  %926 = lshr i128 %925, 64
  %927 = and i128 %925, 18446744073709551615
  %928 = add nuw nsw i128 %924, %927
  %929 = add nuw nsw i128 %863, 1267650600228229401427983728624
  %930 = or i128 %873, 1267650600228229401496703205376
  %931 = shl nuw nsw i128 %923, 32
  %932 = add i128 %913, %931
  %933 = sub nsw i128 %923, %926
  %934 = shl i128 %913, 32
  %935 = shl nuw nsw i128 %928, 32
  %936 = shl nuw nsw i128 %928, 33
  %937 = add nuw nsw i128 %936, %930
  %938 = add nsw i128 %937, %933
  %939 = sub i128 %938, %934
  %940 = shl nuw nsw i128 %928, 1
  %941 = shl nuw nsw i128 %926, 32
  %942 = sub nsw i128 %929, %926
  %943 = sub nsw i128 %942, %941
  %944 = sub nsw i128 %943, %928
  %945 = sub nsw i128 %944, %935
  %946 = add i128 %945, %932
  %947 = shl nuw nsw i128 %926, 33
  %948 = add nuw nsw i128 %870, 1267650600228229401427983728656
  %949 = add nuw nsw i128 %948, %884
  %950 = add nuw nsw i128 %949, %866
  %951 = add nuw nsw i128 %950, %880
  %952 = add nuw nsw i128 %951, %877
  %953 = add nuw nsw i128 %952, %947
  %954 = add i128 %953, %940
  %955 = sub i128 %954, %933
  %956 = sub i128 %955, %931
  %957 = mul nuw nsw i128 %926, 3
  %958 = add nuw nsw i128 %883, 1267650600228229401427983728656
  %959 = add nuw nsw i128 %958, %898
  %960 = add nuw nsw i128 %959, %879
  %961 = add nuw nsw i128 %960, %894
  %962 = add nuw nsw i128 %961, %876
  %963 = add nuw nsw i128 %962, %891
  %964 = add i128 %963, %888
  %965 = add i128 %964, %957
  %966 = sub i128 %965, %935
  %967 = sub i128 %966, %932
  %968 = add i128 %967, %934
  %969 = load i64, i64* %6, align 8
  %970 = zext i64 %969 to i128
  %971 = and i128 %242, 18446744073709551615
  %972 = mul nuw i128 %971, %970
  %973 = lshr i128 %972, 64
  %974 = and i128 %972, 18446744073709551615
  %975 = and i128 %250, 18446744073709551615
  %976 = mul nuw i128 %975, %970
  %977 = lshr i128 %976, 64
  %978 = and i128 %976, 18446744073709551615
  %979 = getelementptr inbounds i64, i64* %6, i64 1
  %980 = load i64, i64* %979, align 8
  %981 = zext i64 %980 to i128
  %982 = mul nuw i128 %971, %981
  %983 = lshr i128 %982, 64
  %984 = and i128 %982, 18446744073709551615
  %985 = add nuw nsw i128 %984, %973
  %986 = add nuw nsw i128 %985, %978
  %987 = and i128 %252, 18446744073709551615
  %988 = mul nuw i128 %987, %970
  %989 = lshr i128 %988, 64
  %990 = and i128 %988, 18446744073709551615
  %991 = mul nuw i128 %975, %981
  %992 = lshr i128 %991, 64
  %993 = and i128 %991, 18446744073709551615
  %994 = getelementptr inbounds i64, i64* %6, i64 2
  %995 = load i64, i64* %994, align 8
  %996 = zext i64 %995 to i128
  %997 = mul nuw i128 %971, %996
  %998 = lshr i128 %997, 64
  %999 = and i128 %997, 18446744073709551615
  %1000 = and i128 %254, 18446744073709551615
  %1001 = mul nuw i128 %1000, %970
  %1002 = lshr i128 %1001, 64
  %1003 = and i128 %1001, 18446744073709551615
  %1004 = mul nuw i128 %987, %981
  %1005 = lshr i128 %1004, 64
  %1006 = and i128 %1004, 18446744073709551615
  %1007 = mul nuw i128 %975, %996
  %1008 = lshr i128 %1007, 64
  %1009 = and i128 %1007, 18446744073709551615
  %1010 = getelementptr inbounds i64, i64* %6, i64 3
  %1011 = load i64, i64* %1010, align 8
  %1012 = zext i64 %1011 to i128
  %1013 = mul nuw i128 %971, %1012
  %1014 = lshr i128 %1013, 64
  %1015 = and i128 %1013, 18446744073709551615
  %1016 = mul nuw i128 %1000, %981
  %1017 = lshr i128 %1016, 64
  %1018 = and i128 %1016, 18446744073709551615
  %1019 = mul nuw i128 %987, %996
  %1020 = lshr i128 %1019, 64
  %1021 = and i128 %1019, 18446744073709551615
  %1022 = mul nuw i128 %975, %1012
  %1023 = lshr i128 %1022, 64
  %1024 = and i128 %1022, 18446744073709551615
  %1025 = add nuw nsw i128 %1008, %1014
  %1026 = add nuw nsw i128 %1025, %1024
  %1027 = add nuw nsw i128 %1026, %1005
  %1028 = add nuw nsw i128 %1027, %1021
  %1029 = add nuw nsw i128 %1028, %1002
  %1030 = add nuw nsw i128 %1029, %1018
  %1031 = mul nuw i128 %1000, %996
  %1032 = lshr i128 %1031, 64
  %1033 = and i128 %1031, 18446744073709551615
  %1034 = mul nuw i128 %987, %1012
  %1035 = lshr i128 %1034, 64
  %1036 = and i128 %1034, 18446744073709551615
  %1037 = add nuw nsw i128 %1020, %1023
  %1038 = add nuw nsw i128 %1037, %1036
  %1039 = add nuw nsw i128 %1038, %1017
  %1040 = add nuw nsw i128 %1039, %1033
  %1041 = add nuw nsw i128 %1032, %1035
  %1042 = mul nuw i128 %1000, %1012
  %1043 = lshr i128 %1042, 64
  %1044 = and i128 %1042, 18446744073709551615
  %1045 = add nuw nsw i128 %1041, %1044
  %1046 = or i128 %986, 1267650600228229401496703205376
  %1047 = shl nuw nsw i128 %1040, 32
  %1048 = add i128 %1030, %1047
  %1049 = sub nsw i128 %1040, %1043
  %1050 = shl i128 %1030, 32
  %1051 = shl nuw nsw i128 %1045, 32
  %1052 = shl nuw nsw i128 %1045, 33
  %1053 = shl nuw nsw i128 %1045, 1
  %1054 = shl nuw nsw i128 %1043, 32
  %1055 = shl nuw nsw i128 %1043, 33
  %1056 = mul nuw nsw i128 %1043, 3
  %1057 = sub i128 163526927429441592784209900992496, %522
  %1058 = add i128 %1057, %974
  %1059 = sub i128 %1058, %1043
  %1060 = sub i128 %1059, %1054
  %1061 = sub i128 %1060, %1045
  %1062 = sub i128 %1061, %1051
  %1063 = add i128 %1062, %1048
  %1064 = sub i128 162259276829213363391578010288128, %515
  %1065 = add i128 %1064, %1046
  %1066 = add i128 %1065, %1052
  %1067 = add i128 %1066, %1049
  %1068 = sub i128 %1067, %1050
  %1069 = sub i128 163526927429441592784209900996624, %532
  %1070 = add i128 %1069, %983
  %1071 = add i128 %1070, %999
  %1072 = add i128 %1071, %977
  %1073 = add i128 %1072, %993
  %1074 = add i128 %1073, %990
  %1075 = add i128 %1074, %1055
  %1076 = add i128 %1075, %1053
  %1077 = sub i128 %1076, %1049
  %1078 = sub i128 %1077, %1047
  %1079 = add nuw nsw i128 %998, 163526927429441592784209900996624
  %1080 = add nuw nsw i128 %1079, %1015
  %1081 = sub i128 %1080, %544
  %1082 = add i128 %1081, %992
  %1083 = add i128 %1082, %1009
  %1084 = add i128 %1083, %989
  %1085 = add i128 %1084, %1006
  %1086 = add i128 %1085, %1003
  %1087 = add i128 %1086, %1056
  %1088 = sub i128 %1087, %1051
  %1089 = sub i128 %1088, %1048
  %1090 = add i128 %1089, %1050
  %1091 = lshr i128 %1078, 64
  %1092 = add nuw nsw i128 %1091, 18446744069414584320
  %1093 = add i128 %1092, %1090
  %1094 = and i128 %1078, 18446744073709551615
  %1095 = add i128 %1063, 18446744073709551615
  %1096 = add i128 %1068, 1298074214633706907132628377272319
  %1097 = lshr i128 %1093, 64
  %1098 = trunc i128 %1097 to i64
  %1099 = and i128 %1093, 18446744073709551615
  %1100 = sub nsw i128 %1099, %1097
  %1101 = shl nuw nsw i128 %1097, 32
  %1102 = add nsw i128 %1100, %1101
  %1103 = lshr i128 %1102, 64
  %1104 = trunc i128 %1103 to i64
  %1105 = add i64 %1104, %1098
  %1106 = and i128 %1102, 18446744073709551615
  %1107 = sub nsw i128 %1106, %1103
  %1108 = shl nuw nsw i128 %1103, 32
  %1109 = add nsw i128 %1107, %1108
  %1110 = zext i64 %1105 to i128
  %1111 = add i128 %1095, %1110
  %1112 = shl nuw nsw i128 %1110, 32
  %1113 = sub i128 %1096, %1112
  %1114 = lshr i128 %1109, 1
  %1115 = trunc i128 %1114 to i64
  %1116 = trunc i128 %1109 to i64
  %1117 = and i64 %1116, 9223372036854775807
  %1118 = sub nsw i64 9223372032559808512, %1117
  %1119 = and i64 %1118, %1116
  %1120 = or i64 %1119, %1115
  %1121 = ashr i64 %1120, 63
  %1122 = zext i64 %1121 to i128
  %1123 = and i64 %1121, 4294967295
  %1124 = zext i64 %1123 to i128
  %1125 = sub i128 %1113, %1124
  %1126 = and i64 %1121, -4294967295
  %1127 = zext i64 %1126 to i128
  %1128 = mul nuw i128 %971, %128
  %1129 = lshr i128 %1128, 64
  %1130 = and i128 %1128, 18446744073709551615
  %1131 = mul nuw i128 %971, %132
  %1132 = lshr i128 %1131, 64
  %1133 = and i128 %1131, 18446744073709551615
  %1134 = add nuw nsw i128 %1133, %1129
  %1135 = mul nuw i128 %975, %128
  %1136 = lshr i128 %1135, 64
  %1137 = and i128 %1135, 18446744073709551615
  %1138 = add nuw nsw i128 %1134, %1137
  %1139 = mul nuw i128 %971, %138
  %1140 = lshr i128 %1139, 64
  %1141 = and i128 %1139, 18446744073709551615
  %1142 = mul nuw i128 %975, %132
  %1143 = lshr i128 %1142, 64
  %1144 = and i128 %1142, 18446744073709551615
  %1145 = mul nuw i128 %987, %128
  %1146 = lshr i128 %1145, 64
  %1147 = and i128 %1145, 18446744073709551615
  %1148 = mul nuw i128 %971, %143
  %1149 = lshr i128 %1148, 64
  %1150 = and i128 %1148, 18446744073709551615
  %1151 = mul nuw i128 %975, %138
  %1152 = lshr i128 %1151, 64
  %1153 = and i128 %1151, 18446744073709551615
  %1154 = add nuw nsw i128 %1152, %1149
  %1155 = mul nuw i128 %987, %132
  %1156 = lshr i128 %1155, 64
  %1157 = and i128 %1155, 18446744073709551615
  %1158 = mul nuw i128 %1000, %128
  %1159 = lshr i128 %1158, 64
  %1160 = and i128 %1158, 18446744073709551615
  %1161 = mul nuw i128 %975, %143
  %1162 = lshr i128 %1161, 64
  %1163 = and i128 %1161, 18446744073709551615
  %1164 = mul nuw i128 %987, %138
  %1165 = lshr i128 %1164, 64
  %1166 = and i128 %1164, 18446744073709551615
  %1167 = add nuw nsw i128 %1165, %1162
  %1168 = mul nuw i128 %1000, %132
  %1169 = lshr i128 %1168, 64
  %1170 = and i128 %1168, 18446744073709551615
  %1171 = add nuw nsw i128 %1154, %1163
  %1172 = add nuw nsw i128 %1171, %1156
  %1173 = add nuw nsw i128 %1172, %1166
  %1174 = add nuw nsw i128 %1173, %1159
  %1175 = add nuw nsw i128 %1174, %1170
  %1176 = mul nuw i128 %987, %143
  %1177 = lshr i128 %1176, 64
  %1178 = and i128 %1176, 18446744073709551615
  %1179 = add nuw nsw i128 %1167, %1178
  %1180 = add nuw nsw i128 %1179, %1169
  %1181 = mul nuw i128 %1000, %138
  %1182 = lshr i128 %1181, 64
  %1183 = and i128 %1181, 18446744073709551615
  %1184 = add nuw nsw i128 %1180, %1183
  %1185 = add nuw nsw i128 %1182, %1177
  %1186 = mul nuw i128 %1000, %143
  %1187 = lshr i128 %1186, 64
  %1188 = and i128 %1186, 18446744073709551615
  %1189 = add nuw nsw i128 %1185, %1188
  %1190 = or i128 %1138, 1267650600228229401496703205376
  %1191 = shl nuw nsw i128 %1184, 32
  %1192 = add i128 %1191, %1175
  %1193 = sub nsw i128 %1184, %1187
  %1194 = shl i128 %1175, 32
  %1195 = shl nuw nsw i128 %1189, 32
  %1196 = shl nuw nsw i128 %1189, 33
  %1197 = shl nuw nsw i128 %1189, 1
  %1198 = shl nuw nsw i128 %1187, 32
  %1199 = shl nuw nsw i128 %1187, 33
  %1200 = add nuw nsw i128 %1132, 1267650600228229401427983728656
  %1201 = add nuw nsw i128 %1200, %1141
  %1202 = add nuw nsw i128 %1201, %1136
  %1203 = add nuw nsw i128 %1202, %1144
  %1204 = add nuw nsw i128 %1203, %1147
  %1205 = add nuw nsw i128 %1204, %1199
  %1206 = sub i128 %1205, %1193
  %1207 = sub i128 %1206, %1191
  %1208 = add i128 %1207, %1197
  %1209 = mul nuw nsw i128 %1187, 3
  %1210 = lshr i128 %1208, 64
  %1211 = add nuw nsw i128 %1140, 1267650600246676145497398312976
  %1212 = add nuw nsw i128 %1211, %1150
  %1213 = add nuw nsw i128 %1212, %1143
  %1214 = add nuw nsw i128 %1213, %1153
  %1215 = add nuw nsw i128 %1214, %1146
  %1216 = add nuw nsw i128 %1215, %1157
  %1217 = add i128 %1216, %1160
  %1218 = add i128 %1217, %1209
  %1219 = sub i128 %1218, %1195
  %1220 = sub i128 %1219, %1192
  %1221 = add i128 %1220, %1194
  %1222 = add i128 %1221, %1210
  %1223 = and i128 %1208, 18446744073709551615
  %1224 = add nuw nsw i128 %1223, 18446673704965373952
  %1225 = lshr i128 %1222, 64
  %1226 = trunc i128 %1225 to i64
  %1227 = and i128 %1222, 18446744073709551615
  %1228 = sub nsw i128 %1227, %1225
  %1229 = shl nuw nsw i128 %1225, 32
  %1230 = add nsw i128 %1228, %1229
  %1231 = lshr i128 %1230, 64
  %1232 = trunc i128 %1231 to i64
  %1233 = add i64 %1232, %1226
  %1234 = and i128 %1230, 18446744073709551615
  %1235 = sub nsw i128 %1234, %1231
  %1236 = shl nuw nsw i128 %1231, 32
  %1237 = add nsw i128 %1235, %1236
  %1238 = zext i64 %1233 to i128
  %1239 = shl nuw nsw i128 %1238, 32
  %1240 = lshr i128 %1237, 1
  %1241 = trunc i128 %1240 to i64
  %1242 = trunc i128 %1237 to i64
  %1243 = and i64 %1242, 9223372036854775807
  %1244 = sub nsw i64 9223372032559808512, %1243
  %1245 = and i64 %1244, %1242
  %1246 = or i64 %1245, %1241
  %1247 = ashr i64 %1246, 63
  %1248 = zext i64 %1247 to i128
  %1249 = add nuw nsw i128 %1130, 1267650600246676145501693280239
  %1250 = sub nsw i128 %1249, %1187
  %1251 = sub nsw i128 %1250, %1198
  %1252 = sub nsw i128 %1251, %1189
  %1253 = sub nsw i128 %1252, %1195
  %1254 = add i128 %1253, %1192
  %1255 = add i128 %1254, %1238
  %1256 = sub i128 %1255, %1248
  %1257 = and i64 %1247, 4294967295
  %1258 = zext i64 %1257 to i128
  %1259 = and i64 %1247, -4294967295
  %1260 = zext i64 %1259 to i128
  %1261 = sub nsw i128 %1237, %1260
  %1262 = lshr i128 %1256, 64
  %1263 = add nuw nsw i128 %1190, 1298074214633706907132628377272319
  %1264 = add nsw i128 %1263, %1193
  %1265 = add i128 %1264, %1196
  %1266 = sub i128 %1265, %1194
  %1267 = sub i128 %1266, %1239
  %1268 = sub i128 %1267, %1258
  %1269 = add i128 %1268, %1262
  %1270 = lshr i128 %1269, 64
  %1271 = add nuw nsw i128 %1224, %1270
  %1272 = lshr i128 %1271, 64
  %1273 = add nsw i128 %1261, %1272
  %1274 = load i64, i64* %7, align 8
  %1275 = zext i64 %1274 to i128
  %1276 = and i128 %1256, 18446744073709551615
  %1277 = mul nuw i128 %1276, %1275
  %1278 = and i128 %1277, 18446744073709551615
  %1279 = and i128 %1269, 18446744073709551615
  %1280 = mul nuw i128 %1279, %1275
  %1281 = lshr i128 %1280, 64
  %1282 = getelementptr inbounds i64, i64* %7, i64 1
  %1283 = load i64, i64* %1282, align 8
  %1284 = zext i64 %1283 to i128
  %1285 = mul nuw i128 %1276, %1284
  %1286 = lshr i128 %1285, 64
  %1287 = and i128 %1271, 18446744073709551615
  %1288 = mul nuw i128 %1287, %1275
  %1289 = lshr i128 %1288, 64
  %1290 = and i128 %1288, 18446744073709551615
  %1291 = mul nuw i128 %1279, %1284
  %1292 = lshr i128 %1291, 64
  %1293 = and i128 %1291, 18446744073709551615
  %1294 = getelementptr inbounds i64, i64* %7, i64 2
  %1295 = load i64, i64* %1294, align 8
  %1296 = zext i64 %1295 to i128
  %1297 = mul nuw i128 %1276, %1296
  %1298 = lshr i128 %1297, 64
  %1299 = and i128 %1297, 18446744073709551615
  %1300 = and i128 %1273, 18446744073709551615
  %1301 = mul nuw i128 %1300, %1275
  %1302 = lshr i128 %1301, 64
  %1303 = and i128 %1301, 18446744073709551615
  %1304 = mul nuw i128 %1287, %1284
  %1305 = lshr i128 %1304, 64
  %1306 = and i128 %1304, 18446744073709551615
  %1307 = mul nuw i128 %1279, %1296
  %1308 = lshr i128 %1307, 64
  %1309 = and i128 %1307, 18446744073709551615
  %1310 = getelementptr inbounds i64, i64* %7, i64 3
  %1311 = load i64, i64* %1310, align 8
  %1312 = zext i64 %1311 to i128
  %1313 = mul nuw i128 %1276, %1312
  %1314 = lshr i128 %1313, 64
  %1315 = and i128 %1313, 18446744073709551615
  %1316 = mul nuw i128 %1300, %1284
  %1317 = lshr i128 %1316, 64
  %1318 = and i128 %1316, 18446744073709551615
  %1319 = mul nuw i128 %1287, %1296
  %1320 = lshr i128 %1319, 64
  %1321 = and i128 %1319, 18446744073709551615
  %1322 = mul nuw i128 %1279, %1312
  %1323 = lshr i128 %1322, 64
  %1324 = and i128 %1322, 18446744073709551615
  %1325 = add nuw nsw i128 %1308, %1314
  %1326 = add nuw nsw i128 %1325, %1324
  %1327 = add nuw nsw i128 %1326, %1305
  %1328 = add nuw nsw i128 %1327, %1321
  %1329 = add nuw nsw i128 %1328, %1302
  %1330 = add nuw nsw i128 %1329, %1318
  %1331 = mul nuw i128 %1300, %1296
  %1332 = lshr i128 %1331, 64
  %1333 = and i128 %1331, 18446744073709551615
  %1334 = mul nuw i128 %1287, %1312
  %1335 = lshr i128 %1334, 64
  %1336 = and i128 %1334, 18446744073709551615
  %1337 = add nuw nsw i128 %1320, %1323
  %1338 = add nuw nsw i128 %1337, %1336
  %1339 = add nuw nsw i128 %1338, %1317
  %1340 = add nuw nsw i128 %1339, %1333
  %1341 = add nuw nsw i128 %1332, %1335
  %1342 = mul nuw i128 %1300, %1312
  %1343 = lshr i128 %1342, 64
  %1344 = and i128 %1342, 18446744073709551615
  %1345 = add nuw nsw i128 %1341, %1344
  %1346 = shl nuw nsw i128 %1340, 32
  %1347 = add i128 %1330, %1346
  %1348 = sub nsw i128 %1340, %1343
  %1349 = shl i128 %1330, 32
  %1350 = shl nuw nsw i128 %1345, 32
  %1351 = shl nuw nsw i128 %1345, 1
  %1352 = shl nuw nsw i128 %1343, 32
  %1353 = shl nuw nsw i128 %1343, 33
  %1354 = mul nuw nsw i128 %1343, 3
  %1355 = sub i128 163526927429441592784209900992496, %946
  %1356 = add i128 %1355, %1278
  %1357 = sub i128 %1356, %1343
  %1358 = sub i128 %1357, %1352
  %1359 = sub i128 %1358, %1345
  %1360 = sub i128 %1359, %1350
  %1361 = add i128 %1360, %1347
  %1362 = sub i128 163526927429441592784209900996624, %956
  %1363 = add i128 %1362, %1286
  %1364 = add i128 %1363, %1299
  %1365 = add i128 %1364, %1281
  %1366 = add i128 %1365, %1293
  %1367 = add i128 %1366, %1290
  %1368 = add i128 %1367, %1353
  %1369 = add i128 %1368, %1351
  %1370 = sub i128 %1369, %1348
  %1371 = sub i128 %1370, %1346
  %1372 = add nuw nsw i128 %1298, 163526927429441592784209900996624
  %1373 = add nuw nsw i128 %1372, %1315
  %1374 = sub i128 %1373, %968
  %1375 = add i128 %1374, %1292
  %1376 = add i128 %1375, %1309
  %1377 = add i128 %1376, %1289
  %1378 = add i128 %1377, %1306
  %1379 = add i128 %1378, %1303
  %1380 = add i128 %1379, %1354
  %1381 = sub i128 %1380, %1350
  %1382 = sub i128 %1381, %1347
  %1383 = add i128 %1382, %1349
  %1384 = shl i128 %1361, 1
  %1385 = shl i128 %1383, 1
  %1386 = lshr i128 %1371, 63
  %1387 = and i128 %1386, 18446744073709551615
  %1388 = add nuw nsw i128 %1387, 18446744069414584320
  %1389 = add i128 %1388, %1385
  %1390 = add i128 %1384, 18446744073709551615
  %1391 = lshr i128 %1389, 64
  %1392 = trunc i128 %1391 to i64
  %1393 = and i128 %1389, 18446744073709551615
  %1394 = sub nsw i128 %1393, %1391
  %1395 = shl nuw nsw i128 %1391, 32
  %1396 = add nsw i128 %1394, %1395
  %1397 = lshr i128 %1396, 64
  %1398 = trunc i128 %1397 to i64
  %1399 = add i64 %1398, %1392
  %1400 = and i128 %1396, 18446744073709551615
  %1401 = sub nsw i128 %1400, %1397
  %1402 = shl nuw nsw i128 %1397, 32
  %1403 = add nsw i128 %1401, %1402
  %1404 = zext i64 %1399 to i128
  %1405 = add i128 %1390, %1404
  %1406 = lshr i128 %1403, 1
  %1407 = trunc i128 %1406 to i64
  %1408 = trunc i128 %1403 to i64
  %1409 = and i64 %1408, 9223372036854775807
  %1410 = sub nsw i64 9223372032559808512, %1409
  %1411 = and i64 %1410, %1408
  %1412 = or i64 %1411, %1407
  %1413 = ashr i64 %1412, 63
  %1414 = zext i64 %1413 to i128
  %1415 = insertelement <2 x i128> undef, i128 %1111, i32 0
  %1416 = insertelement <2 x i128> %1415, i128 %1405, i32 1
  %1417 = insertelement <2 x i128> undef, i128 %1122, i32 0
  %1418 = insertelement <2 x i128> %1417, i128 %1414, i32 1
  %1419 = sub <2 x i128> %1416, %1418
  %1420 = extractelement <2 x i128> %1419, i32 0
  %1421 = lshr i128 %1420, 64
  %1422 = lshr i128 %1277, 64
  %1423 = and i128 %1280, 18446744073709551615
  %1424 = and i128 %1285, 18446744073709551615
  %1425 = add nuw nsw i128 %1424, %1422
  %1426 = add nuw nsw i128 %1425, %1423
  %1427 = or i128 %1426, 1267650600228229401496703205376
  %1428 = shl nuw nsw i128 %1345, 33
  %1429 = sub i128 162259276829213363391578010288128, %939
  %1430 = add i128 %1429, %1427
  %1431 = add i128 %1430, %1428
  %1432 = add i128 %1431, %1348
  %1433 = sub i128 %1432, %1349
  %1434 = shl i128 %1433, 1
  %1435 = shl i128 %1371, 1
  %1436 = and i128 %1435, 18446744073709551614
  %1437 = insertelement <2 x i128> undef, i128 %1094, i32 0
  %1438 = insertelement <2 x i128> %1437, i128 %1436, i32 1
  %1439 = add nuw nsw <2 x i128> %1438, <i128 18446673704965373952, i128 18446673704965373952>
  %1440 = add i128 %1434, 1298074214633706907132628377272319
  %1441 = shl nuw nsw i128 %1404, 32
  %1442 = sub i128 %1440, %1441
  %1443 = and i64 %1413, 4294967295
  %1444 = zext i64 %1443 to i128
  %1445 = sub i128 %1442, %1444
  %1446 = extractelement <2 x i128> %1419, i32 1
  %1447 = lshr i128 %1446, 64
  %1448 = insertelement <2 x i128> undef, i128 %1125, i32 0
  %1449 = insertelement <2 x i128> %1448, i128 %1445, i32 1
  %1450 = insertelement <2 x i128> undef, i128 %1421, i32 0
  %1451 = insertelement <2 x i128> %1450, i128 %1447, i32 1
  %1452 = add <2 x i128> %1449, %1451
  %1453 = extractelement <2 x i128> %1452, i32 0
  %1454 = lshr <2 x i128> %1452, <i128 64, i128 64>
  %1455 = add nuw nsw <2 x i128> %1439, %1454
  %1456 = and i64 %1413, -4294967295
  %1457 = zext i64 %1456 to i128
  %1458 = insertelement <2 x i128> undef, i128 %1109, i32 0
  %1459 = insertelement <2 x i128> %1458, i128 %1403, i32 1
  %1460 = insertelement <2 x i128> undef, i128 %1127, i32 0
  %1461 = insertelement <2 x i128> %1460, i128 %1457, i32 1
  %1462 = sub nsw <2 x i128> %1459, %1461
  %1463 = lshr <2 x i128> %1455, <i128 64, i128 64>
  %1464 = add nsw <2 x i128> %1463, %1462
  %1465 = lshr i128 %659, 64
  %1466 = sub i128 41832469807550016991192877629968, %335
  %1467 = add i128 %1466, %619
  %1468 = add i128 %1467, %650
  %1469 = sub i128 %1468, %208
  %1470 = add i128 %1469, %616
  %1471 = sub i128 %1470, %646
  %1472 = sub i128 %1471, %643
  %1473 = add i128 %1472, %645
  %1474 = add i128 %1473, %1465
  %1475 = and i128 %659, 18446744073709551615
  %1476 = add nuw nsw i128 %1475, 18446673704965373952
  %1477 = lshr i128 %1474, 64
  %1478 = trunc i128 %1477 to i64
  %1479 = and i128 %1474, 18446744073709551615
  %1480 = sub nsw i128 %1479, %1477
  %1481 = shl nuw nsw i128 %1477, 32
  %1482 = add nsw i128 %1480, %1481
  %1483 = lshr i128 %1482, 64
  %1484 = trunc i128 %1483 to i64
  %1485 = add i64 %1484, %1478
  %1486 = and i128 %1482, 18446744073709551615
  %1487 = sub nsw i128 %1486, %1483
  %1488 = shl nuw nsw i128 %1483, 32
  %1489 = add nsw i128 %1487, %1488
  %1490 = zext i64 %1485 to i128
  %1491 = shl nuw nsw i128 %1490, 32
  %1492 = lshr i128 %1489, 1
  %1493 = trunc i128 %1492 to i64
  %1494 = trunc i128 %1489 to i64
  %1495 = and i64 %1494, 9223372036854775807
  %1496 = sub nsw i64 9223372032559808512, %1495
  %1497 = and i64 %1496, %1494
  %1498 = or i64 %1497, %1493
  %1499 = ashr i64 %1498, 63
  %1500 = zext i64 %1499 to i128
  %1501 = sub i128 41832469807550016991197172596207, %320
  %1502 = add i128 %1501, %595
  %1503 = sub i128 %1502, %193
  %1504 = sub i128 %1503, %638
  %1505 = sub i128 %1504, %648
  %1506 = sub i128 %1505, %640
  %1507 = sub i128 %1506, %646
  %1508 = add i128 %1507, %643
  %1509 = add i128 %1508, %1490
  %1510 = sub i128 %1509, %1500
  %1511 = and i64 %1499, 4294967295
  %1512 = zext i64 %1511 to i128
  %1513 = and i64 %1499, -4294967295
  %1514 = zext i64 %1513 to i128
  %1515 = sub nsw i128 %1489, %1514
  %1516 = lshr i128 %1510, 64
  %1517 = sub i128 1338639033841010247980522879844351, %314
  %1518 = add i128 %1517, %641
  %1519 = sub i128 %1518, %187
  %1520 = add i128 %1519, %647
  %1521 = add i128 %1520, %644
  %1522 = sub i128 %1521, %645
  %1523 = sub i128 %1522, %1491
  %1524 = sub i128 %1523, %1512
  %1525 = add i128 %1524, %1516
  %1526 = lshr i128 %1525, 64
  %1527 = add nuw nsw i128 %1476, %1526
  %1528 = lshr i128 %1527, 64
  %1529 = add nsw i128 %1515, %1528
  %1530 = and i128 %1420, 18446744073709551615
  %1531 = and i128 %1510, 18446744073709551615
  %1532 = mul nuw i128 %1530, %1531
  %1533 = and i128 %1525, 18446744073709551615
  %1534 = mul nuw i128 %1530, %1533
  %1535 = and i128 %1453, 18446744073709551615
  %1536 = mul nuw i128 %1535, %1531
  %1537 = and i128 %1527, 18446744073709551615
  %1538 = mul nuw i128 %1530, %1537
  %1539 = mul nuw i128 %1535, %1533
  %1540 = extractelement <2 x i128> %1455, i32 0
  %1541 = and i128 %1540, 18446744073709551615
  %1542 = mul nuw i128 %1541, %1531
  %1543 = and i128 %1529, 18446744073709551615
  %1544 = mul nuw i128 %1530, %1543
  %1545 = lshr i128 %1544, 64
  %1546 = mul nuw i128 %1535, %1537
  %1547 = lshr i128 %1546, 64
  %1548 = add nuw nsw i128 %1547, %1545
  %1549 = mul nuw i128 %1541, %1533
  %1550 = lshr i128 %1549, 64
  %1551 = extractelement <2 x i128> %1464, i32 0
  %1552 = and i128 %1551, 18446744073709551615
  %1553 = mul nuw i128 %1552, %1531
  %1554 = lshr i128 %1553, 64
  %1555 = mul nuw i128 %1535, %1543
  %1556 = lshr i128 %1555, 64
  %1557 = and i128 %1555, 18446744073709551615
  %1558 = mul nuw i128 %1541, %1537
  %1559 = lshr i128 %1558, 64
  %1560 = and i128 %1558, 18446744073709551615
  %1561 = add nuw nsw i128 %1559, %1556
  %1562 = mul nuw i128 %1552, %1533
  %1563 = lshr i128 %1562, 64
  %1564 = and i128 %1562, 18446744073709551615
  %1565 = add nuw nsw i128 %1548, %1557
  %1566 = add nuw nsw i128 %1565, %1550
  %1567 = add nuw nsw i128 %1566, %1560
  %1568 = add nuw nsw i128 %1567, %1554
  %1569 = add nuw nsw i128 %1568, %1564
  %1570 = mul nuw i128 %1541, %1543
  %1571 = lshr i128 %1570, 64
  %1572 = and i128 %1570, 18446744073709551615
  %1573 = add nuw nsw i128 %1561, %1572
  %1574 = add nuw nsw i128 %1573, %1563
  %1575 = mul nuw i128 %1552, %1537
  %1576 = lshr i128 %1575, 64
  %1577 = and i128 %1575, 18446744073709551615
  %1578 = add nuw nsw i128 %1574, %1577
  %1579 = add nuw nsw i128 %1576, %1571
  %1580 = mul nuw i128 %1552, %1543
  %1581 = lshr i128 %1580, 64
  %1582 = and i128 %1580, 18446744073709551615
  %1583 = add nuw nsw i128 %1579, %1582
  %1584 = shl nuw nsw i128 %1578, 32
  %1585 = add i128 %1584, %1569
  %1586 = sub nsw i128 %1578, %1581
  %1587 = shl i128 %1569, 32
  %1588 = shl nuw nsw i128 %1583, 32
  %1589 = trunc <2 x i128> %1419 to <2 x i64>
  %1590 = trunc <2 x i128> %1452 to <2 x i64>
  %1591 = trunc <2 x i128> %1455 to <2 x i64>
  %1592 = trunc <2 x i128> %1464 to <2 x i64>
  %1593 = or <2 x i128> %1452, %1419
  %1594 = or <2 x i128> %1593, %1455
  %1595 = or <2 x i128> %1594, %1464
  %1596 = trunc <2 x i128> %1595 to <2 x i64>
  %1597 = add <2 x i64> %1596, <i64 -1, i64 -1>
  %1598 = shl <2 x i64> %1597, <i64 32, i64 32>
  %1599 = and <2 x i64> %1598, %1597
  %1600 = shl <2 x i64> %1599, <i64 16, i64 16>
  %1601 = and <2 x i64> %1600, %1599
  %1602 = shl <2 x i64> %1601, <i64 8, i64 8>
  %1603 = and <2 x i64> %1602, %1601
  %1604 = shl <2 x i64> %1603, <i64 4, i64 4>
  %1605 = and <2 x i64> %1604, %1603
  %1606 = shl <2 x i64> %1605, <i64 2, i64 2>
  %1607 = and <2 x i64> %1606, %1605
  %1608 = shl <2 x i64> %1607, <i64 1, i64 1>
  %1609 = and <2 x i64> %1608, %1607
  %1610 = xor <2 x i64> %1589, <i64 -1, i64 -1>
  %1611 = xor <2 x i64> %1590, <i64 4294967295, i64 4294967295>
  %1612 = or <2 x i64> %1611, %1610
  %1613 = or <2 x i64> %1612, %1591
  %1614 = xor <2 x i64> %1592, <i64 -4294967295, i64 -4294967295>
  %1615 = or <2 x i64> %1613, %1614
  %1616 = add <2 x i64> %1615, <i64 -1, i64 -1>
  %1617 = shl <2 x i64> %1616, <i64 32, i64 32>
  %1618 = and <2 x i64> %1617, %1616
  %1619 = shl <2 x i64> %1618, <i64 16, i64 16>
  %1620 = and <2 x i64> %1619, %1618
  %1621 = shl <2 x i64> %1620, <i64 8, i64 8>
  %1622 = and <2 x i64> %1621, %1620
  %1623 = shl <2 x i64> %1622, <i64 4, i64 4>
  %1624 = and <2 x i64> %1623, %1622
  %1625 = shl <2 x i64> %1624, <i64 2, i64 2>
  %1626 = and <2 x i64> %1625, %1624
  %1627 = shl <2 x i64> %1626, <i64 1, i64 1>
  %1628 = and <2 x i64> %1627, %1626
  %1629 = or <2 x i64> %1628, %1609
  %1630 = ashr <2 x i64> %1629, <i64 63, i64 63>
  %1631 = zext <2 x i64> %1630 to <2 x i128>
  %1632 = shl nuw <2 x i128> %1631, <i128 64, i128 64>
  %1633 = or <2 x i128> %1632, %1631
  %1634 = icmp eq <2 x i128> %1633, zeroinitializer
  %1635 = extractelement <2 x i1> %1634, i32 0
  %1636 = extractelement <2 x i1> %1634, i32 1
  %1637 = or i1 %1635, %1636
  %1638 = extractelement <2 x i128> %127, i32 0
  %1639 = extractelement <2 x i128> %127, i32 1
  %1640 = or i128 %1638, %1639
  %1641 = icmp ne i128 %1640, 0
  %1642 = or i1 %1641, %1637
  br i1 %1642, label %1644, label %1643

1643:                                             ; preds = %9
  tail call fastcc void @0(i128* %0, i128* %1, i128* %2, i128* nonnull %3, i128* nonnull %4, i128* nonnull %5)
  br label %2672

1644:                                             ; preds = %9
  %1645 = extractelement <2 x i128> %1452, i32 1
  %1646 = mul nuw nsw i128 %1581, 3
  %1647 = and i128 %1553, 18446744073709551615
  %1648 = and i128 %1544, 18446744073709551615
  %1649 = lshr i128 %1542, 64
  %1650 = lshr i128 %1538, 64
  %1651 = lshr i128 %1539, 64
  %1652 = and i128 %1546, 18446744073709551615
  %1653 = and i128 %1549, 18446744073709551615
  %1654 = add nuw nsw i128 %1650, 1267650600228229401427983728656
  %1655 = add nuw nsw i128 %1654, %1648
  %1656 = add nuw nsw i128 %1655, %1651
  %1657 = add nuw nsw i128 %1656, %1652
  %1658 = add nuw nsw i128 %1657, %1649
  %1659 = add nuw nsw i128 %1658, %1653
  %1660 = add i128 %1659, %1647
  %1661 = add i128 %1660, %1646
  %1662 = sub i128 %1661, %1588
  %1663 = sub i128 %1662, %1585
  %1664 = add i128 %1663, %1587
  %1665 = shl nuw nsw i128 %1581, 33
  %1666 = shl nuw nsw i128 %1583, 1
  %1667 = and i128 %1542, 18446744073709551615
  %1668 = and i128 %1538, 18446744073709551615
  %1669 = lshr i128 %1536, 64
  %1670 = lshr i128 %1534, 64
  %1671 = and i128 %1539, 18446744073709551615
  %1672 = add nuw nsw i128 %1670, 1267650600228229401427983728656
  %1673 = add nuw nsw i128 %1672, %1668
  %1674 = add nuw nsw i128 %1673, %1669
  %1675 = add nuw nsw i128 %1674, %1671
  %1676 = add nuw nsw i128 %1675, %1667
  %1677 = add nuw nsw i128 %1676, %1665
  %1678 = sub i128 %1677, %1586
  %1679 = sub i128 %1678, %1584
  %1680 = add i128 %1679, %1666
  %1681 = and i128 %1532, 18446744073709551615
  %1682 = add nuw nsw i128 %1681, 1267650600228229401427983728624
  %1683 = shl nuw nsw i128 %1581, 32
  %1684 = sub nsw i128 %1682, %1581
  %1685 = sub nsw i128 %1684, %1683
  %1686 = sub nsw i128 %1685, %1583
  %1687 = sub nsw i128 %1686, %1588
  %1688 = add i128 %1687, %1585
  %1689 = shl nuw nsw i128 %1583, 33
  %1690 = and i128 %1536, 18446744073709551615
  %1691 = and i128 %1534, 18446744073709551615
  %1692 = lshr i128 %1532, 64
  %1693 = add nuw nsw i128 %1691, %1692
  %1694 = add nuw nsw i128 %1693, %1690
  %1695 = or i128 %1694, 1267650600228229401496703205376
  %1696 = add nsw i128 %1586, %1695
  %1697 = add i128 %1696, %1689
  %1698 = sub i128 %1697, %1587
  %1699 = shl i128 %1063, 1
  %1700 = shl i128 %1068, 1
  %1701 = shl i128 %1078, 1
  %1702 = shl i128 %1090, 1
  %1703 = lshr i128 %1078, 63
  %1704 = and i128 %1703, 18446744073709551615
  %1705 = add nuw nsw i128 %1704, 18446744069414584320
  %1706 = add i128 %1705, %1702
  %1707 = and i128 %1701, 18446744073709551614
  %1708 = add nuw nsw i128 %1707, 18446673704965373952
  %1709 = add i128 %1699, 18446744073709551615
  %1710 = add i128 %1700, 1298074214633706907132628377272319
  %1711 = lshr i128 %1706, 64
  %1712 = trunc i128 %1711 to i64
  %1713 = and i128 %1706, 18446744073709551615
  %1714 = sub nsw i128 %1713, %1711
  %1715 = shl nuw nsw i128 %1711, 32
  %1716 = add nsw i128 %1714, %1715
  %1717 = lshr i128 %1716, 64
  %1718 = trunc i128 %1717 to i64
  %1719 = add i64 %1718, %1712
  %1720 = and i128 %1716, 18446744073709551615
  %1721 = sub nsw i128 %1720, %1717
  %1722 = shl nuw nsw i128 %1717, 32
  %1723 = add nsw i128 %1721, %1722
  %1724 = zext i64 %1719 to i128
  %1725 = add i128 %1709, %1724
  %1726 = shl nuw nsw i128 %1724, 32
  %1727 = sub i128 %1710, %1726
  %1728 = lshr i128 %1723, 1
  %1729 = trunc i128 %1728 to i64
  %1730 = trunc i128 %1723 to i64
  %1731 = and i64 %1730, 9223372036854775807
  %1732 = sub nsw i64 9223372032559808512, %1731
  %1733 = and i64 %1732, %1730
  %1734 = or i64 %1733, %1729
  %1735 = ashr i64 %1734, 63
  %1736 = zext i64 %1735 to i128
  %1737 = sub i128 %1725, %1736
  %1738 = and i64 %1735, 4294967295
  %1739 = zext i64 %1738 to i128
  %1740 = sub i128 %1727, %1739
  %1741 = and i64 %1735, -4294967295
  %1742 = zext i64 %1741 to i128
  %1743 = sub nsw i128 %1723, %1742
  %1744 = lshr i128 %1737, 64
  %1745 = add i128 %1740, %1744
  %1746 = lshr i128 %1745, 64
  %1747 = add nuw nsw i128 %1708, %1746
  %1748 = lshr i128 %1747, 64
  %1749 = add nsw i128 %1743, %1748
  %1750 = and i128 %1737, 18446744073709551615
  %1751 = mul nuw i128 %1750, %1750
  %1752 = lshr i128 %1751, 64
  %1753 = and i128 %1751, 18446744073709551615
  %1754 = and i128 %1745, 18446744073709551615
  %1755 = mul nuw i128 %1754, %1750
  %1756 = lshr i128 %1755, 64
  %1757 = shl i128 %1755, 1
  %1758 = and i128 %1757, 36893488147419103230
  %1759 = add nuw nsw i128 %1758, %1752
  %1760 = and i128 %1747, 18446744073709551615
  %1761 = mul nuw i128 %1760, %1750
  %1762 = lshr i128 %1761, 64
  %1763 = and i128 %1761, 18446744073709551615
  %1764 = add nuw nsw i128 %1763, %1756
  %1765 = and i128 %1749, 18446744073709551615
  %1766 = mul nuw i128 %1765, %1750
  %1767 = lshr i128 %1766, 64
  %1768 = and i128 %1766, 18446744073709551615
  %1769 = mul nuw i128 %1760, %1754
  %1770 = lshr i128 %1769, 64
  %1771 = and i128 %1769, 18446744073709551615
  %1772 = add nuw nsw i128 %1771, %1762
  %1773 = add nuw nsw i128 %1772, %1768
  %1774 = shl nuw nsw i128 %1773, 1
  %1775 = add nuw nsw i128 %1767, %1770
  %1776 = mul nuw i128 %1754, %1754
  %1777 = lshr i128 %1776, 64
  %1778 = and i128 %1776, 18446744073709551615
  %1779 = mul nuw i128 %1765, %1754
  %1780 = lshr i128 %1779, 64
  %1781 = and i128 %1779, 18446744073709551615
  %1782 = add nuw nsw i128 %1775, %1781
  %1783 = shl nuw nsw i128 %1782, 1
  %1784 = mul nuw i128 %1765, %1760
  %1785 = lshr i128 %1784, 64
  %1786 = and i128 %1784, 18446744073709551615
  %1787 = add nuw nsw i128 %1780, %1786
  %1788 = shl nuw nsw i128 %1787, 1
  %1789 = shl nuw nsw i128 %1785, 1
  %1790 = mul nuw i128 %1760, %1760
  %1791 = lshr i128 %1790, 64
  %1792 = and i128 %1790, 18446744073709551615
  %1793 = add nuw nsw i128 %1783, %1792
  %1794 = add nuw nsw i128 %1788, %1791
  %1795 = mul nuw i128 %1765, %1765
  %1796 = lshr i128 %1795, 64
  %1797 = and i128 %1795, 18446744073709551615
  %1798 = add nuw nsw i128 %1789, %1797
  %1799 = or i128 %1759, 1267650600228229401496703205376
  %1800 = shl nuw nsw i128 %1794, 32
  %1801 = add nuw nsw i128 %1800, %1793
  %1802 = sub nsw i128 %1794, %1796
  %1803 = shl nuw nsw i128 %1793, 32
  %1804 = shl nuw nsw i128 %1798, 32
  %1805 = shl nuw nsw i128 %1798, 33
  %1806 = shl nuw nsw i128 %1796, 32
  %1807 = shl nuw nsw i128 %1796, 33
  %1808 = add nuw nsw i128 %1764, %1798
  %1809 = shl nuw nsw i128 %1808, 1
  %1810 = add nuw nsw i128 %1778, 1267650600228229401427983728656
  %1811 = add nuw nsw i128 %1810, %1807
  %1812 = sub nsw i128 %1811, %1802
  %1813 = sub nsw i128 %1812, %1800
  %1814 = add i128 %1813, %1809
  %1815 = mul nuw nsw i128 %1796, 3
  %1816 = lshr i128 %1814, 64
  %1817 = add nuw nsw i128 %1777, 1267650600246676145497398312976
  %1818 = add nuw nsw i128 %1817, %1815
  %1819 = add nuw nsw i128 %1818, %1774
  %1820 = sub nsw i128 %1819, %1804
  %1821 = sub nsw i128 %1820, %1801
  %1822 = add i128 %1821, %1803
  %1823 = add i128 %1822, %1816
  %1824 = and i128 %1814, 18446744073709551615
  %1825 = add nuw nsw i128 %1824, 18446673704965373952
  %1826 = lshr i128 %1823, 64
  %1827 = trunc i128 %1826 to i64
  %1828 = and i128 %1823, 18446744073709551615
  %1829 = sub nsw i128 %1828, %1826
  %1830 = shl nuw nsw i128 %1826, 32
  %1831 = add nsw i128 %1829, %1830
  %1832 = lshr i128 %1831, 64
  %1833 = trunc i128 %1832 to i64
  %1834 = add i64 %1833, %1827
  %1835 = and i128 %1831, 18446744073709551615
  %1836 = sub nsw i128 %1835, %1832
  %1837 = shl nuw nsw i128 %1832, 32
  %1838 = add nsw i128 %1836, %1837
  %1839 = zext i64 %1834 to i128
  %1840 = shl nuw nsw i128 %1839, 32
  %1841 = lshr i128 %1838, 1
  %1842 = trunc i128 %1841 to i64
  %1843 = trunc i128 %1838 to i64
  %1844 = and i64 %1843, 9223372036854775807
  %1845 = sub nsw i64 9223372032559808512, %1844
  %1846 = and i64 %1845, %1843
  %1847 = or i64 %1846, %1842
  %1848 = ashr i64 %1847, 63
  %1849 = zext i64 %1848 to i128
  %1850 = add nuw nsw i128 %1753, 1267650600246676145501693280239
  %1851 = sub nsw i128 %1850, %1796
  %1852 = sub nsw i128 %1851, %1806
  %1853 = sub nsw i128 %1852, %1798
  %1854 = sub nsw i128 %1853, %1804
  %1855 = add nsw i128 %1854, %1801
  %1856 = add i128 %1855, %1839
  %1857 = sub i128 %1856, %1849
  %1858 = and i64 %1848, 4294967295
  %1859 = zext i64 %1858 to i128
  %1860 = and i64 %1848, -4294967295
  %1861 = zext i64 %1860 to i128
  %1862 = sub nsw i128 %1838, %1861
  %1863 = lshr i128 %1857, 64
  %1864 = add nuw nsw i128 %1799, 1298074214633706907132628377272319
  %1865 = add nuw nsw i128 %1864, %1805
  %1866 = add nsw i128 %1865, %1802
  %1867 = sub nsw i128 %1866, %1803
  %1868 = sub i128 %1867, %1840
  %1869 = sub i128 %1868, %1859
  %1870 = add i128 %1869, %1863
  %1871 = lshr i128 %1870, 64
  %1872 = add nuw nsw i128 %1825, %1871
  %1873 = lshr i128 %1872, 64
  %1874 = add nsw i128 %1862, %1873
  %1875 = and i128 %1857, 18446744073709551615
  %1876 = mul nuw i128 %1875, %1530
  %1877 = lshr i128 %1876, 64
  %1878 = and i128 %1876, 18446744073709551615
  %1879 = and i128 %1870, 18446744073709551615
  %1880 = mul nuw i128 %1879, %1530
  %1881 = lshr i128 %1880, 64
  %1882 = and i128 %1880, 18446744073709551615
  %1883 = mul nuw i128 %1875, %1535
  %1884 = lshr i128 %1883, 64
  %1885 = and i128 %1883, 18446744073709551615
  %1886 = add nuw nsw i128 %1885, %1877
  %1887 = add nuw nsw i128 %1886, %1882
  %1888 = and i128 %1872, 18446744073709551615
  %1889 = mul nuw i128 %1888, %1530
  %1890 = lshr i128 %1889, 64
  %1891 = and i128 %1889, 18446744073709551615
  %1892 = mul nuw i128 %1879, %1535
  %1893 = lshr i128 %1892, 64
  %1894 = and i128 %1892, 18446744073709551615
  %1895 = mul nuw i128 %1875, %1541
  %1896 = lshr i128 %1895, 64
  %1897 = and i128 %1895, 18446744073709551615
  %1898 = and i128 %1874, 18446744073709551615
  %1899 = mul nuw i128 %1898, %1530
  %1900 = lshr i128 %1899, 64
  %1901 = and i128 %1899, 18446744073709551615
  %1902 = mul nuw i128 %1888, %1535
  %1903 = lshr i128 %1902, 64
  %1904 = and i128 %1902, 18446744073709551615
  %1905 = mul nuw i128 %1879, %1541
  %1906 = lshr i128 %1905, 64
  %1907 = and i128 %1905, 18446744073709551615
  %1908 = mul nuw i128 %1875, %1552
  %1909 = lshr i128 %1908, 64
  %1910 = and i128 %1908, 18446744073709551615
  %1911 = mul nuw i128 %1898, %1535
  %1912 = lshr i128 %1911, 64
  %1913 = and i128 %1911, 18446744073709551615
  %1914 = mul nuw i128 %1888, %1541
  %1915 = lshr i128 %1914, 64
  %1916 = and i128 %1914, 18446744073709551615
  %1917 = mul nuw i128 %1879, %1552
  %1918 = lshr i128 %1917, 64
  %1919 = and i128 %1917, 18446744073709551615
  %1920 = add nuw nsw i128 %1906, %1909
  %1921 = add nuw nsw i128 %1920, %1919
  %1922 = add nuw nsw i128 %1921, %1903
  %1923 = add nuw nsw i128 %1922, %1916
  %1924 = add nuw nsw i128 %1923, %1900
  %1925 = add nuw nsw i128 %1924, %1913
  %1926 = mul nuw i128 %1898, %1541
  %1927 = lshr i128 %1926, 64
  %1928 = and i128 %1926, 18446744073709551615
  %1929 = mul nuw i128 %1888, %1552
  %1930 = lshr i128 %1929, 64
  %1931 = and i128 %1929, 18446744073709551615
  %1932 = add nuw nsw i128 %1915, %1918
  %1933 = add nuw nsw i128 %1932, %1931
  %1934 = add nuw nsw i128 %1933, %1912
  %1935 = add nuw nsw i128 %1934, %1928
  %1936 = add nuw nsw i128 %1927, %1930
  %1937 = mul nuw i128 %1898, %1552
  %1938 = lshr i128 %1937, 64
  %1939 = and i128 %1937, 18446744073709551615
  %1940 = add nuw nsw i128 %1936, %1939
  %1941 = add nuw nsw i128 %1878, 1267650600228229401427983728624
  %1942 = or i128 %1887, 1267650600228229401496703205376
  %1943 = shl nuw nsw i128 %1935, 32
  %1944 = add i128 %1925, %1943
  %1945 = sub nsw i128 %1935, %1938
  %1946 = shl i128 %1925, 32
  %1947 = shl nuw nsw i128 %1940, 32
  %1948 = shl nuw nsw i128 %1940, 33
  %1949 = add nuw nsw i128 %1948, %1942
  %1950 = add nsw i128 %1949, %1945
  %1951 = sub i128 %1950, %1946
  %1952 = shl nuw nsw i128 %1940, 1
  %1953 = shl nuw nsw i128 %1938, 32
  %1954 = sub nsw i128 %1941, %1938
  %1955 = sub nsw i128 %1954, %1953
  %1956 = sub nsw i128 %1955, %1940
  %1957 = sub nsw i128 %1956, %1947
  %1958 = add i128 %1957, %1944
  %1959 = shl nuw nsw i128 %1938, 33
  %1960 = add nuw nsw i128 %1884, 1267650600228229401427983728656
  %1961 = add nuw nsw i128 %1960, %1897
  %1962 = add nuw nsw i128 %1961, %1881
  %1963 = add nuw nsw i128 %1962, %1894
  %1964 = add nuw nsw i128 %1963, %1891
  %1965 = add nuw nsw i128 %1964, %1959
  %1966 = add i128 %1965, %1952
  %1967 = sub i128 %1966, %1945
  %1968 = sub i128 %1967, %1943
  %1969 = mul nuw nsw i128 %1938, 3
  %1970 = add nuw nsw i128 %1896, 1267650600228229401427983728656
  %1971 = add nuw nsw i128 %1970, %1910
  %1972 = add nuw nsw i128 %1971, %1893
  %1973 = add nuw nsw i128 %1972, %1907
  %1974 = add nuw nsw i128 %1973, %1890
  %1975 = add nuw nsw i128 %1974, %1904
  %1976 = add i128 %1975, %1901
  %1977 = add i128 %1976, %1969
  %1978 = sub i128 %1977, %1947
  %1979 = sub i128 %1978, %1944
  %1980 = add i128 %1979, %1946
  %1981 = lshr i128 %532, 64
  %1982 = add nuw nsw i128 %1981, 18446744069414584320
  %1983 = add i128 %1982, %544
  %1984 = and i128 %532, 18446744073709551615
  %1985 = add nuw nsw i128 %1984, 18446673704965373952
  %1986 = add i128 %522, 18446744073709551615
  %1987 = add i128 %515, 1298074214633706907132628377272319
  %1988 = lshr i128 %1983, 64
  %1989 = trunc i128 %1988 to i64
  %1990 = and i128 %1983, 18446744073709551615
  %1991 = sub nsw i128 %1990, %1988
  %1992 = shl nuw nsw i128 %1988, 32
  %1993 = add nsw i128 %1991, %1992
  %1994 = lshr i128 %1993, 64
  %1995 = trunc i128 %1994 to i64
  %1996 = add i64 %1995, %1989
  %1997 = and i128 %1993, 18446744073709551615
  %1998 = sub nsw i128 %1997, %1994
  %1999 = shl nuw nsw i128 %1994, 32
  %2000 = add nsw i128 %1998, %1999
  %2001 = zext i64 %1996 to i128
  %2002 = add i128 %1986, %2001
  %2003 = shl nuw nsw i128 %2001, 32
  %2004 = sub i128 %1987, %2003
  %2005 = lshr i128 %2000, 1
  %2006 = trunc i128 %2005 to i64
  %2007 = trunc i128 %2000 to i64
  %2008 = and i64 %2007, 9223372036854775807
  %2009 = sub nsw i64 9223372032559808512, %2008
  %2010 = and i64 %2009, %2007
  %2011 = or i64 %2010, %2006
  %2012 = ashr i64 %2011, 63
  %2013 = zext i64 %2012 to i128
  %2014 = sub i128 %2002, %2013
  %2015 = and i64 %2012, 4294967295
  %2016 = zext i64 %2015 to i128
  %2017 = sub i128 %2004, %2016
  %2018 = and i64 %2012, -4294967295
  %2019 = zext i64 %2018 to i128
  %2020 = sub nsw i128 %2000, %2019
  %2021 = lshr i128 %2014, 64
  %2022 = add i128 %2017, %2021
  %2023 = lshr i128 %2022, 64
  %2024 = add nuw nsw i128 %1985, %2023
  %2025 = lshr i128 %2024, 64
  %2026 = add nsw i128 %2020, %2025
  %2027 = and i128 %2014, 18446744073709551615
  %2028 = mul nuw i128 %1875, %2027
  %2029 = lshr i128 %2028, 64
  %2030 = and i128 %2028, 18446744073709551615
  %2031 = mul nuw i128 %1879, %2027
  %2032 = lshr i128 %2031, 64
  %2033 = and i128 %2031, 18446744073709551615
  %2034 = and i128 %2022, 18446744073709551615
  %2035 = mul nuw i128 %1875, %2034
  %2036 = lshr i128 %2035, 64
  %2037 = and i128 %2035, 18446744073709551615
  %2038 = add nuw nsw i128 %2037, %2029
  %2039 = add nuw nsw i128 %2038, %2033
  %2040 = mul nuw i128 %1888, %2027
  %2041 = lshr i128 %2040, 64
  %2042 = and i128 %2040, 18446744073709551615
  %2043 = mul nuw i128 %1879, %2034
  %2044 = lshr i128 %2043, 64
  %2045 = and i128 %2043, 18446744073709551615
  %2046 = and i128 %2024, 18446744073709551615
  %2047 = mul nuw i128 %1875, %2046
  %2048 = lshr i128 %2047, 64
  %2049 = and i128 %2047, 18446744073709551615
  %2050 = mul nuw i128 %1898, %2027
  %2051 = lshr i128 %2050, 64
  %2052 = and i128 %2050, 18446744073709551615
  %2053 = mul nuw i128 %1888, %2034
  %2054 = lshr i128 %2053, 64
  %2055 = and i128 %2053, 18446744073709551615
  %2056 = mul nuw i128 %1879, %2046
  %2057 = lshr i128 %2056, 64
  %2058 = and i128 %2056, 18446744073709551615
  %2059 = and i128 %2026, 18446744073709551615
  %2060 = mul nuw i128 %1875, %2059
  %2061 = lshr i128 %2060, 64
  %2062 = and i128 %2060, 18446744073709551615
  %2063 = mul nuw i128 %1898, %2034
  %2064 = lshr i128 %2063, 64
  %2065 = and i128 %2063, 18446744073709551615
  %2066 = mul nuw i128 %1888, %2046
  %2067 = lshr i128 %2066, 64
  %2068 = and i128 %2066, 18446744073709551615
  %2069 = mul nuw i128 %1879, %2059
  %2070 = lshr i128 %2069, 64
  %2071 = and i128 %2069, 18446744073709551615
  %2072 = add nuw nsw i128 %2057, %2061
  %2073 = add nuw nsw i128 %2072, %2071
  %2074 = add nuw nsw i128 %2073, %2054
  %2075 = add nuw nsw i128 %2074, %2068
  %2076 = add nuw nsw i128 %2075, %2051
  %2077 = add nuw nsw i128 %2076, %2065
  %2078 = mul nuw i128 %1898, %2046
  %2079 = lshr i128 %2078, 64
  %2080 = and i128 %2078, 18446744073709551615
  %2081 = mul nuw i128 %1888, %2059
  %2082 = lshr i128 %2081, 64
  %2083 = and i128 %2081, 18446744073709551615
  %2084 = add nuw nsw i128 %2067, %2070
  %2085 = add nuw nsw i128 %2084, %2083
  %2086 = add nuw nsw i128 %2085, %2064
  %2087 = add nuw nsw i128 %2086, %2080
  %2088 = add nuw nsw i128 %2079, %2082
  %2089 = mul nuw i128 %1898, %2059
  %2090 = lshr i128 %2089, 64
  %2091 = and i128 %2089, 18446744073709551615
  %2092 = add nuw nsw i128 %2088, %2091
  %2093 = add nuw nsw i128 %2030, 1267650600228229401427983728624
  %2094 = or i128 %2039, 1267650600228229401496703205376
  %2095 = shl nuw nsw i128 %2087, 32
  %2096 = add i128 %2077, %2095
  %2097 = sub nsw i128 %2087, %2090
  %2098 = shl i128 %2077, 32
  %2099 = shl nuw nsw i128 %2092, 32
  %2100 = shl nuw nsw i128 %2092, 33
  %2101 = add nuw nsw i128 %2100, %2094
  %2102 = add nsw i128 %2101, %2097
  %2103 = sub i128 %2102, %2098
  %2104 = shl nuw nsw i128 %2092, 1
  %2105 = shl nuw nsw i128 %2090, 32
  %2106 = sub nsw i128 %2093, %2090
  %2107 = sub nsw i128 %2106, %2105
  %2108 = sub nsw i128 %2107, %2092
  %2109 = sub nsw i128 %2108, %2099
  %2110 = add i128 %2109, %2096
  %2111 = shl nuw nsw i128 %2090, 33
  %2112 = add nuw nsw i128 %2036, 1267650600228229401427983728656
  %2113 = add nuw nsw i128 %2112, %2049
  %2114 = add nuw nsw i128 %2113, %2032
  %2115 = add nuw nsw i128 %2114, %2045
  %2116 = add nuw nsw i128 %2115, %2042
  %2117 = add nuw nsw i128 %2116, %2111
  %2118 = add i128 %2117, %2104
  %2119 = sub i128 %2118, %2097
  %2120 = sub i128 %2119, %2095
  %2121 = mul nuw nsw i128 %2090, 3
  %2122 = add nuw nsw i128 %2048, 1267650600228229401427983728656
  %2123 = add nuw nsw i128 %2122, %2062
  %2124 = add nuw nsw i128 %2123, %2044
  %2125 = add nuw nsw i128 %2124, %2058
  %2126 = add nuw nsw i128 %2125, %2041
  %2127 = add nuw nsw i128 %2126, %2055
  %2128 = add i128 %2127, %2052
  %2129 = add i128 %2128, %2121
  %2130 = sub i128 %2129, %2099
  %2131 = sub i128 %2130, %2096
  %2132 = add i128 %2131, %2098
  %2133 = and i128 %1446, 18446744073709551615
  %2134 = mul nuw i128 %2133, %2133
  %2135 = lshr i128 %2134, 64
  %2136 = and i128 %2134, 18446744073709551615
  %2137 = and i128 %1645, 18446744073709551615
  %2138 = mul nuw i128 %2137, %2133
  %2139 = lshr i128 %2138, 64
  %2140 = shl i128 %2138, 1
  %2141 = and i128 %2140, 36893488147419103230
  %2142 = add nuw nsw i128 %2141, %2135
  %2143 = extractelement <2 x i128> %1455, i32 1
  %2144 = and i128 %2143, 18446744073709551615
  %2145 = mul nuw i128 %2144, %2133
  %2146 = lshr i128 %2145, 64
  %2147 = and i128 %2145, 18446744073709551615
  %2148 = add nuw nsw i128 %2147, %2139
  %2149 = extractelement <2 x i128> %1464, i32 1
  %2150 = and i128 %2149, 18446744073709551615
  %2151 = mul nuw i128 %2150, %2133
  %2152 = lshr i128 %2151, 64
  %2153 = and i128 %2151, 18446744073709551615
  %2154 = mul nuw i128 %2144, %2137
  %2155 = lshr i128 %2154, 64
  %2156 = and i128 %2154, 18446744073709551615
  %2157 = add nuw nsw i128 %2156, %2146
  %2158 = add nuw nsw i128 %2157, %2153
  %2159 = add nuw nsw i128 %2152, %2155
  %2160 = mul nuw i128 %2137, %2137
  %2161 = lshr i128 %2160, 64
  %2162 = and i128 %2160, 18446744073709551615
  %2163 = mul nuw i128 %2150, %2137
  %2164 = lshr i128 %2163, 64
  %2165 = and i128 %2163, 18446744073709551615
  %2166 = add nuw nsw i128 %2159, %2165
  %2167 = shl nuw nsw i128 %2166, 1
  %2168 = mul nuw i128 %2150, %2144
  %2169 = lshr i128 %2168, 64
  %2170 = and i128 %2168, 18446744073709551615
  %2171 = add nuw nsw i128 %2164, %2170
  %2172 = shl nuw nsw i128 %2171, 1
  %2173 = shl nuw nsw i128 %2169, 1
  %2174 = mul nuw i128 %2144, %2144
  %2175 = lshr i128 %2174, 64
  %2176 = and i128 %2174, 18446744073709551615
  %2177 = add nuw nsw i128 %2167, %2176
  %2178 = add nuw nsw i128 %2172, %2175
  %2179 = mul nuw i128 %2150, %2150
  %2180 = lshr i128 %2179, 64
  %2181 = and i128 %2179, 18446744073709551615
  %2182 = add nuw nsw i128 %2173, %2181
  %2183 = shl nuw nsw i128 %2178, 32
  %2184 = add nuw nsw i128 %2183, %2177
  %2185 = sub nsw i128 %2178, %2180
  %2186 = shl nuw nsw i128 %2177, 32
  %2187 = shl nuw nsw i128 %2182, 32
  %2188 = shl nuw nsw i128 %2182, 33
  %2189 = shl nuw nsw i128 %2180, 32
  %2190 = shl nuw nsw i128 %2180, 33
  %2191 = mul nuw nsw i128 %2180, 3
  %2192 = mul i128 %2110, -2
  %2193 = add nuw nsw i128 %2136, 41832469807531570247123463044592
  %2194 = sub nsw i128 %2193, %2180
  %2195 = sub nsw i128 %2194, %2189
  %2196 = sub nsw i128 %2195, %2182
  %2197 = sub nsw i128 %2196, %2187
  %2198 = add nsw i128 %2197, %2184
  %2199 = sub i128 %2198, %1958
  %2200 = add i128 %2199, %2192
  %2201 = mul i128 %2103, -2
  %2202 = or i128 %2142, 41832469807531570249391205777408
  %2203 = add nuw nsw i128 %2202, %2188
  %2204 = add nsw i128 %2203, %2185
  %2205 = sub nsw i128 %2204, %2186
  %2206 = sub i128 %2205, %1951
  %2207 = add i128 %2206, %2201
  %2208 = add nuw nsw i128 %2148, %2182
  %2209 = sub i128 %2208, %2120
  %2210 = shl i128 %2209, 1
  %2211 = add nuw nsw i128 %2162, 41832469807531570247123463045648
  %2212 = add nuw nsw i128 %2211, %2190
  %2213 = sub nsw i128 %2212, %2185
  %2214 = sub nsw i128 %2213, %2183
  %2215 = sub i128 %2214, %1968
  %2216 = add i128 %2215, %2210
  %2217 = sub i128 %2158, %2132
  %2218 = shl i128 %2217, 1
  %2219 = add nuw nsw i128 %2161, 41832469807531570247123463045648
  %2220 = add nuw nsw i128 %2219, %2191
  %2221 = sub nsw i128 %2220, %2187
  %2222 = sub nsw i128 %2221, %2184
  %2223 = add i128 %2222, %2186
  %2224 = sub i128 %2223, %1980
  %2225 = add i128 %2224, %2218
  %2226 = add i128 %2120, 162259276829213363382781917267968
  %2227 = sub i128 %2226, %2216
  %2228 = lshr i128 %2227, 64
  %2229 = add i128 %2132, 162259276829231810126851331852288
  %2230 = sub i128 %2229, %2225
  %2231 = add i128 %2230, %2228
  %2232 = and i128 %2227, 18446744073709551615
  %2233 = add nuw nsw i128 %2232, 18446673704965373952
  %2234 = lshr i128 %2231, 64
  %2235 = trunc i128 %2234 to i64
  %2236 = and i128 %2231, 18446744073709551615
  %2237 = sub nsw i128 %2236, %2234
  %2238 = shl nuw nsw i128 %2234, 32
  %2239 = add nsw i128 %2237, %2238
  %2240 = lshr i128 %2239, 64
  %2241 = trunc i128 %2240 to i64
  %2242 = add i64 %2241, %2235
  %2243 = and i128 %2239, 18446744073709551615
  %2244 = sub nsw i128 %2243, %2240
  %2245 = shl nuw nsw i128 %2240, 32
  %2246 = add nsw i128 %2244, %2245
  %2247 = zext i64 %2242 to i128
  %2248 = shl nuw nsw i128 %2247, 32
  %2249 = lshr i128 %2246, 1
  %2250 = trunc i128 %2249 to i64
  %2251 = trunc i128 %2246 to i64
  %2252 = and i64 %2251, 9223372036854775807
  %2253 = sub nsw i64 9223372032559808512, %2252
  %2254 = and i64 %2253, %2251
  %2255 = or i64 %2254, %2250
  %2256 = ashr i64 %2255, 63
  %2257 = zext i64 %2256 to i128
  %2258 = add i128 %2110, 162259276829231810126855626815487
  %2259 = sub i128 %2258, %2200
  %2260 = add i128 %2259, %2247
  %2261 = sub i128 %2260, %2257
  %2262 = and i64 %2256, 4294967295
  %2263 = zext i64 %2262 to i128
  %2264 = and i64 %2256, -4294967295
  %2265 = zext i64 %2264 to i128
  %2266 = sub nsw i128 %2246, %2265
  %2267 = lshr i128 %2261, 64
  %2268 = add i128 %2103, 1460333491462920270524206387560447
  %2269 = sub i128 %2268, %2207
  %2270 = sub i128 %2269, %2248
  %2271 = sub i128 %2270, %2263
  %2272 = add i128 %2271, %2267
  %2273 = lshr i128 %2272, 64
  %2274 = add nuw nsw i128 %2233, %2273
  %2275 = lshr i128 %2274, 64
  %2276 = add nsw i128 %2266, %2275
  %2277 = and i128 %2261, 18446744073709551615
  %2278 = mul nuw i128 %2277, %2133
  %2279 = lshr i128 %2278, 64
  %2280 = and i128 %2278, 18446744073709551615
  %2281 = and i128 %2272, 18446744073709551615
  %2282 = mul nuw i128 %2281, %2133
  %2283 = lshr i128 %2282, 64
  %2284 = and i128 %2282, 18446744073709551615
  %2285 = mul nuw i128 %2277, %2137
  %2286 = lshr i128 %2285, 64
  %2287 = and i128 %2285, 18446744073709551615
  %2288 = and i128 %2274, 18446744073709551615
  %2289 = mul nuw i128 %2288, %2133
  %2290 = lshr i128 %2289, 64
  %2291 = and i128 %2289, 18446744073709551615
  %2292 = mul nuw i128 %2281, %2137
  %2293 = lshr i128 %2292, 64
  %2294 = and i128 %2292, 18446744073709551615
  %2295 = mul nuw i128 %2277, %2144
  %2296 = lshr i128 %2295, 64
  %2297 = and i128 %2295, 18446744073709551615
  %2298 = add nuw nsw i128 %2297, %2286
  %2299 = add nuw nsw i128 %2298, %2283
  %2300 = add nuw nsw i128 %2299, %2294
  %2301 = add nuw nsw i128 %2300, %2291
  %2302 = and i128 %2276, 18446744073709551615
  %2303 = mul nuw i128 %2302, %2133
  %2304 = lshr i128 %2303, 64
  %2305 = and i128 %2303, 18446744073709551615
  %2306 = mul nuw i128 %2288, %2137
  %2307 = lshr i128 %2306, 64
  %2308 = and i128 %2306, 18446744073709551615
  %2309 = mul nuw i128 %2281, %2144
  %2310 = lshr i128 %2309, 64
  %2311 = and i128 %2309, 18446744073709551615
  %2312 = mul nuw i128 %2277, %2150
  %2313 = lshr i128 %2312, 64
  %2314 = and i128 %2312, 18446744073709551615
  %2315 = mul nuw i128 %2302, %2137
  %2316 = lshr i128 %2315, 64
  %2317 = and i128 %2315, 18446744073709551615
  %2318 = mul nuw i128 %2288, %2144
  %2319 = lshr i128 %2318, 64
  %2320 = and i128 %2318, 18446744073709551615
  %2321 = mul nuw i128 %2281, %2150
  %2322 = lshr i128 %2321, 64
  %2323 = and i128 %2321, 18446744073709551615
  %2324 = mul nuw i128 %2302, %2144
  %2325 = lshr i128 %2324, 64
  %2326 = and i128 %2324, 18446744073709551615
  %2327 = mul nuw i128 %2288, %2150
  %2328 = lshr i128 %2327, 64
  %2329 = and i128 %2327, 18446744073709551615
  %2330 = mul nuw i128 %2302, %2150
  %2331 = lshr i128 %2330, 64
  %2332 = and i128 %2330, 18446744073709551615
  %2333 = lshr i128 %956, 64
  %2334 = add nuw nsw i128 %2333, 18446744069414584320
  %2335 = add i128 %2334, %968
  %2336 = and i128 %956, 18446744073709551615
  %2337 = add nuw nsw i128 %2336, 18446673704965373952
  %2338 = add i128 %946, 18446744073709551615
  %2339 = add i128 %939, 1298074214633706907132628377272319
  %2340 = lshr i128 %2335, 64
  %2341 = trunc i128 %2340 to i64
  %2342 = and i128 %2335, 18446744073709551615
  %2343 = sub nsw i128 %2342, %2340
  %2344 = shl nuw nsw i128 %2340, 32
  %2345 = add nsw i128 %2343, %2344
  %2346 = lshr i128 %2345, 64
  %2347 = trunc i128 %2346 to i64
  %2348 = add i64 %2347, %2341
  %2349 = and i128 %2345, 18446744073709551615
  %2350 = sub nsw i128 %2349, %2346
  %2351 = shl nuw nsw i128 %2346, 32
  %2352 = add nsw i128 %2350, %2351
  %2353 = zext i64 %2348 to i128
  %2354 = add i128 %2338, %2353
  %2355 = shl nuw nsw i128 %2353, 32
  %2356 = sub i128 %2339, %2355
  %2357 = lshr i128 %2352, 1
  %2358 = trunc i128 %2357 to i64
  %2359 = trunc i128 %2352 to i64
  %2360 = and i64 %2359, 9223372036854775807
  %2361 = sub nsw i64 9223372032559808512, %2360
  %2362 = and i64 %2361, %2359
  %2363 = or i64 %2362, %2358
  %2364 = ashr i64 %2363, 63
  %2365 = zext i64 %2364 to i128
  %2366 = sub i128 %2354, %2365
  %2367 = and i64 %2364, 4294967295
  %2368 = zext i64 %2367 to i128
  %2369 = sub i128 %2356, %2368
  %2370 = and i64 %2364, -4294967295
  %2371 = zext i64 %2370 to i128
  %2372 = sub nsw i128 %2352, %2371
  %2373 = lshr i128 %2366, 64
  %2374 = add i128 %2369, %2373
  %2375 = lshr i128 %2374, 64
  %2376 = add nuw nsw i128 %2337, %2375
  %2377 = lshr i128 %2376, 64
  %2378 = add nsw i128 %2372, %2377
  %2379 = lshr i128 %1968, 64
  %2380 = add nuw nsw i128 %2379, 18446744069414584320
  %2381 = add i128 %2380, %1980
  %2382 = and i128 %1968, 18446744073709551615
  %2383 = add nuw nsw i128 %2382, 18446673704965373952
  %2384 = add i128 %1958, 18446744073709551615
  %2385 = add i128 %1951, 1298074214633706907132628377272319
  %2386 = lshr i128 %2381, 64
  %2387 = trunc i128 %2386 to i64
  %2388 = and i128 %2381, 18446744073709551615
  %2389 = sub nsw i128 %2388, %2386
  %2390 = shl nuw nsw i128 %2386, 32
  %2391 = add nsw i128 %2389, %2390
  %2392 = lshr i128 %2391, 64
  %2393 = trunc i128 %2392 to i64
  %2394 = add i64 %2393, %2387
  %2395 = and i128 %2391, 18446744073709551615
  %2396 = sub nsw i128 %2395, %2392
  %2397 = shl nuw nsw i128 %2392, 32
  %2398 = add nsw i128 %2396, %2397
  %2399 = zext i64 %2394 to i128
  %2400 = add i128 %2384, %2399
  %2401 = shl nuw nsw i128 %2399, 32
  %2402 = sub i128 %2385, %2401
  %2403 = lshr i128 %2398, 1
  %2404 = trunc i128 %2403 to i64
  %2405 = trunc i128 %2398 to i64
  %2406 = and i64 %2405, 9223372036854775807
  %2407 = sub nsw i64 9223372032559808512, %2406
  %2408 = and i64 %2407, %2405
  %2409 = or i64 %2408, %2404
  %2410 = ashr i64 %2409, 63
  %2411 = zext i64 %2410 to i128
  %2412 = sub i128 %2400, %2411
  %2413 = and i64 %2410, 4294967295
  %2414 = zext i64 %2413 to i128
  %2415 = sub i128 %2402, %2414
  %2416 = and i64 %2410, -4294967295
  %2417 = zext i64 %2416 to i128
  %2418 = sub nsw i128 %2398, %2417
  %2419 = lshr i128 %2412, 64
  %2420 = add i128 %2415, %2419
  %2421 = lshr i128 %2420, 64
  %2422 = add nuw nsw i128 %2383, %2421
  %2423 = lshr i128 %2422, 64
  %2424 = add nsw i128 %2418, %2423
  %2425 = and i128 %2366, 18446744073709551615
  %2426 = and i128 %2412, 18446744073709551615
  %2427 = mul nuw i128 %2426, %2425
  %2428 = lshr i128 %2427, 64
  %2429 = and i128 %2420, 18446744073709551615
  %2430 = mul nuw i128 %2429, %2425
  %2431 = lshr i128 %2430, 64
  %2432 = and i128 %2430, 18446744073709551615
  %2433 = and i128 %2374, 18446744073709551615
  %2434 = mul nuw i128 %2426, %2433
  %2435 = lshr i128 %2434, 64
  %2436 = and i128 %2434, 18446744073709551615
  %2437 = add nuw nsw i128 %2436, %2428
  %2438 = add nuw nsw i128 %2437, %2432
  %2439 = and i128 %2422, 18446744073709551615
  %2440 = mul nuw i128 %2439, %2425
  %2441 = lshr i128 %2440, 64
  %2442 = and i128 %2440, 18446744073709551615
  %2443 = mul nuw i128 %2429, %2433
  %2444 = lshr i128 %2443, 64
  %2445 = and i128 %2443, 18446744073709551615
  %2446 = and i128 %2376, 18446744073709551615
  %2447 = mul nuw i128 %2426, %2446
  %2448 = lshr i128 %2447, 64
  %2449 = and i128 %2447, 18446744073709551615
  %2450 = add nuw nsw i128 %2449, %2435
  %2451 = add nuw nsw i128 %2450, %2431
  %2452 = add nuw nsw i128 %2451, %2445
  %2453 = add nuw nsw i128 %2452, %2442
  %2454 = and i128 %2424, 18446744073709551615
  %2455 = mul nuw i128 %2454, %2425
  %2456 = lshr i128 %2455, 64
  %2457 = and i128 %2455, 18446744073709551615
  %2458 = mul nuw i128 %2439, %2433
  %2459 = lshr i128 %2458, 64
  %2460 = and i128 %2458, 18446744073709551615
  %2461 = mul nuw i128 %2429, %2446
  %2462 = lshr i128 %2461, 64
  %2463 = and i128 %2461, 18446744073709551615
  %2464 = and i128 %2378, 18446744073709551615
  %2465 = mul nuw i128 %2426, %2464
  %2466 = lshr i128 %2465, 64
  %2467 = and i128 %2465, 18446744073709551615
  %2468 = add nuw nsw i128 %2467, %2448
  %2469 = add nuw nsw i128 %2468, %2444
  %2470 = add nuw nsw i128 %2469, %2463
  %2471 = add nuw nsw i128 %2470, %2441
  %2472 = add nuw nsw i128 %2471, %2460
  %2473 = add nuw nsw i128 %2472, %2457
  %2474 = mul nuw i128 %2454, %2433
  %2475 = lshr i128 %2474, 64
  %2476 = and i128 %2474, 18446744073709551615
  %2477 = mul nuw i128 %2439, %2446
  %2478 = lshr i128 %2477, 64
  %2479 = and i128 %2477, 18446744073709551615
  %2480 = mul nuw i128 %2429, %2464
  %2481 = lshr i128 %2480, 64
  %2482 = and i128 %2480, 18446744073709551615
  %2483 = add nuw nsw i128 %2462, %2466
  %2484 = add nuw nsw i128 %2483, %2482
  %2485 = add nuw nsw i128 %2484, %2459
  %2486 = add nuw nsw i128 %2485, %2479
  %2487 = add nuw nsw i128 %2486, %2456
  %2488 = add nuw nsw i128 %2487, %2476
  %2489 = mul nuw i128 %2454, %2446
  %2490 = lshr i128 %2489, 64
  %2491 = and i128 %2489, 18446744073709551615
  %2492 = mul nuw i128 %2439, %2464
  %2493 = lshr i128 %2492, 64
  %2494 = and i128 %2492, 18446744073709551615
  %2495 = add nuw nsw i128 %2478, %2481
  %2496 = add nuw nsw i128 %2495, %2494
  %2497 = add nuw nsw i128 %2496, %2475
  %2498 = add nuw nsw i128 %2497, %2491
  %2499 = add nuw nsw i128 %2490, %2493
  %2500 = mul nuw i128 %2454, %2464
  %2501 = lshr i128 %2500, 64
  %2502 = and i128 %2500, 18446744073709551615
  %2503 = add nuw nsw i128 %2499, %2502
  %2504 = shl i128 %2427, 1
  %2505 = and i128 %2504, 36893488147419103230
  %2506 = shl nuw nsw i128 %2438, 1
  %2507 = shl nuw nsw i128 %2453, 1
  %2508 = shl i128 %2473, 1
  %2509 = shl i128 %2488, 1
  %2510 = shl nuw nsw i128 %2498, 1
  %2511 = shl nuw nsw i128 %2503, 1
  %2512 = shl nuw nsw i128 %2501, 1
  %2513 = or i128 %2301, 1180591620717411303424
  %2514 = add nuw nsw i128 %2279, 1180591621816922931200
  %2515 = add nuw nsw i128 %2514, %2287
  %2516 = add nuw nsw i128 %2515, %2284
  %2517 = sub nsw i128 %2516, %2506
  %2518 = add nuw nsw i128 %2313, 1180591620717411303360
  %2519 = add nuw nsw i128 %2518, %2310
  %2520 = add nuw nsw i128 %2519, %2323
  %2521 = add nuw nsw i128 %2520, %2307
  %2522 = add nuw nsw i128 %2521, %2320
  %2523 = add nuw nsw i128 %2522, %2304
  %2524 = add i128 %2523, %2317
  %2525 = sub i128 %2524, %2509
  %2526 = add nuw nsw i128 %2322, 1180591620717411303360
  %2527 = add nuw nsw i128 %2526, %2319
  %2528 = add nuw nsw i128 %2527, %2329
  %2529 = add nuw nsw i128 %2528, %2316
  %2530 = add nuw nsw i128 %2529, %2326
  %2531 = sub nsw i128 %2530, %2510
  %2532 = add nuw nsw i128 %2328, 1180591620717411303360
  %2533 = add nuw nsw i128 %2532, %2325
  %2534 = add nuw nsw i128 %2533, %2332
  %2535 = sub nsw i128 %2534, %2511
  %2536 = sub nsw i128 1180591620717411303360, %2512
  %2537 = add nuw nsw i128 %2536, %2331
  %2538 = or i128 %2517, 40564819207303340847894502572032
  %2539 = shl i128 %2531, 32
  %2540 = add i128 %2525, %2539
  %2541 = sub i128 %2531, %2537
  %2542 = shl i128 %2525, 32
  %2543 = shl nsw i128 %2535, 32
  %2544 = shl nsw i128 %2535, 33
  %2545 = add nsw i128 %2544, %2538
  %2546 = add i128 %2545, %2541
  %2547 = sub i128 %2546, %2542
  %2548 = shl nsw i128 %2535, 1
  %2549 = shl nuw nsw i128 %2537, 32
  %2550 = sub nsw i128 40564819208483932466412890619200, %2505
  %2551 = add nuw nsw i128 %2550, %2280
  %2552 = sub nsw i128 %2551, %2537
  %2553 = sub nsw i128 %2552, %2549
  %2554 = sub nsw i128 %2553, %2535
  %2555 = sub i128 %2554, %2543
  %2556 = add i128 %2555, %2540
  %2557 = shl nuw nsw i128 %2537, 33
  %2558 = sub nsw i128 40564819207303340845695479316992, %2507
  %2559 = add i128 %2558, %2513
  %2560 = add i128 %2559, %2557
  %2561 = add i128 %2560, %2548
  %2562 = sub i128 %2561, %2541
  %2563 = sub i128 %2562, %2539
  %2564 = mul nuw nsw i128 %2537, 3
  %2565 = add nuw nsw i128 %2296, 40564819208483932465038501085760
  %2566 = add nuw nsw i128 %2565, %2314
  %2567 = add nuw nsw i128 %2566, %2293
  %2568 = add nuw nsw i128 %2567, %2311
  %2569 = add nuw nsw i128 %2568, %2290
  %2570 = add nuw nsw i128 %2569, %2308
  %2571 = add i128 %2570, %2305
  %2572 = sub i128 %2571, %2508
  %2573 = add i128 %2572, %2564
  %2574 = sub i128 %2573, %2543
  %2575 = sub i128 %2574, %2540
  %2576 = add i128 %2575, %2542
  %2577 = xor i128 %1638, -1
  %2578 = extractelement <2 x i64> %124, i32 0
  %2579 = and i64 %2578, %969
  %2580 = zext i64 %2579 to i128
  %2581 = and i128 %2200, %2577
  %2582 = or i128 %2581, %2580
  %2583 = and i64 %2578, %980
  %2584 = zext i64 %2583 to i128
  %2585 = and i128 %2207, %2577
  %2586 = or i128 %2585, %2584
  %2587 = and i64 %2578, %995
  %2588 = zext i64 %2587 to i128
  %2589 = and i128 %2216, %2577
  %2590 = or i128 %2589, %2588
  %2591 = and i64 %2578, %1011
  %2592 = zext i64 %2591 to i128
  %2593 = and i128 %2225, %2577
  %2594 = or i128 %2593, %2592
  %2595 = xor i128 %2582, %391
  %2596 = and i128 %2595, %1639
  %2597 = xor i128 %2596, %2582
  %2598 = xor i128 %2586, %394
  %2599 = and i128 %2598, %1639
  %2600 = xor i128 %2599, %2586
  %2601 = xor i128 %2590, %386
  %2602 = and i128 %2601, %1639
  %2603 = xor i128 %2602, %2590
  %2604 = xor i128 %2594, %383
  %2605 = and i128 %2604, %1639
  %2606 = xor i128 %2605, %2594
  %2607 = and i64 %2578, %1274
  %2608 = zext i64 %2607 to i128
  %2609 = and i128 %2556, %2577
  %2610 = or i128 %2609, %2608
  %2611 = and i64 %2578, %1283
  %2612 = zext i64 %2611 to i128
  %2613 = and i128 %2547, %2577
  %2614 = or i128 %2613, %2612
  %2615 = and i64 %2578, %1295
  %2616 = zext i64 %2615 to i128
  %2617 = and i128 %2563, %2577
  %2618 = or i128 %2617, %2616
  %2619 = and i64 %2578, %1311
  %2620 = zext i64 %2619 to i128
  %2621 = and i128 %2576, %2577
  %2622 = or i128 %2621, %2620
  %2623 = xor i128 %2610, %751
  %2624 = and i128 %2623, %1639
  %2625 = xor i128 %2624, %2610
  %2626 = xor i128 %2614, %754
  %2627 = and i128 %2626, %1639
  %2628 = xor i128 %2627, %2614
  %2629 = xor i128 %2618, %746
  %2630 = and i128 %2629, %1639
  %2631 = xor i128 %2630, %2618
  %2632 = xor i128 %2622, %743
  %2633 = and i128 %2632, %1639
  %2634 = xor i128 %2633, %2622
  %2635 = and i64 %2578, %71
  %2636 = zext i64 %2635 to i128
  %2637 = and i128 %1688, %2577
  %2638 = or i128 %2637, %2636
  %2639 = and i64 %2578, %73
  %2640 = zext i64 %2639 to i128
  %2641 = and i128 %1698, %2577
  %2642 = or i128 %2641, %2640
  %2643 = and i64 %2578, %75
  %2644 = zext i64 %2643 to i128
  %2645 = and i128 %1680, %2577
  %2646 = or i128 %2645, %2644
  %2647 = and i64 %2578, %77
  %2648 = zext i64 %2647 to i128
  %2649 = and i128 %1664, %2577
  %2650 = or i128 %2649, %2648
  %2651 = xor i128 %2638, %19
  %2652 = and i128 %2651, %1639
  %2653 = xor i128 %2652, %2638
  %2654 = xor i128 %2642, %22
  %2655 = and i128 %2654, %1639
  %2656 = xor i128 %2655, %2642
  %2657 = xor i128 %2646, %14
  %2658 = and i128 %2657, %1639
  %2659 = xor i128 %2658, %2646
  %2660 = xor i128 %2650, %11
  %2661 = and i128 %2660, %1639
  %2662 = xor i128 %2661, %2650
  store i128 %2597, i128* %0, align 16
  %2663 = getelementptr inbounds i128, i128* %0, i64 1
  store i128 %2600, i128* %2663, align 16
  %2664 = getelementptr inbounds i128, i128* %0, i64 2
  store i128 %2603, i128* %2664, align 16
  %2665 = getelementptr inbounds i128, i128* %0, i64 3
  store i128 %2606, i128* %2665, align 16
  store i128 %2625, i128* %1, align 16
  %2666 = getelementptr inbounds i128, i128* %1, i64 1
  store i128 %2628, i128* %2666, align 16
  %2667 = getelementptr inbounds i128, i128* %1, i64 2
  store i128 %2631, i128* %2667, align 16
  %2668 = getelementptr inbounds i128, i128* %1, i64 3
  store i128 %2634, i128* %2668, align 16
  store i128 %2653, i128* %2, align 16
  %2669 = getelementptr inbounds i128, i128* %2, i64 1
  store i128 %2656, i128* %2669, align 16
  %2670 = getelementptr inbounds i128, i128* %2, i64 2
  store i128 %2659, i128* %2670, align 16
  %2671 = getelementptr inbounds i128, i128* %2, i64 3
  store i128 %2662, i128* %2671, align 16
  br label %2672

2672:                                             ; preds = %1644, %1643
  ret void
}

; Function Attrs: argmemonly nounwind willreturn
declare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #1

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jmulsw_nistp256_inner(i128* %0, i128* nocapture %1, i128* nocapture %2, %0* nocapture readnone %3, i64* nocapture readonly %4, i64* nocapture readonly %5, i64* nocapture readonly %6, %1* %7) local_unnamed_addr #0 {
  %9 = alloca [4 x i128], align 16
  %10 = alloca [4 x i128], align 16
  %11 = alloca [4 x i128], align 16
  %12 = alloca [4 x i128], align 16
  %13 = alloca [4 x i128], align 16
  %14 = alloca [4 x i128], align 16
  %15 = tail call i64 @__gmpz_sizeinbase(%1* %7, i32 2) #6
  %16 = trunc i64 %15 to i32
  %17 = sdiv i32 %16, 2
  %18 = sitofp i32 %17 to float
  %19 = sitofp i32 %16 to float
  br label %20

20:                                               ; preds = %20, %8
  %21 = phi float [ %18, %8 ], [ %33, %20 ]
  %22 = phi i32 [ 1, %8 ], [ %23, %20 ]
  %23 = add nuw nsw i32 %22, 1
  %24 = shl i32 1, %22
  %25 = sitofp i32 %24 to float
  %26 = shl i32 2, %22
  %27 = add nsw i32 %26, -1
  %28 = sitofp i32 %27 to float
  %29 = fmul float %19, %28
  %30 = mul nsw i32 %26, %23
  %31 = sitofp i32 %30 to float
  %32 = fdiv float %29, %31
  %33 = fadd float %32, %25
  %34 = fcmp olt float %33, %21
  br i1 %34, label %20, label %35

35:                                               ; preds = %20
  %36 = add nsw i32 %22, -1
  %37 = shl i32 1, %36
  %38 = sext i32 %37 to i64
  %39 = shl nsw i64 %38, 5
  %40 = tail call noalias i8* @malloc(i64 %39) #7
  %41 = bitcast i8* %40 to [4 x i64]*
  %42 = tail call noalias i8* @malloc(i64 %39) #7
  %43 = bitcast i8* %42 to [4 x i64]*
  %44 = tail call noalias i8* @malloc(i64 %39) #7
  %45 = bitcast i8* %44 to [4 x i64]*
  %46 = add nsw i32 %37, -1
  %47 = sext i32 %46 to i64
  %48 = getelementptr inbounds [4 x i64], [4 x i64]* %41, i64 %47, i64 0
  %49 = getelementptr inbounds [4 x i64], [4 x i64]* %43, i64 %47, i64 0
  %50 = getelementptr inbounds [4 x i64], [4 x i64]* %45, i64 %47, i64 0
  %51 = bitcast [4 x i128]* %9 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %51) #7
  %52 = bitcast [4 x i128]* %10 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %52) #7
  %53 = bitcast [4 x i128]* %11 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %53) #7
  %54 = bitcast [4 x i128]* %12 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %54) #7
  %55 = bitcast [4 x i128]* %13 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %55) #7
  %56 = bitcast [4 x i128]* %14 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %56) #7
  %57 = getelementptr inbounds [4 x i128], [4 x i128]* %12, i64 0, i64 0
  %58 = load i64, i64* %4, align 8
  %59 = zext i64 %58 to i128
  store i128 %59, i128* %57, align 16
  %60 = getelementptr inbounds i64, i64* %4, i64 1
  %61 = load i64, i64* %60, align 8
  %62 = zext i64 %61 to i128
  %63 = getelementptr inbounds [4 x i128], [4 x i128]* %12, i64 0, i64 1
  store i128 %62, i128* %63, align 16
  %64 = getelementptr inbounds i64, i64* %4, i64 2
  %65 = load i64, i64* %64, align 8
  %66 = zext i64 %65 to i128
  %67 = getelementptr inbounds [4 x i128], [4 x i128]* %12, i64 0, i64 2
  store i128 %66, i128* %67, align 16
  %68 = getelementptr inbounds i64, i64* %4, i64 3
  %69 = load i64, i64* %68, align 8
  %70 = zext i64 %69 to i128
  %71 = getelementptr inbounds [4 x i128], [4 x i128]* %12, i64 0, i64 3
  store i128 %70, i128* %71, align 16
  %72 = getelementptr inbounds [4 x i128], [4 x i128]* %13, i64 0, i64 0
  %73 = load i64, i64* %5, align 8
  %74 = zext i64 %73 to i128
  store i128 %74, i128* %72, align 16
  %75 = getelementptr inbounds i64, i64* %5, i64 1
  %76 = load i64, i64* %75, align 8
  %77 = zext i64 %76 to i128
  %78 = getelementptr inbounds [4 x i128], [4 x i128]* %13, i64 0, i64 1
  store i128 %77, i128* %78, align 16
  %79 = getelementptr inbounds i64, i64* %5, i64 2
  %80 = load i64, i64* %79, align 8
  %81 = zext i64 %80 to i128
  %82 = getelementptr inbounds [4 x i128], [4 x i128]* %13, i64 0, i64 2
  store i128 %81, i128* %82, align 16
  %83 = getelementptr inbounds i64, i64* %5, i64 3
  %84 = load i64, i64* %83, align 8
  %85 = zext i64 %84 to i128
  %86 = getelementptr inbounds [4 x i128], [4 x i128]* %13, i64 0, i64 3
  store i128 %85, i128* %86, align 16
  %87 = getelementptr inbounds [4 x i128], [4 x i128]* %14, i64 0, i64 0
  %88 = load i64, i64* %6, align 8
  %89 = zext i64 %88 to i128
  store i128 %89, i128* %87, align 16
  %90 = getelementptr inbounds i64, i64* %6, i64 1
  %91 = load i64, i64* %90, align 8
  %92 = zext i64 %91 to i128
  %93 = getelementptr inbounds [4 x i128], [4 x i128]* %14, i64 0, i64 1
  store i128 %92, i128* %93, align 16
  %94 = getelementptr inbounds i64, i64* %6, i64 2
  %95 = load i64, i64* %94, align 8
  %96 = zext i64 %95 to i128
  %97 = getelementptr inbounds [4 x i128], [4 x i128]* %14, i64 0, i64 2
  store i128 %96, i128* %97, align 16
  %98 = getelementptr inbounds i64, i64* %6, i64 3
  %99 = load i64, i64* %98, align 8
  %100 = zext i64 %99 to i128
  %101 = getelementptr inbounds [4 x i128], [4 x i128]* %14, i64 0, i64 3
  store i128 %100, i128* %101, align 16
  %102 = getelementptr inbounds [4 x i128], [4 x i128]* %9, i64 0, i64 0
  %103 = getelementptr inbounds [4 x i128], [4 x i128]* %10, i64 0, i64 0
  %104 = getelementptr inbounds [4 x i128], [4 x i128]* %11, i64 0, i64 0
  call fastcc void @0(i128* nonnull %102, i128* nonnull %103, i128* nonnull %104, i128* nonnull %57, i128* nonnull %72, i128* nonnull %87) #7
  %105 = getelementptr inbounds [4 x i128], [4 x i128]* %9, i64 0, i64 3
  %106 = load i128, i128* %105, align 16
  %107 = add i128 %106, 18446744069414584320
  %108 = getelementptr inbounds [4 x i128], [4 x i128]* %9, i64 0, i64 2
  %109 = load i128, i128* %108, align 16
  %110 = lshr i128 %109, 64
  %111 = add i128 %107, %110
  %112 = and i128 %109, 18446744073709551615
  %113 = add nuw nsw i128 %112, 18446673704965373952
  %114 = load i128, i128* %102, align 16
  %115 = add i128 %114, 18446744073709551615
  %116 = getelementptr inbounds [4 x i128], [4 x i128]* %9, i64 0, i64 1
  %117 = load i128, i128* %116, align 16
  %118 = add i128 %117, 1298074214633706907132628377272319
  %119 = lshr i128 %111, 64
  %120 = trunc i128 %119 to i64
  %121 = and i128 %111, 18446744073709551615
  %122 = sub nsw i128 %121, %119
  %123 = shl nuw nsw i128 %119, 32
  %124 = add nsw i128 %122, %123
  %125 = lshr i128 %124, 64
  %126 = trunc i128 %125 to i64
  %127 = add i64 %126, %120
  %128 = and i128 %124, 18446744073709551615
  %129 = sub nsw i128 %128, %125
  %130 = shl nuw nsw i128 %125, 32
  %131 = add nsw i128 %129, %130
  %132 = zext i64 %127 to i128
  %133 = add i128 %115, %132
  %134 = shl nuw nsw i128 %132, 32
  %135 = sub i128 %118, %134
  %136 = lshr i128 %131, 1
  %137 = trunc i128 %136 to i64
  %138 = trunc i128 %131 to i64
  %139 = and i64 %138, 9223372036854775807
  %140 = sub nsw i64 9223372032559808512, %139
  %141 = and i64 %140, %138
  %142 = or i64 %141, %137
  %143 = ashr i64 %142, 63
  %144 = zext i64 %143 to i128
  %145 = sub i128 %133, %144
  %146 = and i64 %143, 4294967295
  %147 = zext i64 %146 to i128
  %148 = sub i128 %135, %147
  %149 = and i64 %143, -4294967295
  %150 = zext i64 %149 to i128
  %151 = sub nsw i128 %131, %150
  %152 = lshr i128 %145, 64
  %153 = add i128 %148, %152
  %154 = trunc i128 %145 to i64
  %155 = lshr i128 %153, 64
  %156 = add nuw nsw i128 %113, %155
  %157 = trunc i128 %153 to i64
  %158 = lshr i128 %156, 64
  %159 = add nsw i128 %151, %158
  %160 = trunc i128 %156 to i64
  store i64 %154, i64* %48, align 8
  %161 = getelementptr inbounds [4 x i64], [4 x i64]* %41, i64 %47, i64 1
  store i64 %157, i64* %161, align 8
  %162 = getelementptr inbounds [4 x i64], [4 x i64]* %41, i64 %47, i64 2
  store i64 %160, i64* %162, align 8
  %163 = trunc i128 %159 to i64
  %164 = getelementptr inbounds [4 x i64], [4 x i64]* %41, i64 %47, i64 3
  store i64 %163, i64* %164, align 8
  %165 = getelementptr inbounds [4 x i128], [4 x i128]* %10, i64 0, i64 3
  %166 = load i128, i128* %165, align 16
  %167 = add i128 %166, 18446744069414584320
  %168 = getelementptr inbounds [4 x i128], [4 x i128]* %10, i64 0, i64 2
  %169 = load i128, i128* %168, align 16
  %170 = lshr i128 %169, 64
  %171 = add i128 %167, %170
  %172 = and i128 %169, 18446744073709551615
  %173 = add nuw nsw i128 %172, 18446673704965373952
  %174 = load i128, i128* %103, align 16
  %175 = add i128 %174, 18446744073709551615
  %176 = getelementptr inbounds [4 x i128], [4 x i128]* %10, i64 0, i64 1
  %177 = load i128, i128* %176, align 16
  %178 = add i128 %177, 1298074214633706907132628377272319
  %179 = lshr i128 %171, 64
  %180 = trunc i128 %179 to i64
  %181 = and i128 %171, 18446744073709551615
  %182 = sub nsw i128 %181, %179
  %183 = shl nuw nsw i128 %179, 32
  %184 = add nsw i128 %182, %183
  %185 = lshr i128 %184, 64
  %186 = trunc i128 %185 to i64
  %187 = add i64 %186, %180
  %188 = and i128 %184, 18446744073709551615
  %189 = sub nsw i128 %188, %185
  %190 = shl nuw nsw i128 %185, 32
  %191 = add nsw i128 %189, %190
  %192 = zext i64 %187 to i128
  %193 = add i128 %175, %192
  %194 = shl nuw nsw i128 %192, 32
  %195 = sub i128 %178, %194
  %196 = lshr i128 %191, 1
  %197 = trunc i128 %196 to i64
  %198 = trunc i128 %191 to i64
  %199 = and i64 %198, 9223372036854775807
  %200 = sub nsw i64 9223372032559808512, %199
  %201 = and i64 %200, %198
  %202 = or i64 %201, %197
  %203 = ashr i64 %202, 63
  %204 = zext i64 %203 to i128
  %205 = sub i128 %193, %204
  %206 = and i64 %203, 4294967295
  %207 = zext i64 %206 to i128
  %208 = sub i128 %195, %207
  %209 = and i64 %203, -4294967295
  %210 = zext i64 %209 to i128
  %211 = sub nsw i128 %191, %210
  %212 = lshr i128 %205, 64
  %213 = add i128 %208, %212
  %214 = trunc i128 %205 to i64
  %215 = lshr i128 %213, 64
  %216 = add nuw nsw i128 %173, %215
  %217 = trunc i128 %213 to i64
  %218 = lshr i128 %216, 64
  %219 = add nsw i128 %211, %218
  %220 = trunc i128 %216 to i64
  store i64 %214, i64* %49, align 8
  %221 = getelementptr inbounds [4 x i64], [4 x i64]* %43, i64 %47, i64 1
  store i64 %217, i64* %221, align 8
  %222 = getelementptr inbounds [4 x i64], [4 x i64]* %43, i64 %47, i64 2
  store i64 %220, i64* %222, align 8
  %223 = trunc i128 %219 to i64
  %224 = getelementptr inbounds [4 x i64], [4 x i64]* %43, i64 %47, i64 3
  store i64 %223, i64* %224, align 8
  %225 = getelementptr inbounds [4 x i128], [4 x i128]* %11, i64 0, i64 3
  %226 = load i128, i128* %225, align 16
  %227 = add i128 %226, 18446744069414584320
  %228 = getelementptr inbounds [4 x i128], [4 x i128]* %11, i64 0, i64 2
  %229 = load i128, i128* %228, align 16
  %230 = lshr i128 %229, 64
  %231 = add i128 %227, %230
  %232 = and i128 %229, 18446744073709551615
  %233 = add nuw nsw i128 %232, 18446673704965373952
  %234 = load i128, i128* %104, align 16
  %235 = add i128 %234, 18446744073709551615
  %236 = getelementptr inbounds [4 x i128], [4 x i128]* %11, i64 0, i64 1
  %237 = load i128, i128* %236, align 16
  %238 = add i128 %237, 1298074214633706907132628377272319
  %239 = lshr i128 %231, 64
  %240 = trunc i128 %239 to i64
  %241 = and i128 %231, 18446744073709551615
  %242 = sub nsw i128 %241, %239
  %243 = shl nuw nsw i128 %239, 32
  %244 = add nsw i128 %242, %243
  %245 = lshr i128 %244, 64
  %246 = trunc i128 %245 to i64
  %247 = add i64 %246, %240
  %248 = and i128 %244, 18446744073709551615
  %249 = sub nsw i128 %248, %245
  %250 = shl nuw nsw i128 %245, 32
  %251 = add nsw i128 %249, %250
  %252 = zext i64 %247 to i128
  %253 = add i128 %235, %252
  %254 = shl nuw nsw i128 %252, 32
  %255 = sub i128 %238, %254
  %256 = lshr i128 %251, 1
  %257 = trunc i128 %256 to i64
  %258 = trunc i128 %251 to i64
  %259 = and i64 %258, 9223372036854775807
  %260 = sub nsw i64 9223372032559808512, %259
  %261 = and i64 %260, %258
  %262 = or i64 %261, %257
  %263 = ashr i64 %262, 63
  %264 = zext i64 %263 to i128
  %265 = sub i128 %253, %264
  %266 = and i64 %263, 4294967295
  %267 = zext i64 %266 to i128
  %268 = sub i128 %255, %267
  %269 = and i64 %263, -4294967295
  %270 = zext i64 %269 to i128
  %271 = sub nsw i128 %251, %270
  %272 = lshr i128 %265, 64
  %273 = add i128 %268, %272
  %274 = trunc i128 %265 to i64
  %275 = lshr i128 %273, 64
  %276 = add nuw nsw i128 %233, %275
  %277 = trunc i128 %273 to i64
  %278 = lshr i128 %276, 64
  %279 = add nsw i128 %271, %278
  %280 = trunc i128 %276 to i64
  store i64 %274, i64* %50, align 8
  %281 = getelementptr inbounds [4 x i64], [4 x i64]* %45, i64 %47, i64 1
  store i64 %277, i64* %281, align 8
  %282 = getelementptr inbounds [4 x i64], [4 x i64]* %45, i64 %47, i64 2
  store i64 %280, i64* %282, align 8
  %283 = trunc i128 %279 to i64
  %284 = getelementptr inbounds [4 x i64], [4 x i64]* %45, i64 %47, i64 3
  store i64 %283, i64* %284, align 8
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %56) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %55) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %54) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %53) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %52) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %51) #7
  %285 = bitcast i64* %4 to <2 x i64>*
  %286 = load <2 x i64>, <2 x i64>* %285, align 8
  %287 = bitcast i8* %40 to <2 x i64>*
  store <2 x i64> %286, <2 x i64>* %287, align 8
  %288 = getelementptr inbounds i8, i8* %40, i64 16
  %289 = bitcast i64* %64 to <2 x i64>*
  %290 = load <2 x i64>, <2 x i64>* %289, align 8
  %291 = bitcast i8* %288 to <2 x i64>*
  store <2 x i64> %290, <2 x i64>* %291, align 8
  %292 = bitcast i8* %42 to i64*
  %293 = load i64, i64* %5, align 8
  store i64 %293, i64* %292, align 8
  %294 = load i64, i64* %75, align 8
  %295 = getelementptr inbounds i8, i8* %42, i64 8
  %296 = bitcast i8* %295 to i64*
  store i64 %294, i64* %296, align 8
  %297 = load i64, i64* %79, align 8
  %298 = getelementptr inbounds i8, i8* %42, i64 16
  %299 = bitcast i8* %298 to i64*
  store i64 %297, i64* %299, align 8
  %300 = load i64, i64* %83, align 8
  %301 = getelementptr inbounds i8, i8* %42, i64 24
  %302 = bitcast i8* %301 to i64*
  store i64 %300, i64* %302, align 8
  %303 = bitcast i8* %44 to i64*
  %304 = load i64, i64* %6, align 8
  store i64 %304, i64* %303, align 8
  %305 = load i64, i64* %90, align 8
  %306 = getelementptr inbounds i8, i8* %44, i64 8
  %307 = bitcast i8* %306 to i64*
  store i64 %305, i64* %307, align 8
  %308 = load i64, i64* %94, align 8
  %309 = getelementptr inbounds i8, i8* %44, i64 16
  %310 = bitcast i8* %309 to i64*
  store i64 %308, i64* %310, align 8
  %311 = load i64, i64* %98, align 8
  %312 = getelementptr inbounds i8, i8* %44, i64 24
  %313 = bitcast i8* %312 to i64*
  store i64 %311, i64* %313, align 8
  %314 = icmp sgt i32 %37, 1
  br i1 %314, label %315, label %524

315:                                              ; preds = %35
  %316 = zext i32 %37 to i64
  br label %317

317:                                              ; preds = %317, %315
  %318 = phi i64 [ %311, %315 ], [ %520, %317 ]
  %319 = phi i64 [ %308, %315 ], [ %517, %317 ]
  %320 = phi i64 [ %305, %315 ], [ %514, %317 ]
  %321 = phi i64 [ %304, %315 ], [ %511, %317 ]
  %322 = phi i64 [ %300, %315 ], [ %463, %317 ]
  %323 = phi i64 [ %297, %315 ], [ %460, %317 ]
  %324 = phi i64 [ %294, %315 ], [ %457, %317 ]
  %325 = phi i64 [ %293, %315 ], [ %454, %317 ]
  %326 = phi i64 [ 1, %315 ], [ %522, %317 ]
  %327 = getelementptr inbounds [4 x i64], [4 x i64]* %41, i64 %326, i64 0
  %328 = getelementptr inbounds [4 x i64], [4 x i64]* %43, i64 %326, i64 0
  %329 = getelementptr inbounds [4 x i64], [4 x i64]* %45, i64 %326, i64 0
  %330 = add nsw i64 %326, -1
  %331 = getelementptr inbounds [4 x i64], [4 x i64]* %41, i64 %330, i64 0
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %51) #7
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %52) #7
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %53) #7
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %54) #7
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %55) #7
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %56) #7
  %332 = load i64, i64* %331, align 8
  %333 = zext i64 %332 to i128
  store i128 %333, i128* %57, align 16
  %334 = getelementptr inbounds [4 x i64], [4 x i64]* %41, i64 %330, i64 1
  %335 = load i64, i64* %334, align 8
  %336 = zext i64 %335 to i128
  store i128 %336, i128* %63, align 16
  %337 = getelementptr inbounds [4 x i64], [4 x i64]* %41, i64 %330, i64 2
  %338 = load i64, i64* %337, align 8
  %339 = zext i64 %338 to i128
  store i128 %339, i128* %67, align 16
  %340 = getelementptr inbounds [4 x i64], [4 x i64]* %41, i64 %330, i64 3
  %341 = load i64, i64* %340, align 8
  %342 = zext i64 %341 to i128
  store i128 %342, i128* %71, align 16
  %343 = zext i64 %325 to i128
  store i128 %343, i128* %72, align 16
  %344 = zext i64 %324 to i128
  store i128 %344, i128* %78, align 16
  %345 = zext i64 %323 to i128
  store i128 %345, i128* %82, align 16
  %346 = zext i64 %322 to i128
  store i128 %346, i128* %86, align 16
  %347 = zext i64 %321 to i128
  store i128 %347, i128* %87, align 16
  %348 = zext i64 %320 to i128
  store i128 %348, i128* %93, align 16
  %349 = zext i64 %319 to i128
  store i128 %349, i128* %97, align 16
  %350 = zext i64 %318 to i128
  store i128 %350, i128* %101, align 16
  call fastcc void @1(i128* nonnull %102, i128* nonnull %103, i128* nonnull %104, i128* nonnull %57, i128* nonnull %72, i128* nonnull %87, i64* %48, i64* %49, i64* %50) #7
  %351 = load i128, i128* %105, align 16
  %352 = add i128 %351, 18446744069414584320
  %353 = load i128, i128* %108, align 16
  %354 = lshr i128 %353, 64
  %355 = add i128 %352, %354
  %356 = and i128 %353, 18446744073709551615
  %357 = add nuw nsw i128 %356, 18446673704965373952
  %358 = load i128, i128* %102, align 16
  %359 = add i128 %358, 18446744073709551615
  %360 = load i128, i128* %116, align 16
  %361 = add i128 %360, 1298074214633706907132628377272319
  %362 = lshr i128 %355, 64
  %363 = trunc i128 %362 to i64
  %364 = and i128 %355, 18446744073709551615
  %365 = sub nsw i128 %364, %362
  %366 = shl nuw nsw i128 %362, 32
  %367 = add nsw i128 %365, %366
  %368 = lshr i128 %367, 64
  %369 = trunc i128 %368 to i64
  %370 = add i64 %369, %363
  %371 = and i128 %367, 18446744073709551615
  %372 = sub nsw i128 %371, %368
  %373 = shl nuw nsw i128 %368, 32
  %374 = add nsw i128 %372, %373
  %375 = zext i64 %370 to i128
  %376 = add i128 %359, %375
  %377 = shl nuw nsw i128 %375, 32
  %378 = sub i128 %361, %377
  %379 = lshr i128 %374, 1
  %380 = trunc i128 %379 to i64
  %381 = trunc i128 %374 to i64
  %382 = and i64 %381, 9223372036854775807
  %383 = sub nsw i64 9223372032559808512, %382
  %384 = and i64 %383, %381
  %385 = or i64 %384, %380
  %386 = ashr i64 %385, 63
  %387 = zext i64 %386 to i128
  %388 = sub i128 %376, %387
  %389 = and i64 %386, 4294967295
  %390 = zext i64 %389 to i128
  %391 = sub i128 %378, %390
  %392 = and i64 %386, -4294967295
  %393 = zext i64 %392 to i128
  %394 = sub nsw i128 %374, %393
  %395 = lshr i128 %388, 64
  %396 = add i128 %391, %395
  %397 = trunc i128 %388 to i64
  %398 = lshr i128 %396, 64
  %399 = add nuw nsw i128 %357, %398
  %400 = trunc i128 %396 to i64
  %401 = lshr i128 %399, 64
  %402 = add nsw i128 %394, %401
  %403 = trunc i128 %399 to i64
  store i64 %397, i64* %327, align 8
  %404 = getelementptr inbounds [4 x i64], [4 x i64]* %41, i64 %326, i64 1
  store i64 %400, i64* %404, align 8
  %405 = getelementptr inbounds [4 x i64], [4 x i64]* %41, i64 %326, i64 2
  store i64 %403, i64* %405, align 8
  %406 = trunc i128 %402 to i64
  %407 = getelementptr inbounds [4 x i64], [4 x i64]* %41, i64 %326, i64 3
  store i64 %406, i64* %407, align 8
  %408 = load i128, i128* %165, align 16
  %409 = add i128 %408, 18446744069414584320
  %410 = load i128, i128* %168, align 16
  %411 = lshr i128 %410, 64
  %412 = add i128 %409, %411
  %413 = and i128 %410, 18446744073709551615
  %414 = add nuw nsw i128 %413, 18446673704965373952
  %415 = load i128, i128* %103, align 16
  %416 = add i128 %415, 18446744073709551615
  %417 = load i128, i128* %176, align 16
  %418 = add i128 %417, 1298074214633706907132628377272319
  %419 = lshr i128 %412, 64
  %420 = trunc i128 %419 to i64
  %421 = and i128 %412, 18446744073709551615
  %422 = sub nsw i128 %421, %419
  %423 = shl nuw nsw i128 %419, 32
  %424 = add nsw i128 %422, %423
  %425 = lshr i128 %424, 64
  %426 = trunc i128 %425 to i64
  %427 = add i64 %426, %420
  %428 = and i128 %424, 18446744073709551615
  %429 = sub nsw i128 %428, %425
  %430 = shl nuw nsw i128 %425, 32
  %431 = add nsw i128 %429, %430
  %432 = zext i64 %427 to i128
  %433 = add i128 %416, %432
  %434 = shl nuw nsw i128 %432, 32
  %435 = sub i128 %418, %434
  %436 = lshr i128 %431, 1
  %437 = trunc i128 %436 to i64
  %438 = trunc i128 %431 to i64
  %439 = and i64 %438, 9223372036854775807
  %440 = sub nsw i64 9223372032559808512, %439
  %441 = and i64 %440, %438
  %442 = or i64 %441, %437
  %443 = ashr i64 %442, 63
  %444 = zext i64 %443 to i128
  %445 = sub i128 %433, %444
  %446 = and i64 %443, 4294967295
  %447 = zext i64 %446 to i128
  %448 = sub i128 %435, %447
  %449 = and i64 %443, -4294967295
  %450 = zext i64 %449 to i128
  %451 = sub nsw i128 %431, %450
  %452 = lshr i128 %445, 64
  %453 = add i128 %448, %452
  %454 = trunc i128 %445 to i64
  %455 = lshr i128 %453, 64
  %456 = add nuw nsw i128 %414, %455
  %457 = trunc i128 %453 to i64
  %458 = lshr i128 %456, 64
  %459 = add nsw i128 %451, %458
  %460 = trunc i128 %456 to i64
  store i64 %454, i64* %328, align 8
  %461 = getelementptr inbounds [4 x i64], [4 x i64]* %43, i64 %326, i64 1
  store i64 %457, i64* %461, align 8
  %462 = getelementptr inbounds [4 x i64], [4 x i64]* %43, i64 %326, i64 2
  store i64 %460, i64* %462, align 8
  %463 = trunc i128 %459 to i64
  %464 = getelementptr inbounds [4 x i64], [4 x i64]* %43, i64 %326, i64 3
  store i64 %463, i64* %464, align 8
  %465 = load i128, i128* %225, align 16
  %466 = add i128 %465, 18446744069414584320
  %467 = load i128, i128* %228, align 16
  %468 = lshr i128 %467, 64
  %469 = add i128 %466, %468
  %470 = and i128 %467, 18446744073709551615
  %471 = add nuw nsw i128 %470, 18446673704965373952
  %472 = load i128, i128* %104, align 16
  %473 = add i128 %472, 18446744073709551615
  %474 = load i128, i128* %236, align 16
  %475 = add i128 %474, 1298074214633706907132628377272319
  %476 = lshr i128 %469, 64
  %477 = trunc i128 %476 to i64
  %478 = and i128 %469, 18446744073709551615
  %479 = sub nsw i128 %478, %476
  %480 = shl nuw nsw i128 %476, 32
  %481 = add nsw i128 %479, %480
  %482 = lshr i128 %481, 64
  %483 = trunc i128 %482 to i64
  %484 = add i64 %483, %477
  %485 = and i128 %481, 18446744073709551615
  %486 = sub nsw i128 %485, %482
  %487 = shl nuw nsw i128 %482, 32
  %488 = add nsw i128 %486, %487
  %489 = zext i64 %484 to i128
  %490 = add i128 %473, %489
  %491 = shl nuw nsw i128 %489, 32
  %492 = sub i128 %475, %491
  %493 = lshr i128 %488, 1
  %494 = trunc i128 %493 to i64
  %495 = trunc i128 %488 to i64
  %496 = and i64 %495, 9223372036854775807
  %497 = sub nsw i64 9223372032559808512, %496
  %498 = and i64 %497, %495
  %499 = or i64 %498, %494
  %500 = ashr i64 %499, 63
  %501 = zext i64 %500 to i128
  %502 = sub i128 %490, %501
  %503 = and i64 %500, 4294967295
  %504 = zext i64 %503 to i128
  %505 = sub i128 %492, %504
  %506 = and i64 %500, -4294967295
  %507 = zext i64 %506 to i128
  %508 = sub nsw i128 %488, %507
  %509 = lshr i128 %502, 64
  %510 = add i128 %505, %509
  %511 = trunc i128 %502 to i64
  %512 = lshr i128 %510, 64
  %513 = add nuw nsw i128 %471, %512
  %514 = trunc i128 %510 to i64
  %515 = lshr i128 %513, 64
  %516 = add nsw i128 %508, %515
  %517 = trunc i128 %513 to i64
  store i64 %511, i64* %329, align 8
  %518 = getelementptr inbounds [4 x i64], [4 x i64]* %45, i64 %326, i64 1
  store i64 %514, i64* %518, align 8
  %519 = getelementptr inbounds [4 x i64], [4 x i64]* %45, i64 %326, i64 2
  store i64 %517, i64* %519, align 8
  %520 = trunc i128 %516 to i64
  %521 = getelementptr inbounds [4 x i64], [4 x i64]* %45, i64 %326, i64 3
  store i64 %520, i64* %521, align 8
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %56) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %55) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %54) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %53) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %52) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %51) #7
  %522 = add nuw nsw i64 %326, 1
  %523 = icmp eq i64 %522, %316
  br i1 %523, label %524, label %317

524:                                              ; preds = %317, %35
  %525 = bitcast i128* %0 to i8*
  call void @llvm.memset.p0i8.i64(i8* align 16 %525, i8 0, i64 64, i1 false)
  %526 = bitcast i128* %1 to i8*
  call void @llvm.memset.p0i8.i64(i8* align 16 %526, i8 0, i64 64, i1 false)
  %527 = bitcast i128* %2 to i8*
  call void @llvm.memset.p0i8.i64(i8* align 16 %527, i8 0, i64 64, i1 false)
  %528 = icmp sgt i32 %16, -1
  br i1 %528, label %529, label %590

529:                                              ; preds = %524, %583
  %530 = phi i32 [ %584, %583 ], [ %16, %524 ]
  %531 = sext i32 %530 to i64
  br label %532

532:                                              ; preds = %529, %536
  %533 = phi i64 [ %531, %529 ], [ %537, %536 ]
  %534 = call i32 @__gmpz_tstbit(%1* %7, i64 %533) #6
  %535 = icmp eq i32 %534, 0
  br i1 %535, label %536, label %539

536:                                              ; preds = %532
  call fastcc void @0(i128* %0, i128* %1, i128* %2, i128* %0, i128* %1, i128* %2)
  %537 = add nsw i64 %533, -1
  %538 = icmp sgt i64 %533, 0
  br i1 %538, label %532, label %590

539:                                              ; preds = %532
  %540 = trunc i64 %533 to i32
  %541 = sub nsw i32 %540, %22
  %542 = add nsw i32 %541, 1
  %543 = icmp sgt i32 %542, 0
  %544 = select i1 %543, i32 %542, i32 0
  %545 = icmp slt i32 %544, %540
  br i1 %545, label %546, label %561

546:                                              ; preds = %539
  %547 = zext i32 %544 to i64
  %548 = shl i64 %533, 32
  %549 = ashr exact i64 %548, 32
  br label %550

550:                                              ; preds = %546, %555
  %551 = phi i64 [ %547, %546 ], [ %556, %555 ]
  %552 = phi i32 [ %544, %546 ], [ %557, %555 ]
  %553 = call i32 @__gmpz_tstbit(%1* %7, i64 %551) #6
  %554 = icmp eq i32 %553, 0
  br i1 %554, label %555, label %559

555:                                              ; preds = %550
  %556 = add nuw nsw i64 %551, 1
  %557 = add nuw nsw i32 %552, 1
  %558 = icmp slt i64 %556, %549
  br i1 %558, label %550, label %561

559:                                              ; preds = %550
  %560 = trunc i64 %551 to i32
  br label %561

561:                                              ; preds = %555, %559, %539
  %562 = phi i32 [ %544, %539 ], [ %560, %559 ], [ %557, %555 ]
  %563 = icmp slt i32 %562, %540
  br i1 %563, label %564, label %568

564:                                              ; preds = %561
  %565 = shl i64 %533, 32
  %566 = ashr exact i64 %565, 32
  %567 = sext i32 %562 to i64
  br label %571

568:                                              ; preds = %571, %561
  %569 = phi i32 [ 0, %561 ], [ %576, %571 ]
  %570 = icmp sgt i32 %562, %540
  br i1 %570, label %583, label %579

571:                                              ; preds = %564, %571
  %572 = phi i64 [ %566, %564 ], [ %577, %571 ]
  %573 = phi i32 [ 0, %564 ], [ %576, %571 ]
  %574 = shl i32 %573, 1
  %575 = call i32 @__gmpz_tstbit(%1* %7, i64 %572) #6
  %576 = or i32 %575, %574
  %577 = add nsw i64 %572, -1
  %578 = icmp sgt i64 %577, %567
  br i1 %578, label %571, label %568

579:                                              ; preds = %568, %579
  %580 = phi i32 [ %581, %579 ], [ %540, %568 ]
  call fastcc void @0(i128* %0, i128* %1, i128* %2, i128* %0, i128* %1, i128* %2)
  %581 = add nsw i32 %580, -1
  %582 = icmp sgt i32 %580, %562
  br i1 %582, label %579, label %583

583:                                              ; preds = %579, %568
  %584 = phi i32 [ %540, %568 ], [ %581, %579 ]
  %585 = sext i32 %569 to i64
  %586 = getelementptr inbounds [4 x i64], [4 x i64]* %41, i64 %585, i64 0
  %587 = getelementptr inbounds [4 x i64], [4 x i64]* %43, i64 %585, i64 0
  %588 = getelementptr inbounds [4 x i64], [4 x i64]* %45, i64 %585, i64 0
  call fastcc void @1(i128* %0, i128* %1, i128* %2, i128* %0, i128* %1, i128* %2, i64* %586, i64* %587, i64* %588)
  %589 = icmp sgt i32 %584, -1
  br i1 %589, label %529, label %590

590:                                              ; preds = %583, %536, %524
  call void @free(i8* %44) #7
  call void @free(i8* %42) #7
  call void @free(i8* %40) #7
  ret void
}

; Function Attrs: nounwind
declare dso_local noalias i8* @malloc(i64) local_unnamed_addr #4

; Function Attrs: nounwind
declare dso_local void @free(i8* nocapture) local_unnamed_addr #4

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jsmul_init_nistp256_inner(%4* nocapture %0, %0* nocapture readnone %1, i64 %2, i64 %3) local_unnamed_addr #0 {
  %5 = getelementptr inbounds %4, %4* %0, i64 0, i32 1
  store i64 %2, i64* %5, align 8
  %6 = getelementptr inbounds %4, %4* %0, i64 0, i32 2
  %7 = icmp ult i64 %2, %3
  %8 = select i1 %7, i64 %2, i64 %3
  store i64 %8, i64* %6, align 8
  %9 = add i64 %2, -1
  %10 = add i64 %9, %3
  %11 = udiv i64 %10, %3
  %12 = getelementptr inbounds %4, %4* %0, i64 0, i32 3
  store i64 %11, i64* %12, align 8
  %13 = shl i64 %11, 3
  %14 = tail call noalias i8* @malloc(i64 %13) #7
  %15 = getelementptr inbounds %4, %4* %0, i64 0, i32 4
  %16 = bitcast [4 x i64]*** %15 to i8**
  store i8* %14, i8** %16, align 8
  %17 = tail call noalias i8* @malloc(i64 %13) #7
  %18 = getelementptr inbounds %4, %4* %0, i64 0, i32 5
  %19 = bitcast [4 x i64]*** %18 to i8**
  store i8* %17, i8** %19, align 8
  %20 = tail call noalias i8* @malloc(i64 %13) #7
  %21 = getelementptr inbounds %4, %4* %0, i64 0, i32 6
  %22 = bitcast [4 x i64]*** %21 to i8**
  store i8* %20, i8** %22, align 8
  %23 = icmp ult i64 %10, %3
  br i1 %23, label %60, label %24

24:                                               ; preds = %4
  %25 = trunc i64 %3 to i32
  %26 = shl i32 1, %25
  %27 = sext i32 %26 to i64
  %28 = add i64 %11, -1
  br label %29

29:                                               ; preds = %24, %42
  %30 = phi i64 [ %27, %24 ], [ %44, %42 ]
  %31 = phi i64 [ 0, %24 ], [ %58, %42 ]
  %32 = phi i64 [ %3, %24 ], [ %43, %42 ]
  %33 = icmp eq i64 %31, %28
  br i1 %33, label %34, label %42

34:                                               ; preds = %29
  %35 = mul i64 %32, %28
  %36 = sub i64 %2, %35
  %37 = icmp ult i64 %36, %32
  br i1 %37, label %38, label %42

38:                                               ; preds = %34
  %39 = trunc i64 %36 to i32
  %40 = shl i32 1, %39
  %41 = sext i32 %40 to i64
  br label %42

42:                                               ; preds = %38, %34, %29
  %43 = phi i64 [ %36, %38 ], [ %32, %34 ], [ %32, %29 ]
  %44 = phi i64 [ %41, %38 ], [ %30, %34 ], [ %30, %29 ]
  %45 = shl i64 %44, 5
  %46 = tail call noalias i8* @malloc(i64 %45) #7
  %47 = load [4 x i64]**, [4 x i64]*** %15, align 8
  %48 = getelementptr inbounds [4 x i64]*, [4 x i64]** %47, i64 %31
  %49 = bitcast [4 x i64]** %48 to i8**
  store i8* %46, i8** %49, align 8
  %50 = tail call noalias i8* @malloc(i64 %45) #7
  %51 = load [4 x i64]**, [4 x i64]*** %18, align 8
  %52 = getelementptr inbounds [4 x i64]*, [4 x i64]** %51, i64 %31
  %53 = bitcast [4 x i64]** %52 to i8**
  store i8* %50, i8** %53, align 8
  %54 = tail call noalias i8* @malloc(i64 %45) #7
  %55 = load [4 x i64]**, [4 x i64]*** %21, align 8
  %56 = getelementptr inbounds [4 x i64]*, [4 x i64]** %55, i64 %31
  %57 = bitcast [4 x i64]** %56 to i8**
  store i8* %54, i8** %57, align 8
  %58 = add nuw i64 %31, 1
  %59 = icmp ult i64 %58, %11
  br i1 %59, label %29, label %60

60:                                               ; preds = %42, %4
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @vec_jsmul_clear_nistp256_inner(%4* nocapture readonly %0) local_unnamed_addr #0 {
  %2 = getelementptr inbounds %4, %4* %0, i64 0, i32 3
  %3 = load i64, i64* %2, align 8
  %4 = getelementptr inbounds %4, %4* %0, i64 0, i32 2
  %5 = load i64, i64* %4, align 8
  %6 = trunc i64 %5 to i32
  %7 = shl i32 1, %6
  %8 = sext i32 %7 to i64
  %9 = icmp eq i64 %3, 0
  br i1 %9, label %10, label %14

10:                                               ; preds = %1
  %11 = getelementptr inbounds %4, %4* %0, i64 0, i32 4
  %12 = getelementptr inbounds %4, %4* %0, i64 0, i32 5
  %13 = getelementptr inbounds %4, %4* %0, i64 0, i32 6
  br label %49

14:                                               ; preds = %1
  %15 = add i64 %3, -1
  %16 = getelementptr inbounds %4, %4* %0, i64 0, i32 1
  %17 = getelementptr inbounds %4, %4* %0, i64 0, i32 4
  %18 = getelementptr inbounds %4, %4* %0, i64 0, i32 5
  %19 = getelementptr inbounds %4, %4* %0, i64 0, i32 6
  br label %20

20:                                               ; preds = %32, %14
  %21 = phi i64 [ %8, %14 ], [ %34, %32 ]
  %22 = phi i64 [ %5, %14 ], [ %33, %32 ]
  %23 = phi i64 [ 0, %14 ], [ %47, %32 ]
  %24 = icmp eq i64 %23, %15
  br i1 %24, label %25, label %32

25:                                               ; preds = %20
  %26 = load i64, i64* %16, align 8
  %27 = mul i64 %22, %15
  %28 = sub i64 %26, %27
  %29 = trunc i64 %28 to i32
  %30 = shl i32 1, %29
  %31 = sext i32 %30 to i64
  br label %32

32:                                               ; preds = %25, %20
  %33 = phi i64 [ %28, %25 ], [ %22, %20 ]
  %34 = phi i64 [ %31, %25 ], [ %21, %20 ]
  %35 = load [4 x i64]**, [4 x i64]*** %17, align 8
  %36 = getelementptr inbounds [4 x i64]*, [4 x i64]** %35, i64 %23
  %37 = bitcast [4 x i64]** %36 to i8**
  %38 = load i8*, i8** %37, align 8
  tail call void @free(i8* %38) #7
  %39 = load [4 x i64]**, [4 x i64]*** %18, align 8
  %40 = getelementptr inbounds [4 x i64]*, [4 x i64]** %39, i64 %23
  %41 = bitcast [4 x i64]** %40 to i8**
  %42 = load i8*, i8** %41, align 8
  tail call void @free(i8* %42) #7
  %43 = load [4 x i64]**, [4 x i64]*** %19, align 8
  %44 = getelementptr inbounds [4 x i64]*, [4 x i64]** %43, i64 %23
  %45 = bitcast [4 x i64]** %44 to i8**
  %46 = load i8*, i8** %45, align 8
  tail call void @free(i8* %46) #7
  %47 = add nuw i64 %23, 1
  %48 = icmp eq i64 %47, %3
  br i1 %48, label %49, label %20

49:                                               ; preds = %32, %10
  %50 = phi [4 x i64]*** [ %13, %10 ], [ %19, %32 ]
  %51 = phi [4 x i64]*** [ %12, %10 ], [ %18, %32 ]
  %52 = phi [4 x i64]*** [ %11, %10 ], [ %17, %32 ]
  %53 = phi i64 [ %8, %10 ], [ %34, %32 ]
  %54 = bitcast [4 x i64]*** %52 to i8**
  %55 = load i8*, i8** %54, align 8
  tail call void @free(i8* %55) #7
  %56 = bitcast [4 x i64]*** %51 to i8**
  %57 = load i8*, i8** %56, align 8
  tail call void @free(i8* %57) #7
  %58 = bitcast [4 x i64]*** %50 to i8**
  %59 = load i8*, i8** %58, align 8
  tail call void @free(i8* %59) #7
  %60 = trunc i64 %53 to i32
  ret i32 %60
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jsmul_precomp_nistp256_inner(%4* nocapture readonly %0, %0* nocapture readnone %1, [4 x i64]* nocapture readonly %2, [4 x i64]* nocapture readonly %3, [4 x i64]* nocapture readonly %4) local_unnamed_addr #0 {
  %6 = alloca [4 x i128], align 16
  %7 = alloca [4 x i128], align 16
  %8 = alloca [4 x i128], align 16
  %9 = alloca [4 x i128], align 16
  %10 = alloca [4 x i128], align 16
  %11 = alloca [4 x i128], align 16
  %12 = getelementptr inbounds %4, %4* %0, i64 0, i32 2
  %13 = load i64, i64* %12, align 8
  %14 = getelementptr inbounds %4, %4* %0, i64 0, i32 3
  %15 = load i64, i64* %14, align 8
  %16 = icmp eq i64 %15, 0
  br i1 %16, label %363, label %17

17:                                               ; preds = %5
  %18 = trunc i64 %13 to i32
  %19 = shl i32 1, %18
  %20 = sext i32 %19 to i64
  %21 = getelementptr inbounds %4, %4* %0, i64 0, i32 1
  %22 = getelementptr inbounds %4, %4* %0, i64 0, i32 4
  %23 = getelementptr inbounds %4, %4* %0, i64 0, i32 5
  %24 = getelementptr inbounds %4, %4* %0, i64 0, i32 6
  %25 = bitcast [4 x i128]* %6 to i8*
  %26 = bitcast [4 x i128]* %7 to i8*
  %27 = bitcast [4 x i128]* %8 to i8*
  %28 = bitcast [4 x i128]* %9 to i8*
  %29 = bitcast [4 x i128]* %10 to i8*
  %30 = bitcast [4 x i128]* %11 to i8*
  %31 = getelementptr inbounds [4 x i128], [4 x i128]* %9, i64 0, i64 0
  %32 = getelementptr inbounds [4 x i128], [4 x i128]* %9, i64 0, i64 1
  %33 = getelementptr inbounds [4 x i128], [4 x i128]* %9, i64 0, i64 2
  %34 = getelementptr inbounds [4 x i128], [4 x i128]* %9, i64 0, i64 3
  %35 = getelementptr inbounds [4 x i128], [4 x i128]* %10, i64 0, i64 0
  %36 = getelementptr inbounds [4 x i128], [4 x i128]* %10, i64 0, i64 1
  %37 = getelementptr inbounds [4 x i128], [4 x i128]* %10, i64 0, i64 2
  %38 = getelementptr inbounds [4 x i128], [4 x i128]* %10, i64 0, i64 3
  %39 = getelementptr inbounds [4 x i128], [4 x i128]* %11, i64 0, i64 0
  %40 = getelementptr inbounds [4 x i128], [4 x i128]* %11, i64 0, i64 1
  %41 = getelementptr inbounds [4 x i128], [4 x i128]* %11, i64 0, i64 2
  %42 = getelementptr inbounds [4 x i128], [4 x i128]* %11, i64 0, i64 3
  %43 = getelementptr inbounds [4 x i128], [4 x i128]* %6, i64 0, i64 0
  %44 = getelementptr inbounds [4 x i128], [4 x i128]* %7, i64 0, i64 0
  %45 = getelementptr inbounds [4 x i128], [4 x i128]* %8, i64 0, i64 0
  %46 = getelementptr inbounds [4 x i128], [4 x i128]* %6, i64 0, i64 3
  %47 = getelementptr inbounds [4 x i128], [4 x i128]* %6, i64 0, i64 2
  %48 = getelementptr inbounds [4 x i128], [4 x i128]* %6, i64 0, i64 1
  %49 = getelementptr inbounds [4 x i128], [4 x i128]* %7, i64 0, i64 3
  %50 = getelementptr inbounds [4 x i128], [4 x i128]* %7, i64 0, i64 2
  %51 = getelementptr inbounds [4 x i128], [4 x i128]* %7, i64 0, i64 1
  %52 = getelementptr inbounds [4 x i128], [4 x i128]* %8, i64 0, i64 3
  %53 = getelementptr inbounds [4 x i128], [4 x i128]* %8, i64 0, i64 2
  %54 = getelementptr inbounds [4 x i128], [4 x i128]* %8, i64 0, i64 1
  br label %55

55:                                               ; preds = %17, %356
  %56 = phi i64 [ %15, %17 ], [ %361, %356 ]
  %57 = phi [4 x i64]* [ %2, %17 ], [ %357, %356 ]
  %58 = phi [4 x i64]* [ %3, %17 ], [ %358, %356 ]
  %59 = phi [4 x i64]* [ %4, %17 ], [ %359, %356 ]
  %60 = phi i64 [ 0, %17 ], [ %360, %356 ]
  %61 = phi i64 [ %20, %17 ], [ %74, %356 ]
  %62 = phi i64 [ %13, %17 ], [ %73, %356 ]
  %63 = add i64 %56, -1
  %64 = icmp eq i64 %60, %63
  br i1 %64, label %65, label %72

65:                                               ; preds = %55
  %66 = load i64, i64* %21, align 8
  %67 = mul i64 %60, %62
  %68 = sub i64 %66, %67
  %69 = trunc i64 %68 to i32
  %70 = shl i32 1, %69
  %71 = sext i32 %70 to i64
  br label %72

72:                                               ; preds = %65, %55
  %73 = phi i64 [ %68, %65 ], [ %62, %55 ]
  %74 = phi i64 [ %71, %65 ], [ %61, %55 ]
  %75 = load [4 x i64]**, [4 x i64]*** %22, align 8
  %76 = getelementptr inbounds [4 x i64]*, [4 x i64]** %75, i64 %60
  %77 = load [4 x i64]*, [4 x i64]** %76, align 8
  %78 = load [4 x i64]**, [4 x i64]*** %23, align 8
  %79 = getelementptr inbounds [4 x i64]*, [4 x i64]** %78, i64 %60
  %80 = load [4 x i64]*, [4 x i64]** %79, align 8
  %81 = load [4 x i64]**, [4 x i64]*** %24, align 8
  %82 = getelementptr inbounds [4 x i64]*, [4 x i64]** %81, i64 %60
  %83 = load [4 x i64]*, [4 x i64]** %82, align 8
  %84 = bitcast [4 x i64]* %77 to i8*
  call void @llvm.memset.p0i8.i64(i8* align 8 %84, i8 0, i64 32, i1 false)
  %85 = bitcast [4 x i64]* %80 to i8*
  call void @llvm.memset.p0i8.i64(i8* align 8 %85, i8 0, i64 32, i1 false)
  %86 = bitcast [4 x i64]* %83 to i8*
  call void @llvm.memset.p0i8.i64(i8* align 8 %86, i8 0, i64 32, i1 false)
  %87 = icmp eq i64 %73, 0
  br i1 %87, label %88, label %90

88:                                               ; preds = %90, %72
  %89 = icmp ugt i64 %74, 1
  br i1 %89, label %133, label %356

90:                                               ; preds = %72, %90
  %91 = phi i32 [ %130, %90 ], [ 1, %72 ]
  %92 = phi i64 [ %131, %90 ], [ 0, %72 ]
  %93 = sext i32 %91 to i64
  %94 = getelementptr inbounds [4 x i64], [4 x i64]* %77, i64 %93, i64 0
  %95 = getelementptr inbounds [4 x i64], [4 x i64]* %57, i64 %92, i64 0
  %96 = load i64, i64* %95, align 8
  store i64 %96, i64* %94, align 8
  %97 = getelementptr inbounds [4 x i64], [4 x i64]* %57, i64 %92, i64 1
  %98 = load i64, i64* %97, align 8
  %99 = getelementptr inbounds [4 x i64], [4 x i64]* %77, i64 %93, i64 1
  store i64 %98, i64* %99, align 8
  %100 = getelementptr inbounds [4 x i64], [4 x i64]* %57, i64 %92, i64 2
  %101 = load i64, i64* %100, align 8
  %102 = getelementptr inbounds [4 x i64], [4 x i64]* %77, i64 %93, i64 2
  store i64 %101, i64* %102, align 8
  %103 = getelementptr inbounds [4 x i64], [4 x i64]* %57, i64 %92, i64 3
  %104 = load i64, i64* %103, align 8
  %105 = getelementptr inbounds [4 x i64], [4 x i64]* %77, i64 %93, i64 3
  store i64 %104, i64* %105, align 8
  %106 = getelementptr inbounds [4 x i64], [4 x i64]* %80, i64 %93, i64 0
  %107 = getelementptr inbounds [4 x i64], [4 x i64]* %58, i64 %92, i64 0
  %108 = load i64, i64* %107, align 8
  store i64 %108, i64* %106, align 8
  %109 = getelementptr inbounds [4 x i64], [4 x i64]* %58, i64 %92, i64 1
  %110 = load i64, i64* %109, align 8
  %111 = getelementptr inbounds [4 x i64], [4 x i64]* %80, i64 %93, i64 1
  store i64 %110, i64* %111, align 8
  %112 = getelementptr inbounds [4 x i64], [4 x i64]* %58, i64 %92, i64 2
  %113 = load i64, i64* %112, align 8
  %114 = getelementptr inbounds [4 x i64], [4 x i64]* %80, i64 %93, i64 2
  store i64 %113, i64* %114, align 8
  %115 = getelementptr inbounds [4 x i64], [4 x i64]* %58, i64 %92, i64 3
  %116 = load i64, i64* %115, align 8
  %117 = getelementptr inbounds [4 x i64], [4 x i64]* %80, i64 %93, i64 3
  store i64 %116, i64* %117, align 8
  %118 = getelementptr inbounds [4 x i64], [4 x i64]* %83, i64 %93, i64 0
  %119 = getelementptr inbounds [4 x i64], [4 x i64]* %59, i64 %92, i64 0
  %120 = load i64, i64* %119, align 8
  store i64 %120, i64* %118, align 8
  %121 = getelementptr inbounds [4 x i64], [4 x i64]* %59, i64 %92, i64 1
  %122 = load i64, i64* %121, align 8
  %123 = getelementptr inbounds [4 x i64], [4 x i64]* %83, i64 %93, i64 1
  store i64 %122, i64* %123, align 8
  %124 = getelementptr inbounds [4 x i64], [4 x i64]* %59, i64 %92, i64 2
  %125 = load i64, i64* %124, align 8
  %126 = getelementptr inbounds [4 x i64], [4 x i64]* %83, i64 %93, i64 2
  store i64 %125, i64* %126, align 8
  %127 = getelementptr inbounds [4 x i64], [4 x i64]* %59, i64 %92, i64 3
  %128 = load i64, i64* %127, align 8
  %129 = getelementptr inbounds [4 x i64], [4 x i64]* %83, i64 %93, i64 3
  store i64 %128, i64* %129, align 8
  %130 = shl i32 %91, 1
  %131 = add nuw i64 %92, 1
  %132 = icmp eq i64 %131, %73
  br i1 %132, label %88, label %90

133:                                              ; preds = %88, %133
  %134 = phi i64 [ %354, %133 ], [ 1, %88 ]
  %135 = trunc i64 %134 to i32
  %136 = sub nsw i32 0, %135
  %137 = and i32 %135, %136
  %138 = getelementptr inbounds [4 x i64], [4 x i64]* %77, i64 %134, i64 0
  %139 = getelementptr inbounds [4 x i64], [4 x i64]* %80, i64 %134, i64 0
  %140 = getelementptr inbounds [4 x i64], [4 x i64]* %83, i64 %134, i64 0
  %141 = xor i32 %137, %135
  %142 = zext i32 %141 to i64
  %143 = getelementptr inbounds [4 x i64], [4 x i64]* %77, i64 %142, i64 0
  %144 = getelementptr inbounds [4 x i64], [4 x i64]* %80, i64 %142, i64 0
  %145 = getelementptr inbounds [4 x i64], [4 x i64]* %83, i64 %142, i64 0
  %146 = zext i32 %137 to i64
  %147 = getelementptr inbounds [4 x i64], [4 x i64]* %77, i64 %146, i64 0
  %148 = getelementptr inbounds [4 x i64], [4 x i64]* %80, i64 %146, i64 0
  %149 = getelementptr inbounds [4 x i64], [4 x i64]* %83, i64 %146, i64 0
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %25) #7
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %26) #7
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %27) #7
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %28) #7
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %29) #7
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %30) #7
  %150 = load i64, i64* %143, align 8
  %151 = zext i64 %150 to i128
  store i128 %151, i128* %31, align 16
  %152 = getelementptr inbounds [4 x i64], [4 x i64]* %77, i64 %142, i64 1
  %153 = load i64, i64* %152, align 8
  %154 = zext i64 %153 to i128
  store i128 %154, i128* %32, align 16
  %155 = getelementptr inbounds [4 x i64], [4 x i64]* %77, i64 %142, i64 2
  %156 = load i64, i64* %155, align 8
  %157 = zext i64 %156 to i128
  store i128 %157, i128* %33, align 16
  %158 = getelementptr inbounds [4 x i64], [4 x i64]* %77, i64 %142, i64 3
  %159 = load i64, i64* %158, align 8
  %160 = zext i64 %159 to i128
  store i128 %160, i128* %34, align 16
  %161 = load i64, i64* %144, align 8
  %162 = zext i64 %161 to i128
  store i128 %162, i128* %35, align 16
  %163 = getelementptr inbounds [4 x i64], [4 x i64]* %80, i64 %142, i64 1
  %164 = load i64, i64* %163, align 8
  %165 = zext i64 %164 to i128
  store i128 %165, i128* %36, align 16
  %166 = getelementptr inbounds [4 x i64], [4 x i64]* %80, i64 %142, i64 2
  %167 = load i64, i64* %166, align 8
  %168 = zext i64 %167 to i128
  store i128 %168, i128* %37, align 16
  %169 = getelementptr inbounds [4 x i64], [4 x i64]* %80, i64 %142, i64 3
  %170 = load i64, i64* %169, align 8
  %171 = zext i64 %170 to i128
  store i128 %171, i128* %38, align 16
  %172 = load i64, i64* %145, align 8
  %173 = zext i64 %172 to i128
  store i128 %173, i128* %39, align 16
  %174 = getelementptr inbounds [4 x i64], [4 x i64]* %83, i64 %142, i64 1
  %175 = load i64, i64* %174, align 8
  %176 = zext i64 %175 to i128
  store i128 %176, i128* %40, align 16
  %177 = getelementptr inbounds [4 x i64], [4 x i64]* %83, i64 %142, i64 2
  %178 = load i64, i64* %177, align 8
  %179 = zext i64 %178 to i128
  store i128 %179, i128* %41, align 16
  %180 = getelementptr inbounds [4 x i64], [4 x i64]* %83, i64 %142, i64 3
  %181 = load i64, i64* %180, align 8
  %182 = zext i64 %181 to i128
  store i128 %182, i128* %42, align 16
  call fastcc void @1(i128* nonnull %43, i128* nonnull %44, i128* nonnull %45, i128* nonnull %31, i128* nonnull %35, i128* nonnull %39, i64* %147, i64* %148, i64* %149) #7
  %183 = load i128, i128* %46, align 16
  %184 = add i128 %183, 18446744069414584320
  %185 = load i128, i128* %47, align 16
  %186 = lshr i128 %185, 64
  %187 = add i128 %184, %186
  %188 = and i128 %185, 18446744073709551615
  %189 = add nuw nsw i128 %188, 18446673704965373952
  %190 = load i128, i128* %43, align 16
  %191 = add i128 %190, 18446744073709551615
  %192 = load i128, i128* %48, align 16
  %193 = add i128 %192, 1298074214633706907132628377272319
  %194 = lshr i128 %187, 64
  %195 = trunc i128 %194 to i64
  %196 = and i128 %187, 18446744073709551615
  %197 = sub nsw i128 %196, %194
  %198 = shl nuw nsw i128 %194, 32
  %199 = add nsw i128 %197, %198
  %200 = lshr i128 %199, 64
  %201 = trunc i128 %200 to i64
  %202 = add i64 %201, %195
  %203 = and i128 %199, 18446744073709551615
  %204 = sub nsw i128 %203, %200
  %205 = shl nuw nsw i128 %200, 32
  %206 = add nsw i128 %204, %205
  %207 = zext i64 %202 to i128
  %208 = add i128 %191, %207
  %209 = shl nuw nsw i128 %207, 32
  %210 = sub i128 %193, %209
  %211 = lshr i128 %206, 1
  %212 = trunc i128 %211 to i64
  %213 = trunc i128 %206 to i64
  %214 = and i64 %213, 9223372036854775807
  %215 = sub nsw i64 9223372032559808512, %214
  %216 = and i64 %215, %213
  %217 = or i64 %216, %212
  %218 = ashr i64 %217, 63
  %219 = zext i64 %218 to i128
  %220 = sub i128 %208, %219
  %221 = and i64 %218, 4294967295
  %222 = zext i64 %221 to i128
  %223 = sub i128 %210, %222
  %224 = and i64 %218, -4294967295
  %225 = zext i64 %224 to i128
  %226 = sub nsw i128 %206, %225
  %227 = lshr i128 %220, 64
  %228 = add i128 %223, %227
  %229 = trunc i128 %220 to i64
  %230 = lshr i128 %228, 64
  %231 = add nuw nsw i128 %189, %230
  %232 = trunc i128 %228 to i64
  %233 = lshr i128 %231, 64
  %234 = add nsw i128 %226, %233
  %235 = trunc i128 %231 to i64
  store i64 %229, i64* %138, align 8
  %236 = getelementptr inbounds [4 x i64], [4 x i64]* %77, i64 %134, i64 1
  store i64 %232, i64* %236, align 8
  %237 = getelementptr inbounds [4 x i64], [4 x i64]* %77, i64 %134, i64 2
  store i64 %235, i64* %237, align 8
  %238 = trunc i128 %234 to i64
  %239 = getelementptr inbounds [4 x i64], [4 x i64]* %77, i64 %134, i64 3
  store i64 %238, i64* %239, align 8
  %240 = load i128, i128* %49, align 16
  %241 = add i128 %240, 18446744069414584320
  %242 = load i128, i128* %50, align 16
  %243 = lshr i128 %242, 64
  %244 = add i128 %241, %243
  %245 = and i128 %242, 18446744073709551615
  %246 = add nuw nsw i128 %245, 18446673704965373952
  %247 = load i128, i128* %44, align 16
  %248 = add i128 %247, 18446744073709551615
  %249 = load i128, i128* %51, align 16
  %250 = add i128 %249, 1298074214633706907132628377272319
  %251 = lshr i128 %244, 64
  %252 = trunc i128 %251 to i64
  %253 = and i128 %244, 18446744073709551615
  %254 = sub nsw i128 %253, %251
  %255 = shl nuw nsw i128 %251, 32
  %256 = add nsw i128 %254, %255
  %257 = lshr i128 %256, 64
  %258 = trunc i128 %257 to i64
  %259 = add i64 %258, %252
  %260 = and i128 %256, 18446744073709551615
  %261 = sub nsw i128 %260, %257
  %262 = shl nuw nsw i128 %257, 32
  %263 = add nsw i128 %261, %262
  %264 = zext i64 %259 to i128
  %265 = add i128 %248, %264
  %266 = shl nuw nsw i128 %264, 32
  %267 = sub i128 %250, %266
  %268 = lshr i128 %263, 1
  %269 = trunc i128 %268 to i64
  %270 = trunc i128 %263 to i64
  %271 = and i64 %270, 9223372036854775807
  %272 = sub nsw i64 9223372032559808512, %271
  %273 = and i64 %272, %270
  %274 = or i64 %273, %269
  %275 = ashr i64 %274, 63
  %276 = zext i64 %275 to i128
  %277 = sub i128 %265, %276
  %278 = and i64 %275, 4294967295
  %279 = zext i64 %278 to i128
  %280 = sub i128 %267, %279
  %281 = and i64 %275, -4294967295
  %282 = zext i64 %281 to i128
  %283 = sub nsw i128 %263, %282
  %284 = lshr i128 %277, 64
  %285 = add i128 %280, %284
  %286 = trunc i128 %277 to i64
  %287 = lshr i128 %285, 64
  %288 = add nuw nsw i128 %246, %287
  %289 = trunc i128 %285 to i64
  %290 = lshr i128 %288, 64
  %291 = add nsw i128 %283, %290
  %292 = trunc i128 %288 to i64
  store i64 %286, i64* %139, align 8
  %293 = getelementptr inbounds [4 x i64], [4 x i64]* %80, i64 %134, i64 1
  store i64 %289, i64* %293, align 8
  %294 = getelementptr inbounds [4 x i64], [4 x i64]* %80, i64 %134, i64 2
  store i64 %292, i64* %294, align 8
  %295 = trunc i128 %291 to i64
  %296 = getelementptr inbounds [4 x i64], [4 x i64]* %80, i64 %134, i64 3
  store i64 %295, i64* %296, align 8
  %297 = load i128, i128* %52, align 16
  %298 = add i128 %297, 18446744069414584320
  %299 = load i128, i128* %53, align 16
  %300 = lshr i128 %299, 64
  %301 = add i128 %298, %300
  %302 = and i128 %299, 18446744073709551615
  %303 = add nuw nsw i128 %302, 18446673704965373952
  %304 = load i128, i128* %45, align 16
  %305 = add i128 %304, 18446744073709551615
  %306 = load i128, i128* %54, align 16
  %307 = add i128 %306, 1298074214633706907132628377272319
  %308 = lshr i128 %301, 64
  %309 = trunc i128 %308 to i64
  %310 = and i128 %301, 18446744073709551615
  %311 = sub nsw i128 %310, %308
  %312 = shl nuw nsw i128 %308, 32
  %313 = add nsw i128 %311, %312
  %314 = lshr i128 %313, 64
  %315 = trunc i128 %314 to i64
  %316 = add i64 %315, %309
  %317 = and i128 %313, 18446744073709551615
  %318 = sub nsw i128 %317, %314
  %319 = shl nuw nsw i128 %314, 32
  %320 = add nsw i128 %318, %319
  %321 = zext i64 %316 to i128
  %322 = add i128 %305, %321
  %323 = shl nuw nsw i128 %321, 32
  %324 = sub i128 %307, %323
  %325 = lshr i128 %320, 1
  %326 = trunc i128 %325 to i64
  %327 = trunc i128 %320 to i64
  %328 = and i64 %327, 9223372036854775807
  %329 = sub nsw i64 9223372032559808512, %328
  %330 = and i64 %329, %327
  %331 = or i64 %330, %326
  %332 = ashr i64 %331, 63
  %333 = zext i64 %332 to i128
  %334 = sub i128 %322, %333
  %335 = and i64 %332, 4294967295
  %336 = zext i64 %335 to i128
  %337 = sub i128 %324, %336
  %338 = and i64 %332, -4294967295
  %339 = zext i64 %338 to i128
  %340 = sub nsw i128 %320, %339
  %341 = lshr i128 %334, 64
  %342 = add i128 %337, %341
  %343 = trunc i128 %334 to i64
  %344 = lshr i128 %342, 64
  %345 = add nuw nsw i128 %303, %344
  %346 = trunc i128 %342 to i64
  %347 = lshr i128 %345, 64
  %348 = add nsw i128 %340, %347
  %349 = trunc i128 %345 to i64
  store i64 %343, i64* %140, align 8
  %350 = getelementptr inbounds [4 x i64], [4 x i64]* %83, i64 %134, i64 1
  store i64 %346, i64* %350, align 8
  %351 = getelementptr inbounds [4 x i64], [4 x i64]* %83, i64 %134, i64 2
  store i64 %349, i64* %351, align 8
  %352 = trunc i128 %348 to i64
  %353 = getelementptr inbounds [4 x i64], [4 x i64]* %83, i64 %134, i64 3
  store i64 %352, i64* %353, align 8
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %30) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %29) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %28) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %27) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %26) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %25) #7
  %354 = add nuw i64 %134, 1
  %355 = icmp eq i64 %354, %74
  br i1 %355, label %356, label %133

356:                                              ; preds = %133, %88
  %357 = getelementptr inbounds [4 x i64], [4 x i64]* %57, i64 %73
  %358 = getelementptr inbounds [4 x i64], [4 x i64]* %58, i64 %73
  %359 = getelementptr inbounds [4 x i64], [4 x i64]* %59, i64 %73
  %360 = add nuw i64 %60, 1
  %361 = load i64, i64* %14, align 8
  %362 = icmp ult i64 %360, %361
  br i1 %362, label %55, label %363

363:                                              ; preds = %356, %5
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jsmul_table_nistp256_inner(i64* nocapture %0, i64* nocapture %1, i64* nocapture %2, %0* nocapture readnone %3, %4* nocapture readonly %4, [1 x %1]* readonly %5, i64 %6) local_unnamed_addr #0 {
  %8 = alloca [4 x i128], align 16
  %9 = alloca [4 x i128], align 16
  %10 = alloca [4 x i128], align 16
  %11 = bitcast [4 x i128]* %8 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %11) #7
  %12 = bitcast [4 x i128]* %9 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %12) #7
  %13 = bitcast [4 x i128]* %10 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %13) #7
  %14 = getelementptr inbounds %4, %4* %4, i64 0, i32 1
  %15 = load i64, i64* %14, align 8
  %16 = getelementptr inbounds %4, %4* %4, i64 0, i32 3
  %17 = load i64, i64* %16, align 8
  %18 = getelementptr inbounds %4, %4* %4, i64 0, i32 2
  %19 = load i64, i64* %18, align 8
  %20 = add i64 %17, -1
  %21 = getelementptr inbounds %4, %4* %4, i64 0, i32 4
  %22 = load [4 x i64]**, [4 x i64]*** %21, align 8
  %23 = getelementptr inbounds %4, %4* %4, i64 0, i32 5
  %24 = load [4 x i64]**, [4 x i64]*** %23, align 8
  %25 = getelementptr inbounds %4, %4* %4, i64 0, i32 6
  %26 = load [4 x i64]**, [4 x i64]*** %25, align 8
  %27 = getelementptr inbounds [4 x i128], [4 x i128]* %8, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %11, i8 0, i64 64, i1 false)
  %28 = getelementptr inbounds [4 x i128], [4 x i128]* %9, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %12, i8 0, i64 64, i1 false)
  %29 = getelementptr inbounds [4 x i128], [4 x i128]* %10, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %13, i8 0, i64 64, i1 false)
  %30 = trunc i64 %6 to i32
  %31 = add i32 %30, -1
  %32 = icmp sgt i32 %31, -1
  br i1 %32, label %33, label %100

33:                                               ; preds = %7
  %34 = mul i64 %20, %19
  %35 = sub i64 %15, %34
  %36 = icmp eq i64 %17, 0
  %37 = trunc i64 %35 to i32
  %38 = add i32 %37, -1
  %39 = icmp sgt i32 %38, -1
  %40 = trunc i64 %19 to i32
  %41 = add i32 %40, -1
  %42 = icmp sgt i32 %41, -1
  %43 = sext i32 %38 to i64
  %44 = sext i32 %41 to i64
  %45 = sext i32 %31 to i64
  br label %50

46:                                               ; preds = %85, %50
  %47 = add i32 %52, -1
  %48 = icmp sgt i32 %47, -1
  %49 = add nsw i64 %51, -1
  br i1 %48, label %50, label %100

50:                                               ; preds = %33, %46
  %51 = phi i64 [ %45, %33 ], [ %49, %46 ]
  %52 = phi i32 [ %31, %33 ], [ %47, %46 ]
  call fastcc void @0(i128* nonnull %27, i128* nonnull %28, i128* nonnull %29, i128* nonnull %27, i128* nonnull %28, i128* nonnull %29)
  br i1 %36, label %46, label %53

53:                                               ; preds = %50, %85
  %54 = phi i64 [ %97, %85 ], [ 0, %50 ]
  %55 = phi [1 x %1]* [ %98, %85 ], [ %5, %50 ]
  %56 = icmp eq i64 %54, %20
  br i1 %56, label %57, label %71

57:                                               ; preds = %53
  br i1 %39, label %58, label %85

58:                                               ; preds = %57, %58
  %59 = phi i64 [ %70, %58 ], [ %43, %57 ]
  %60 = phi i32 [ %68, %58 ], [ %38, %57 ]
  %61 = phi i32 [ %67, %58 ], [ 0, %57 ]
  %62 = shl i32 %61, 1
  %63 = getelementptr inbounds [1 x %1], [1 x %1]* %55, i64 %59, i64 0
  %64 = call i32 @__gmpz_tstbit(%1* %63, i64 %51) #6
  %65 = icmp ne i32 %64, 0
  %66 = zext i1 %65 to i32
  %67 = or i32 %62, %66
  %68 = add i32 %60, -1
  %69 = icmp sgt i32 %68, -1
  %70 = add nsw i64 %59, -1
  br i1 %69, label %58, label %85

71:                                               ; preds = %53
  br i1 %42, label %72, label %85

72:                                               ; preds = %71, %72
  %73 = phi i64 [ %84, %72 ], [ %44, %71 ]
  %74 = phi i32 [ %82, %72 ], [ %41, %71 ]
  %75 = phi i32 [ %81, %72 ], [ 0, %71 ]
  %76 = shl i32 %75, 1
  %77 = getelementptr inbounds [1 x %1], [1 x %1]* %55, i64 %73, i64 0
  %78 = call i32 @__gmpz_tstbit(%1* %77, i64 %51) #6
  %79 = icmp ne i32 %78, 0
  %80 = zext i1 %79 to i32
  %81 = or i32 %76, %80
  %82 = add i32 %74, -1
  %83 = icmp sgt i32 %82, -1
  %84 = add nsw i64 %73, -1
  br i1 %83, label %72, label %85

85:                                               ; preds = %72, %58, %71, %57
  %86 = phi i32 [ 0, %57 ], [ 0, %71 ], [ %67, %58 ], [ %81, %72 ]
  %87 = getelementptr inbounds [4 x i64]*, [4 x i64]** %22, i64 %54
  %88 = load [4 x i64]*, [4 x i64]** %87, align 8
  %89 = sext i32 %86 to i64
  %90 = getelementptr inbounds [4 x i64], [4 x i64]* %88, i64 %89, i64 0
  %91 = getelementptr inbounds [4 x i64]*, [4 x i64]** %24, i64 %54
  %92 = load [4 x i64]*, [4 x i64]** %91, align 8
  %93 = getelementptr inbounds [4 x i64], [4 x i64]* %92, i64 %89, i64 0
  %94 = getelementptr inbounds [4 x i64]*, [4 x i64]** %26, i64 %54
  %95 = load [4 x i64]*, [4 x i64]** %94, align 8
  %96 = getelementptr inbounds [4 x i64], [4 x i64]* %95, i64 %89, i64 0
  call fastcc void @1(i128* nonnull %27, i128* nonnull %28, i128* nonnull %29, i128* nonnull %27, i128* nonnull %28, i128* nonnull %29, i64* %90, i64* %93, i64* %96)
  %97 = add nuw i64 %54, 1
  %98 = getelementptr inbounds [1 x %1], [1 x %1]* %55, i64 %19
  %99 = icmp eq i64 %97, %17
  br i1 %99, label %46, label %53

100:                                              ; preds = %46, %7
  call fastcc void @2(i64* %0, i128* nonnull %27)
  call fastcc void @2(i64* %1, i128* nonnull %28)
  call fastcc void @2(i64* %2, i128* nonnull %29)
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %13) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %12) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %11) #7
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define internal fastcc void @2(i64* nocapture %0, i128* nocapture readonly %1) unnamed_addr #0 {
  %3 = getelementptr inbounds i128, i128* %1, i64 3
  %4 = load i128, i128* %3, align 16
  %5 = add i128 %4, 18446744069414584320
  %6 = getelementptr inbounds i128, i128* %1, i64 2
  %7 = load i128, i128* %6, align 16
  %8 = lshr i128 %7, 64
  %9 = add i128 %5, %8
  %10 = and i128 %7, 18446744073709551615
  %11 = add nuw nsw i128 %10, 18446673704965373952
  %12 = load i128, i128* %1, align 16
  %13 = add i128 %12, 18446744073709551615
  %14 = getelementptr inbounds i128, i128* %1, i64 1
  %15 = load i128, i128* %14, align 16
  %16 = add i128 %15, 1298074214633706907132628377272319
  %17 = lshr i128 %9, 64
  %18 = trunc i128 %17 to i64
  %19 = and i128 %9, 18446744073709551615
  %20 = sub nsw i128 %19, %17
  %21 = shl nuw nsw i128 %17, 32
  %22 = add nsw i128 %20, %21
  %23 = lshr i128 %22, 64
  %24 = trunc i128 %23 to i64
  %25 = add i64 %24, %18
  %26 = and i128 %22, 18446744073709551615
  %27 = sub nsw i128 %26, %23
  %28 = shl nuw nsw i128 %23, 32
  %29 = add nsw i128 %27, %28
  %30 = zext i64 %25 to i128
  %31 = add i128 %13, %30
  %32 = shl nuw nsw i128 %30, 32
  %33 = sub i128 %16, %32
  %34 = lshr i128 %29, 1
  %35 = trunc i128 %34 to i64
  %36 = trunc i128 %29 to i64
  %37 = and i64 %36, 9223372036854775807
  %38 = sub nsw i64 9223372032559808512, %37
  %39 = and i64 %38, %36
  %40 = or i64 %39, %35
  %41 = ashr i64 %40, 63
  %42 = zext i64 %41 to i128
  %43 = sub i128 %31, %42
  %44 = and i64 %41, 4294967295
  %45 = zext i64 %44 to i128
  %46 = sub i128 %33, %45
  %47 = and i64 %41, -4294967295
  %48 = zext i64 %47 to i128
  %49 = sub nsw i128 %29, %48
  %50 = lshr i128 %43, 64
  %51 = add i128 %46, %50
  %52 = trunc i128 %43 to i64
  %53 = lshr i128 %51, 64
  %54 = add nuw nsw i128 %11, %53
  %55 = trunc i128 %51 to i64
  %56 = lshr i128 %54, 64
  %57 = add nsw i128 %56, %49
  %58 = trunc i128 %54 to i64
  %59 = getelementptr inbounds i64, i64* %0, i64 1
  %60 = getelementptr inbounds i64, i64* %0, i64 2
  %61 = trunc i128 %57 to i64
  %62 = getelementptr inbounds i64, i64* %0, i64 3
  %63 = and i128 %57, 18446744073709551615
  %64 = sub nsw i128 18446744069414584321, %63
  %65 = lshr i128 %64, 64
  %66 = trunc i128 %65 to i64
  %67 = xor i64 %61, -4294967295
  %68 = add i64 %67, -1
  %69 = shl i64 %68, 32
  %70 = and i64 %69, %68
  %71 = shl i64 %70, 16
  %72 = and i64 %71, %70
  %73 = shl i64 %72, 8
  %74 = and i64 %73, %72
  %75 = shl i64 %74, 4
  %76 = and i64 %75, %74
  %77 = shl i64 %76, 2
  %78 = and i64 %77, %76
  %79 = shl i64 %78, 1
  %80 = and i64 %79, %78
  %81 = ashr i64 %80, 63
  %82 = and i128 %54, 18446744073709551615
  %83 = sub nsw i128 0, %82
  %84 = lshr i128 %83, 64
  %85 = trunc i128 %84 to i64
  %86 = and i64 %81, %85
  %87 = or i64 %86, %66
  %88 = add i64 %58, -1
  %89 = shl i64 %88, 32
  %90 = and i64 %89, %88
  %91 = shl i64 %90, 16
  %92 = and i64 %91, %90
  %93 = shl i64 %92, 8
  %94 = and i64 %93, %92
  %95 = shl i64 %94, 4
  %96 = and i64 %95, %94
  %97 = shl i64 %96, 2
  %98 = and i64 %97, %96
  %99 = shl i64 %98, 1
  %100 = and i64 %99, %98
  %101 = ashr i64 %100, 63
  %102 = and i64 %101, %81
  %103 = and i128 %51, 18446744073709551615
  %104 = sub nsw i128 4294967295, %103
  %105 = lshr i128 %104, 64
  %106 = trunc i128 %105 to i64
  %107 = and i64 %102, %106
  %108 = or i64 %107, %87
  %109 = xor i64 %55, 4294967295
  %110 = add i64 %109, -1
  %111 = shl i64 %110, 32
  %112 = and i64 %111, %110
  %113 = shl i64 %112, 16
  %114 = and i64 %113, %112
  %115 = shl i64 %114, 8
  %116 = and i64 %115, %114
  %117 = shl i64 %116, 4
  %118 = and i64 %117, %116
  %119 = shl i64 %118, 2
  %120 = and i64 %119, %118
  %121 = shl i64 %120, 1
  %122 = and i64 %121, %120
  %123 = ashr i64 %122, 63
  %124 = and i64 %123, %102
  %125 = sub i64 -2, %52
  %126 = shl i64 %125, 32
  %127 = and i64 %126, %125
  %128 = shl i64 %127, 16
  %129 = and i64 %128, %127
  %130 = shl i64 %129, 8
  %131 = and i64 %130, %129
  %132 = shl i64 %131, 4
  %133 = and i64 %132, %131
  %134 = shl i64 %133, 2
  %135 = and i64 %134, %133
  %136 = shl i64 %135, 1
  %137 = and i64 %136, %135
  %138 = ashr i64 %137, 63
  %139 = and i64 %138, %124
  %140 = or i64 %139, %108
  %141 = and i128 %43, 18446744073709551615
  %142 = zext i64 %140 to i128
  %143 = sub nsw i128 %141, %142
  %144 = lshr i128 %143, 64
  %145 = trunc i128 %143 to i64
  store i64 %145, i64* %0, align 8
  %146 = and i128 %51, 18446744073709551615
  %147 = and i128 %144, 1
  %148 = sub nsw i128 %146, %147
  %149 = lshr i128 %148, 64
  %150 = and i128 %54, 18446744073709551615
  %151 = and i128 %149, 1
  %152 = sub nsw i128 %150, %151
  %153 = lshr i128 %152, 64
  %154 = and i128 %153, 1
  %155 = sub nsw i128 %57, %154
  %156 = and i64 %140, 4294967295
  %157 = and i128 %148, 18446744073709551615
  %158 = zext i64 %156 to i128
  %159 = sub nsw i128 %157, %158
  %160 = lshr i128 %159, 64
  %161 = trunc i128 %159 to i64
  store i64 %161, i64* %59, align 8
  %162 = and i128 %152, 18446744073709551615
  %163 = and i128 %160, 1
  %164 = sub nsw i128 %162, %163
  %165 = lshr i128 %164, 64
  %166 = trunc i128 %164 to i64
  %167 = and i128 %165, 1
  %168 = sub nsw i128 %155, %167
  %169 = trunc i128 %168 to i64
  store i64 %166, i64* %60, align 8
  %170 = and i64 %140, -4294967295
  %171 = sub i64 %169, %170
  store i64 %171, i64* %62, align 8
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jsmul_block_batch_nistp256_inner(i128* %0, i128* nocapture %1, i128* nocapture %2, %0* nocapture readnone %3, [4 x i64]* nocapture readonly %4, [4 x i64]* nocapture readonly %5, [4 x i64]* nocapture readonly %6, [1 x %1]* readonly %7, i64 %8, i64 %9, i64 %10, i64 %11) local_unnamed_addr #0 {
  %13 = alloca [1 x %4], align 16
  %14 = alloca [4 x i64], align 16
  %15 = alloca [4 x i64], align 16
  %16 = alloca [4 x i64], align 16
  %17 = bitcast [1 x %4]* %13 to i8*
  call void @llvm.lifetime.start.p0i8(i64 56, i8* nonnull %17) #7
  %18 = bitcast [4 x i64]* %14 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %18) #7
  %19 = bitcast [4 x i64]* %15 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %19) #7
  %20 = bitcast [4 x i64]* %16 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %20) #7
  %21 = icmp ult i64 %8, %10
  %22 = select i1 %21, i64 %8, i64 %10
  %23 = getelementptr inbounds [1 x %4], [1 x %4]* %13, i64 0, i64 0
  call void @vec_jsmul_init_nistp256_inner(%4* nonnull %23, %0* undef, i64 %22, i64 %9)
  %24 = bitcast i128* %0 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 16 %24, i8 0, i64 64, i1 false)
  %25 = bitcast i128* %1 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 16 %25, i8 0, i64 64, i1 false)
  %26 = bitcast i128* %2 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 16 %26, i8 0, i64 64, i1 false)
  %27 = icmp eq i64 %8, 0
  br i1 %27, label %51, label %28

28:                                               ; preds = %12
  %29 = getelementptr inbounds [4 x i64], [4 x i64]* %14, i64 0, i64 0
  %30 = getelementptr inbounds [4 x i64], [4 x i64]* %15, i64 0, i64 0
  %31 = getelementptr inbounds [4 x i64], [4 x i64]* %16, i64 0, i64 0
  br label %32

32:                                               ; preds = %28, %43
  %33 = phi i64 [ 0, %28 ], [ %49, %43 ]
  %34 = phi [4 x i64]* [ %4, %28 ], [ %45, %43 ]
  %35 = phi i64 [ %22, %28 ], [ %44, %43 ]
  %36 = phi [4 x i64]* [ %5, %28 ], [ %46, %43 ]
  %37 = phi [4 x i64]* [ %6, %28 ], [ %47, %43 ]
  %38 = phi [1 x %1]* [ %7, %28 ], [ %48, %43 ]
  %39 = sub i64 %8, %33
  %40 = icmp ult i64 %39, %35
  br i1 %40, label %41, label %43

41:                                               ; preds = %32
  %42 = call i32 @vec_jsmul_clear_nistp256_inner(%4* nonnull %23)
  call void @vec_jsmul_init_nistp256_inner(%4* nonnull %23, %0* undef, i64 %39, i64 %9)
  br label %43

43:                                               ; preds = %41, %32
  %44 = phi i64 [ %39, %41 ], [ %35, %32 ]
  call void @vec_jsmul_precomp_nistp256_inner(%4* nonnull %23, %0* undef, [4 x i64]* %34, [4 x i64]* %36, [4 x i64]* %37)
  call void @vec_jsmul_table_nistp256_inner(i64* nonnull %29, i64* nonnull %30, i64* nonnull %31, %0* undef, %4* nonnull %23, [1 x %1]* %38, i64 %11)
  call fastcc void @1(i128* %0, i128* %1, i128* %2, i128* %0, i128* %1, i128* %2, i64* nonnull %29, i64* nonnull %30, i64* nonnull %31)
  %45 = getelementptr inbounds [4 x i64], [4 x i64]* %34, i64 %44
  %46 = getelementptr inbounds [4 x i64], [4 x i64]* %36, i64 %44
  %47 = getelementptr inbounds [4 x i64], [4 x i64]* %37, i64 %44
  %48 = getelementptr inbounds [1 x %1], [1 x %1]* %38, i64 %44
  %49 = add i64 %44, %33
  %50 = icmp ult i64 %49, %8
  br i1 %50, label %32, label %51

51:                                               ; preds = %43, %12
  %52 = call i32 @vec_jsmul_clear_nistp256_inner(%4* nonnull %23)
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %20) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %19) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %18) #7
  call void @llvm.lifetime.end.p0i8(i64 56, i8* nonnull %17) #7
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jsmul_nistp256_inner(i128* %0, i128* nocapture %1, i128* nocapture %2, %0* nocapture readnone %3, [4 x i64]* nocapture readonly %4, [4 x i64]* nocapture readonly %5, [4 x i64]* nocapture readonly %6, [1 x %1]* %7, i64 %8) local_unnamed_addr #0 {
  %10 = icmp eq i64 %8, 0
  br i1 %10, label %20, label %11

11:                                               ; preds = %9, %11
  %12 = phi i64 [ %18, %11 ], [ 0, %9 ]
  %13 = phi i64 [ %17, %11 ], [ 0, %9 ]
  %14 = getelementptr inbounds [1 x %1], [1 x %1]* %7, i64 %12, i64 0
  %15 = tail call i64 @__gmpz_sizeinbase(%1* %14, i32 2) #6
  %16 = icmp ugt i64 %15, %13
  %17 = select i1 %16, i64 %15, i64 %13
  %18 = add nuw i64 %12, 1
  %19 = icmp eq i64 %18, %8
  br i1 %19, label %20, label %11

20:                                               ; preds = %11, %9
  %21 = phi i64 [ 0, %9 ], [ %17, %11 ]
  %22 = trunc i64 %21 to i32
  %23 = tail call i32 @vec_smul_block_width(i32 %22, i32 100) #7
  %24 = sext i32 %23 to i64
  tail call void @vec_jsmul_block_batch_nistp256_inner(i128* %0, i128* %1, i128* %2, %0* undef, [4 x i64]* %4, [4 x i64]* %5, [4 x i64]* %6, [1 x %1]* %7, i64 %8, i64 %24, i64 100, i64 %21)
  ret void
}

declare dso_local i32 @vec_smul_block_width(i32, i32) local_unnamed_addr #5

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jfmul_init_nistp256_inner(%5* nocapture %0, %0* %1, i64 %2) local_unnamed_addr #0 {
  %4 = getelementptr inbounds %0, %0* %1, i64 0, i32 6, i64 0
  %5 = tail call i64 @__gmpz_sizeinbase(%1* nonnull %4, i32 2) #6
  %6 = trunc i64 %5 to i32
  %7 = trunc i64 %2 to i32
  %8 = tail call i32 @vec_fmul_block_width(i32 %6, i32 %7) #7
  %9 = getelementptr inbounds %5, %5* %0, i64 0, i32 0, i64 0
  %10 = sext i32 %8 to i64
  tail call void @vec_jsmul_init_nistp256_inner(%4* %9, %0* undef, i64 %10, i64 %10)
  %11 = add nsw i32 %8, -1
  %12 = add nsw i32 %11, %6
  %13 = sdiv i32 %12, %8
  %14 = sext i32 %13 to i64
  %15 = getelementptr inbounds %5, %5* %0, i64 0, i32 1
  store i64 %14, i64* %15, align 8
  ret void
}

declare dso_local i32 @vec_fmul_block_width(i32, i32) local_unnamed_addr #5

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jfmul_clear_free_nistp256_inner(%5* nocapture %0) local_unnamed_addr #0 {
  %2 = getelementptr inbounds %5, %5* %0, i64 0, i32 0, i64 0
  %3 = tail call i32 @vec_jsmul_clear_nistp256_inner(%4* %2)
  %4 = bitcast %5* %0 to i8*
  tail call void @free(i8* %4) #7
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jfmul_prcmp_nistp256_inner(%0* nocapture readnone %0, %5* nocapture readonly %1, i64* nocapture readonly %2, i64* nocapture readonly %3, i64* nocapture readonly %4) local_unnamed_addr #0 {
  %6 = alloca [4 x i128], align 16
  %7 = alloca [4 x i128], align 16
  %8 = alloca [4 x i128], align 16
  %9 = alloca [4 x i128], align 16
  %10 = alloca [4 x i128], align 16
  %11 = alloca [4 x i128], align 16
  %12 = getelementptr inbounds %5, %5* %1, i64 0, i32 0, i64 0
  %13 = getelementptr inbounds %5, %5* %1, i64 0, i32 0, i64 0, i32 2
  %14 = load i64, i64* %13, align 8
  %15 = shl i64 %14, 5
  %16 = tail call noalias i8* @malloc(i64 %15) #7
  %17 = bitcast i8* %16 to [4 x i64]*
  %18 = tail call noalias i8* @malloc(i64 %15) #7
  %19 = bitcast i8* %18 to [4 x i64]*
  %20 = tail call noalias i8* @malloc(i64 %15) #7
  %21 = bitcast i8* %20 to [4 x i64]*
  %22 = bitcast i64* %2 to <2 x i64>*
  %23 = load <2 x i64>, <2 x i64>* %22, align 8
  %24 = bitcast i8* %16 to <2 x i64>*
  store <2 x i64> %23, <2 x i64>* %24, align 8
  %25 = getelementptr inbounds i64, i64* %2, i64 2
  %26 = getelementptr inbounds i8, i8* %16, i64 16
  %27 = bitcast i64* %25 to <2 x i64>*
  %28 = load <2 x i64>, <2 x i64>* %27, align 8
  %29 = bitcast i8* %26 to <2 x i64>*
  store <2 x i64> %28, <2 x i64>* %29, align 8
  %30 = bitcast i8* %18 to i64*
  %31 = load i64, i64* %3, align 8
  store i64 %31, i64* %30, align 8
  %32 = getelementptr inbounds i64, i64* %3, i64 1
  %33 = load i64, i64* %32, align 8
  %34 = getelementptr inbounds i8, i8* %18, i64 8
  %35 = bitcast i8* %34 to i64*
  store i64 %33, i64* %35, align 8
  %36 = getelementptr inbounds i64, i64* %3, i64 2
  %37 = load i64, i64* %36, align 8
  %38 = getelementptr inbounds i8, i8* %18, i64 16
  %39 = bitcast i8* %38 to i64*
  store i64 %37, i64* %39, align 8
  %40 = getelementptr inbounds i64, i64* %3, i64 3
  %41 = load i64, i64* %40, align 8
  %42 = getelementptr inbounds i8, i8* %18, i64 24
  %43 = bitcast i8* %42 to i64*
  store i64 %41, i64* %43, align 8
  %44 = bitcast i8* %20 to i64*
  %45 = load i64, i64* %4, align 8
  store i64 %45, i64* %44, align 8
  %46 = getelementptr inbounds i64, i64* %4, i64 1
  %47 = load i64, i64* %46, align 8
  %48 = getelementptr inbounds i8, i8* %20, i64 8
  %49 = bitcast i8* %48 to i64*
  store i64 %47, i64* %49, align 8
  %50 = getelementptr inbounds i64, i64* %4, i64 2
  %51 = load i64, i64* %50, align 8
  %52 = getelementptr inbounds i8, i8* %20, i64 16
  %53 = bitcast i8* %52 to i64*
  store i64 %51, i64* %53, align 8
  %54 = getelementptr inbounds i64, i64* %4, i64 3
  %55 = load i64, i64* %54, align 8
  %56 = getelementptr inbounds i8, i8* %20, i64 24
  %57 = bitcast i8* %56 to i64*
  store i64 %55, i64* %57, align 8
  %58 = icmp ugt i64 %14, 1
  br i1 %58, label %59, label %342

59:                                               ; preds = %5
  %60 = getelementptr inbounds %5, %5* %1, i64 0, i32 1
  %61 = bitcast [4 x i128]* %6 to i8*
  %62 = bitcast [4 x i128]* %7 to i8*
  %63 = bitcast [4 x i128]* %8 to i8*
  %64 = bitcast [4 x i128]* %9 to i8*
  %65 = bitcast [4 x i128]* %10 to i8*
  %66 = bitcast [4 x i128]* %11 to i8*
  %67 = getelementptr inbounds [4 x i128], [4 x i128]* %9, i64 0, i64 0
  %68 = getelementptr inbounds [4 x i128], [4 x i128]* %9, i64 0, i64 1
  %69 = getelementptr inbounds [4 x i128], [4 x i128]* %9, i64 0, i64 2
  %70 = getelementptr inbounds [4 x i128], [4 x i128]* %9, i64 0, i64 3
  %71 = getelementptr inbounds [4 x i128], [4 x i128]* %10, i64 0, i64 0
  %72 = getelementptr inbounds [4 x i128], [4 x i128]* %10, i64 0, i64 1
  %73 = getelementptr inbounds [4 x i128], [4 x i128]* %10, i64 0, i64 2
  %74 = getelementptr inbounds [4 x i128], [4 x i128]* %10, i64 0, i64 3
  %75 = getelementptr inbounds [4 x i128], [4 x i128]* %11, i64 0, i64 0
  %76 = getelementptr inbounds [4 x i128], [4 x i128]* %11, i64 0, i64 1
  %77 = getelementptr inbounds [4 x i128], [4 x i128]* %11, i64 0, i64 2
  %78 = getelementptr inbounds [4 x i128], [4 x i128]* %11, i64 0, i64 3
  %79 = getelementptr inbounds [4 x i128], [4 x i128]* %6, i64 0, i64 0
  %80 = getelementptr inbounds [4 x i128], [4 x i128]* %7, i64 0, i64 0
  %81 = getelementptr inbounds [4 x i128], [4 x i128]* %8, i64 0, i64 0
  %82 = getelementptr inbounds [4 x i128], [4 x i128]* %6, i64 0, i64 3
  %83 = getelementptr inbounds [4 x i128], [4 x i128]* %6, i64 0, i64 2
  %84 = getelementptr inbounds [4 x i128], [4 x i128]* %6, i64 0, i64 1
  %85 = getelementptr inbounds [4 x i128], [4 x i128]* %7, i64 0, i64 3
  %86 = getelementptr inbounds [4 x i128], [4 x i128]* %7, i64 0, i64 2
  %87 = getelementptr inbounds [4 x i128], [4 x i128]* %7, i64 0, i64 1
  %88 = getelementptr inbounds [4 x i128], [4 x i128]* %8, i64 0, i64 3
  %89 = getelementptr inbounds [4 x i128], [4 x i128]* %8, i64 0, i64 2
  %90 = getelementptr inbounds [4 x i128], [4 x i128]* %8, i64 0, i64 1
  %91 = load i64, i64* %60, align 8
  br label %92

92:                                               ; preds = %330, %59
  %93 = phi i64 [ %91, %59 ], [ %331, %330 ]
  %94 = phi i64 [ %55, %59 ], [ %332, %330 ]
  %95 = phi i64 [ %51, %59 ], [ %333, %330 ]
  %96 = phi i64 [ %47, %59 ], [ %334, %330 ]
  %97 = phi i64 [ %45, %59 ], [ %335, %330 ]
  %98 = phi i64 [ %41, %59 ], [ %336, %330 ]
  %99 = phi i64 [ %37, %59 ], [ %337, %330 ]
  %100 = phi i64 [ %33, %59 ], [ %338, %330 ]
  %101 = phi i64 [ %31, %59 ], [ %339, %330 ]
  %102 = phi i64 [ 1, %59 ], [ %340, %330 ]
  %103 = getelementptr inbounds [4 x i64], [4 x i64]* %17, i64 %102, i64 0
  %104 = add i64 %102, -1
  %105 = getelementptr inbounds [4 x i64], [4 x i64]* %17, i64 %104, i64 0
  %106 = load i64, i64* %105, align 8
  store i64 %106, i64* %103, align 8
  %107 = getelementptr inbounds [4 x i64], [4 x i64]* %17, i64 %104, i64 1
  %108 = load i64, i64* %107, align 8
  %109 = getelementptr inbounds [4 x i64], [4 x i64]* %17, i64 %102, i64 1
  store i64 %108, i64* %109, align 8
  %110 = getelementptr inbounds [4 x i64], [4 x i64]* %17, i64 %104, i64 2
  %111 = load i64, i64* %110, align 8
  %112 = getelementptr inbounds [4 x i64], [4 x i64]* %17, i64 %102, i64 2
  store i64 %111, i64* %112, align 8
  %113 = getelementptr inbounds [4 x i64], [4 x i64]* %17, i64 %104, i64 3
  %114 = load i64, i64* %113, align 8
  %115 = getelementptr inbounds [4 x i64], [4 x i64]* %17, i64 %102, i64 3
  store i64 %114, i64* %115, align 8
  %116 = getelementptr inbounds [4 x i64], [4 x i64]* %19, i64 %102, i64 0
  store i64 %101, i64* %116, align 8
  %117 = getelementptr inbounds [4 x i64], [4 x i64]* %19, i64 %102, i64 1
  store i64 %100, i64* %117, align 8
  %118 = getelementptr inbounds [4 x i64], [4 x i64]* %19, i64 %102, i64 2
  store i64 %99, i64* %118, align 8
  %119 = getelementptr inbounds [4 x i64], [4 x i64]* %19, i64 %102, i64 3
  store i64 %98, i64* %119, align 8
  %120 = getelementptr inbounds [4 x i64], [4 x i64]* %21, i64 %102, i64 0
  store i64 %97, i64* %120, align 8
  %121 = getelementptr inbounds [4 x i64], [4 x i64]* %21, i64 %102, i64 1
  store i64 %96, i64* %121, align 8
  %122 = getelementptr inbounds [4 x i64], [4 x i64]* %21, i64 %102, i64 2
  store i64 %95, i64* %122, align 8
  %123 = getelementptr inbounds [4 x i64], [4 x i64]* %21, i64 %102, i64 3
  store i64 %94, i64* %123, align 8
  %124 = icmp eq i64 %93, 0
  br i1 %124, label %330, label %125

125:                                              ; preds = %92, %316
  %126 = phi i64 [ %312, %316 ], [ %94, %92 ]
  %127 = phi i64 [ %311, %316 ], [ %95, %92 ]
  %128 = phi i64 [ %308, %316 ], [ %96, %92 ]
  %129 = phi i64 [ %305, %316 ], [ %97, %92 ]
  %130 = phi i64 [ %258, %316 ], [ %98, %92 ]
  %131 = phi i64 [ %257, %316 ], [ %99, %92 ]
  %132 = phi i64 [ %254, %316 ], [ %100, %92 ]
  %133 = phi i64 [ %251, %316 ], [ %101, %92 ]
  %134 = phi i64 [ %320, %316 ], [ %114, %92 ]
  %135 = phi i64 [ %319, %316 ], [ %111, %92 ]
  %136 = phi i64 [ %318, %316 ], [ %108, %92 ]
  %137 = phi i64 [ %317, %316 ], [ %106, %92 ]
  %138 = phi i64 [ %313, %316 ], [ 0, %92 ]
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %61) #7
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %62) #7
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %63) #7
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %64) #7
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %65) #7
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %66) #7
  %139 = zext i64 %137 to i128
  store i128 %139, i128* %67, align 16
  %140 = zext i64 %136 to i128
  store i128 %140, i128* %68, align 16
  %141 = zext i64 %135 to i128
  store i128 %141, i128* %69, align 16
  %142 = zext i64 %134 to i128
  store i128 %142, i128* %70, align 16
  %143 = zext i64 %133 to i128
  store i128 %143, i128* %71, align 16
  %144 = zext i64 %132 to i128
  store i128 %144, i128* %72, align 16
  %145 = zext i64 %131 to i128
  store i128 %145, i128* %73, align 16
  %146 = zext i64 %130 to i128
  store i128 %146, i128* %74, align 16
  %147 = zext i64 %129 to i128
  store i128 %147, i128* %75, align 16
  %148 = zext i64 %128 to i128
  store i128 %148, i128* %76, align 16
  %149 = zext i64 %127 to i128
  store i128 %149, i128* %77, align 16
  %150 = zext i64 %126 to i128
  store i128 %150, i128* %78, align 16
  call fastcc void @0(i128* nonnull %79, i128* nonnull %80, i128* nonnull %81, i128* nonnull %67, i128* nonnull %71, i128* nonnull %75) #7
  %151 = load i128, i128* %82, align 16
  %152 = add i128 %151, 18446744069414584320
  %153 = load i128, i128* %83, align 16
  %154 = lshr i128 %153, 64
  %155 = add i128 %152, %154
  %156 = and i128 %153, 18446744073709551615
  %157 = add nuw nsw i128 %156, 18446673704965373952
  %158 = load i128, i128* %79, align 16
  %159 = add i128 %158, 18446744073709551615
  %160 = load i128, i128* %84, align 16
  %161 = add i128 %160, 1298074214633706907132628377272319
  %162 = lshr i128 %155, 64
  %163 = trunc i128 %162 to i64
  %164 = and i128 %155, 18446744073709551615
  %165 = sub nsw i128 %164, %162
  %166 = shl nuw nsw i128 %162, 32
  %167 = add nsw i128 %165, %166
  %168 = lshr i128 %167, 64
  %169 = trunc i128 %168 to i64
  %170 = add i64 %169, %163
  %171 = and i128 %167, 18446744073709551615
  %172 = sub nsw i128 %171, %168
  %173 = shl nuw nsw i128 %168, 32
  %174 = add nsw i128 %172, %173
  %175 = zext i64 %170 to i128
  %176 = add i128 %159, %175
  %177 = shl nuw nsw i128 %175, 32
  %178 = sub i128 %161, %177
  %179 = lshr i128 %174, 1
  %180 = trunc i128 %179 to i64
  %181 = trunc i128 %174 to i64
  %182 = and i64 %181, 9223372036854775807
  %183 = sub nsw i64 9223372032559808512, %182
  %184 = and i64 %183, %181
  %185 = or i64 %184, %180
  %186 = ashr i64 %185, 63
  %187 = zext i64 %186 to i128
  %188 = sub i128 %176, %187
  %189 = and i64 %186, 4294967295
  %190 = zext i64 %189 to i128
  %191 = sub i128 %178, %190
  %192 = and i64 %186, -4294967295
  %193 = zext i64 %192 to i128
  %194 = sub nsw i128 %174, %193
  %195 = lshr i128 %188, 64
  %196 = add i128 %191, %195
  %197 = trunc i128 %188 to i64
  %198 = lshr i128 %196, 64
  %199 = add nuw nsw i128 %157, %198
  %200 = trunc i128 %196 to i64
  %201 = lshr i128 %199, 64
  %202 = add nsw i128 %194, %201
  %203 = trunc i128 %199 to i64
  store i64 %197, i64* %103, align 8
  store i64 %200, i64* %109, align 8
  store i64 %203, i64* %112, align 8
  %204 = trunc i128 %202 to i64
  store i64 %204, i64* %115, align 8
  %205 = load i128, i128* %85, align 16
  %206 = add i128 %205, 18446744069414584320
  %207 = load i128, i128* %86, align 16
  %208 = lshr i128 %207, 64
  %209 = add i128 %206, %208
  %210 = and i128 %207, 18446744073709551615
  %211 = add nuw nsw i128 %210, 18446673704965373952
  %212 = load i128, i128* %80, align 16
  %213 = add i128 %212, 18446744073709551615
  %214 = load i128, i128* %87, align 16
  %215 = add i128 %214, 1298074214633706907132628377272319
  %216 = lshr i128 %209, 64
  %217 = trunc i128 %216 to i64
  %218 = and i128 %209, 18446744073709551615
  %219 = sub nsw i128 %218, %216
  %220 = shl nuw nsw i128 %216, 32
  %221 = add nsw i128 %219, %220
  %222 = lshr i128 %221, 64
  %223 = trunc i128 %222 to i64
  %224 = add i64 %223, %217
  %225 = and i128 %221, 18446744073709551615
  %226 = sub nsw i128 %225, %222
  %227 = shl nuw nsw i128 %222, 32
  %228 = add nsw i128 %226, %227
  %229 = zext i64 %224 to i128
  %230 = add i128 %213, %229
  %231 = shl nuw nsw i128 %229, 32
  %232 = sub i128 %215, %231
  %233 = lshr i128 %228, 1
  %234 = trunc i128 %233 to i64
  %235 = trunc i128 %228 to i64
  %236 = and i64 %235, 9223372036854775807
  %237 = sub nsw i64 9223372032559808512, %236
  %238 = and i64 %237, %235
  %239 = or i64 %238, %234
  %240 = ashr i64 %239, 63
  %241 = zext i64 %240 to i128
  %242 = sub i128 %230, %241
  %243 = and i64 %240, 4294967295
  %244 = zext i64 %243 to i128
  %245 = sub i128 %232, %244
  %246 = and i64 %240, -4294967295
  %247 = zext i64 %246 to i128
  %248 = sub nsw i128 %228, %247
  %249 = lshr i128 %242, 64
  %250 = add i128 %245, %249
  %251 = trunc i128 %242 to i64
  %252 = lshr i128 %250, 64
  %253 = add nuw nsw i128 %211, %252
  %254 = trunc i128 %250 to i64
  %255 = lshr i128 %253, 64
  %256 = add nsw i128 %248, %255
  %257 = trunc i128 %253 to i64
  store i64 %251, i64* %116, align 8
  store i64 %254, i64* %117, align 8
  store i64 %257, i64* %118, align 8
  %258 = trunc i128 %256 to i64
  store i64 %258, i64* %119, align 8
  %259 = load i128, i128* %88, align 16
  %260 = add i128 %259, 18446744069414584320
  %261 = load i128, i128* %89, align 16
  %262 = lshr i128 %261, 64
  %263 = add i128 %260, %262
  %264 = and i128 %261, 18446744073709551615
  %265 = add nuw nsw i128 %264, 18446673704965373952
  %266 = load i128, i128* %81, align 16
  %267 = add i128 %266, 18446744073709551615
  %268 = load i128, i128* %90, align 16
  %269 = add i128 %268, 1298074214633706907132628377272319
  %270 = lshr i128 %263, 64
  %271 = trunc i128 %270 to i64
  %272 = and i128 %263, 18446744073709551615
  %273 = sub nsw i128 %272, %270
  %274 = shl nuw nsw i128 %270, 32
  %275 = add nsw i128 %273, %274
  %276 = lshr i128 %275, 64
  %277 = trunc i128 %276 to i64
  %278 = add i64 %277, %271
  %279 = and i128 %275, 18446744073709551615
  %280 = sub nsw i128 %279, %276
  %281 = shl nuw nsw i128 %276, 32
  %282 = add nsw i128 %280, %281
  %283 = zext i64 %278 to i128
  %284 = add i128 %267, %283
  %285 = shl nuw nsw i128 %283, 32
  %286 = sub i128 %269, %285
  %287 = lshr i128 %282, 1
  %288 = trunc i128 %287 to i64
  %289 = trunc i128 %282 to i64
  %290 = and i64 %289, 9223372036854775807
  %291 = sub nsw i64 9223372032559808512, %290
  %292 = and i64 %291, %289
  %293 = or i64 %292, %288
  %294 = ashr i64 %293, 63
  %295 = zext i64 %294 to i128
  %296 = sub i128 %284, %295
  %297 = and i64 %294, 4294967295
  %298 = zext i64 %297 to i128
  %299 = sub i128 %286, %298
  %300 = and i64 %294, -4294967295
  %301 = zext i64 %300 to i128
  %302 = sub nsw i128 %282, %301
  %303 = lshr i128 %296, 64
  %304 = add i128 %299, %303
  %305 = trunc i128 %296 to i64
  %306 = lshr i128 %304, 64
  %307 = add nuw nsw i128 %265, %306
  %308 = trunc i128 %304 to i64
  %309 = lshr i128 %307, 64
  %310 = add nsw i128 %302, %309
  %311 = trunc i128 %307 to i64
  store i64 %305, i64* %120, align 8
  store i64 %308, i64* %121, align 8
  store i64 %311, i64* %122, align 8
  %312 = trunc i128 %310 to i64
  store i64 %312, i64* %123, align 8
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %66) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %65) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %64) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %63) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %62) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %61) #7
  %313 = add nuw i64 %138, 1
  %314 = load i64, i64* %60, align 8
  %315 = icmp ult i64 %313, %314
  br i1 %315, label %316, label %321

316:                                              ; preds = %125
  %317 = load i64, i64* %103, align 8
  %318 = load i64, i64* %109, align 8
  %319 = load i64, i64* %112, align 8
  %320 = load i64, i64* %115, align 8
  br label %125

321:                                              ; preds = %125
  %322 = trunc i128 %242 to i64
  %323 = trunc i128 %250 to i64
  %324 = trunc i128 %253 to i64
  %325 = trunc i128 %256 to i64
  %326 = trunc i128 %296 to i64
  %327 = trunc i128 %304 to i64
  %328 = trunc i128 %307 to i64
  %329 = trunc i128 %310 to i64
  br label %330

330:                                              ; preds = %321, %92
  %331 = phi i64 [ 0, %92 ], [ %314, %321 ]
  %332 = phi i64 [ %94, %92 ], [ %329, %321 ]
  %333 = phi i64 [ %95, %92 ], [ %328, %321 ]
  %334 = phi i64 [ %96, %92 ], [ %327, %321 ]
  %335 = phi i64 [ %97, %92 ], [ %326, %321 ]
  %336 = phi i64 [ %98, %92 ], [ %325, %321 ]
  %337 = phi i64 [ %99, %92 ], [ %324, %321 ]
  %338 = phi i64 [ %100, %92 ], [ %323, %321 ]
  %339 = phi i64 [ %101, %92 ], [ %322, %321 ]
  %340 = add nuw i64 %102, 1
  %341 = icmp eq i64 %340, %14
  br i1 %341, label %342, label %92

342:                                              ; preds = %330, %5
  call void @vec_jsmul_precomp_nistp256_inner(%4* %12, %0* undef, [4 x i64]* %17, [4 x i64]* %19, [4 x i64]* %21)
  call void @free(i8* %16) #7
  call void @free(i8* %18) #7
  call void @free(i8* %20) #7
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jfmul_cmp_nistp256_inner(i64* nocapture %0, i64* nocapture %1, i64* nocapture %2, %0* nocapture readnone %3, %5* nocapture readonly %4, %1* %5) local_unnamed_addr #0 {
  %7 = alloca [1 x %1], align 16
  %8 = getelementptr inbounds %5, %5* %4, i64 0, i32 0, i64 0
  %9 = getelementptr inbounds %5, %5* %4, i64 0, i32 0, i64 0, i32 2
  %10 = load i64, i64* %9, align 8
  %11 = bitcast [1 x %1]* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 16, i8* nonnull %11) #7
  %12 = getelementptr inbounds [1 x %1], [1 x %1]* %7, i64 0, i64 0
  call void @__gmpz_init(%1* nonnull %12) #7
  %13 = call [1 x %1]* @vec_array_alloc_init(i64 %10) #7
  call void @__gmpz_set(%1* nonnull %12, %1* %5) #7
  %14 = icmp eq i64 %10, 0
  %15 = getelementptr inbounds %5, %5* %4, i64 0, i32 1
  br i1 %14, label %23, label %16

16:                                               ; preds = %6, %16
  %17 = phi i64 [ %21, %16 ], [ 0, %6 ]
  %18 = getelementptr inbounds [1 x %1], [1 x %1]* %13, i64 %17, i64 0
  %19 = load i64, i64* %15, align 8
  call void @__gmpz_tdiv_r_2exp(%1* %18, %1* nonnull %12, i64 %19) #7
  %20 = load i64, i64* %15, align 8
  call void @__gmpz_tdiv_q_2exp(%1* nonnull %12, %1* nonnull %12, i64 %20) #7
  %21 = add nuw i64 %17, 1
  %22 = icmp eq i64 %21, %10
  br i1 %22, label %23, label %16

23:                                               ; preds = %16, %6
  %24 = load i64, i64* %15, align 8
  call void @vec_jsmul_table_nistp256_inner(i64* %0, i64* %1, i64* %2, %0* undef, %4* %8, [1 x %1]* %13, i64 %24)
  call void @vec_array_clear_free([1 x %1]* %13, i64 %10) #7
  call void @__gmpz_clear(%1* nonnull %12) #7
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %11) #7
  ret void
}

declare dso_local void @__gmpz_init(%1*) local_unnamed_addr #5

declare dso_local [1 x %1]* @vec_array_alloc_init(i64) local_unnamed_addr #5

declare dso_local void @__gmpz_set(%1*, %1*) local_unnamed_addr #5

declare dso_local void @__gmpz_tdiv_r_2exp(%1*, %1*, i64) local_unnamed_addr #5

declare dso_local void @__gmpz_tdiv_q_2exp(%1*, %1*, i64) local_unnamed_addr #5

declare dso_local void @vec_array_clear_free([1 x %1]*, i64) local_unnamed_addr #5

declare dso_local void @__gmpz_clear(%1*) local_unnamed_addr #5

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jmulsw_nistp256(%1* %0, %1* %1, %1* %2, %0* nocapture readnone %3, %1* %4, %1* %5, %1* %6, %1* %7) local_unnamed_addr #0 {
  %9 = alloca [4 x i64], align 16
  %10 = alloca [4 x i64], align 16
  %11 = alloca [4 x i64], align 16
  %12 = alloca [4 x i64], align 16
  %13 = alloca [4 x i128], align 16
  %14 = alloca [4 x i128], align 16
  %15 = alloca [4 x i128], align 16
  %16 = bitcast [4 x i64]* %10 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %16) #7
  %17 = bitcast [4 x i64]* %11 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %17) #7
  %18 = bitcast [4 x i64]* %12 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %18) #7
  %19 = bitcast [4 x i128]* %13 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %19) #7
  %20 = bitcast [4 x i128]* %14 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %20) #7
  %21 = bitcast [4 x i128]* %15 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %21) #7
  %22 = getelementptr inbounds [4 x i64], [4 x i64]* %10, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %16, i8 0, i64 32, i1 false) #7
  %23 = call i8* @__gmpz_export(i8* nonnull %16, i64* null, i32 -1, i64 8, i32 0, i64 0, %1* %4) #7
  %24 = getelementptr inbounds [4 x i64], [4 x i64]* %11, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %17, i8 0, i64 32, i1 false) #7
  %25 = call i8* @__gmpz_export(i8* nonnull %17, i64* null, i32 -1, i64 8, i32 0, i64 0, %1* %5) #7
  %26 = getelementptr inbounds [4 x i64], [4 x i64]* %12, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %18, i8 0, i64 32, i1 false) #7
  %27 = call i8* @__gmpz_export(i8* nonnull %18, i64* null, i32 -1, i64 8, i32 0, i64 0, %1* %6) #7
  %28 = getelementptr inbounds [4 x i128], [4 x i128]* %13, i64 0, i64 0
  %29 = getelementptr inbounds [4 x i128], [4 x i128]* %14, i64 0, i64 0
  %30 = getelementptr inbounds [4 x i128], [4 x i128]* %15, i64 0, i64 0
  call void @vec_jmulsw_nistp256_inner(i128* nonnull %28, i128* nonnull %29, i128* nonnull %30, %0* undef, i64* nonnull %22, i64* nonnull %24, i64* nonnull %26, %1* %7)
  %31 = bitcast [4 x i64]* %9 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %31) #7
  %32 = getelementptr inbounds [4 x i64], [4 x i64]* %9, i64 0, i64 0
  call fastcc void @2(i64* nonnull %32, i128* nonnull %28) #7
  %33 = load i64, i64* %32, align 16
  %34 = getelementptr inbounds [4 x i64], [4 x i64]* %9, i64 0, i64 1
  %35 = load i64, i64* %34, align 8
  %36 = or i64 %35, %33
  %37 = getelementptr inbounds [4 x i64], [4 x i64]* %9, i64 0, i64 2
  %38 = load i64, i64* %37, align 16
  %39 = or i64 %36, %38
  %40 = getelementptr inbounds [4 x i64], [4 x i64]* %9, i64 0, i64 3
  %41 = load i64, i64* %40, align 8
  %42 = or i64 %39, %41
  %43 = add i64 %42, -1
  %44 = xor i64 %33, -1
  %45 = xor i64 %35, 4294967295
  %46 = or i64 %45, %44
  %47 = or i64 %46, %38
  %48 = xor i64 %41, -4294967295
  %49 = or i64 %47, %48
  %50 = add i64 %49, -1
  %51 = insertelement <2 x i64> undef, i64 %50, i32 0
  %52 = insertelement <2 x i64> %51, i64 %43, i32 1
  %53 = shl <2 x i64> %52, <i64 32, i64 32>
  %54 = and <2 x i64> %53, %52
  %55 = shl <2 x i64> %54, <i64 16, i64 16>
  %56 = and <2 x i64> %55, %54
  %57 = shl <2 x i64> %56, <i64 8, i64 8>
  %58 = and <2 x i64> %57, %56
  %59 = shl <2 x i64> %58, <i64 4, i64 4>
  %60 = and <2 x i64> %59, %58
  %61 = shl <2 x i64> %60, <i64 2, i64 2>
  %62 = and <2 x i64> %61, %60
  %63 = shl <2 x i64> %62, <i64 1, i64 1>
  %64 = and <2 x i64> %63, %62
  %65 = extractelement <2 x i64> %64, i32 0
  %66 = extractelement <2 x i64> %64, i32 1
  %67 = or i64 %65, %66
  %68 = ashr i64 %67, 63
  %69 = zext i64 %68 to i128
  %70 = shl nuw i128 %69, 64
  %71 = or i128 %70, %69
  %72 = icmp eq i128 %71, 0
  br i1 %72, label %74, label %73

73:                                               ; preds = %8
  call void @__gmpz_set_si(%1* %0, i64 0) #7
  br label %75

74:                                               ; preds = %8
  call void @__gmpz_import(%1* %0, i64 4, i32 -1, i64 8, i32 0, i64 0, i8* nonnull %31) #7
  br label %75

75:                                               ; preds = %73, %74
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %31) #7
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %31) #7
  call fastcc void @2(i64* nonnull %32, i128* nonnull %29) #7
  %76 = load i64, i64* %32, align 16
  %77 = load i64, i64* %34, align 8
  %78 = or i64 %77, %76
  %79 = load i64, i64* %37, align 16
  %80 = or i64 %78, %79
  %81 = load i64, i64* %40, align 8
  %82 = or i64 %80, %81
  %83 = add i64 %82, -1
  %84 = xor i64 %76, -1
  %85 = xor i64 %77, 4294967295
  %86 = or i64 %85, %84
  %87 = or i64 %86, %79
  %88 = xor i64 %81, -4294967295
  %89 = or i64 %87, %88
  %90 = add i64 %89, -1
  %91 = insertelement <2 x i64> undef, i64 %90, i32 0
  %92 = insertelement <2 x i64> %91, i64 %83, i32 1
  %93 = shl <2 x i64> %92, <i64 32, i64 32>
  %94 = and <2 x i64> %93, %92
  %95 = shl <2 x i64> %94, <i64 16, i64 16>
  %96 = and <2 x i64> %95, %94
  %97 = shl <2 x i64> %96, <i64 8, i64 8>
  %98 = and <2 x i64> %97, %96
  %99 = shl <2 x i64> %98, <i64 4, i64 4>
  %100 = and <2 x i64> %99, %98
  %101 = shl <2 x i64> %100, <i64 2, i64 2>
  %102 = and <2 x i64> %101, %100
  %103 = shl <2 x i64> %102, <i64 1, i64 1>
  %104 = and <2 x i64> %103, %102
  %105 = extractelement <2 x i64> %104, i32 0
  %106 = extractelement <2 x i64> %104, i32 1
  %107 = or i64 %105, %106
  %108 = ashr i64 %107, 63
  %109 = zext i64 %108 to i128
  %110 = shl nuw i128 %109, 64
  %111 = or i128 %110, %109
  %112 = icmp eq i128 %111, 0
  br i1 %112, label %114, label %113

113:                                              ; preds = %75
  call void @__gmpz_set_si(%1* %1, i64 0) #7
  br label %115

114:                                              ; preds = %75
  call void @__gmpz_import(%1* %1, i64 4, i32 -1, i64 8, i32 0, i64 0, i8* nonnull %31) #7
  br label %115

115:                                              ; preds = %113, %114
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %31) #7
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %31) #7
  call fastcc void @2(i64* nonnull %32, i128* nonnull %30) #7
  %116 = load i64, i64* %32, align 16
  %117 = load i64, i64* %34, align 8
  %118 = or i64 %117, %116
  %119 = load i64, i64* %37, align 16
  %120 = or i64 %118, %119
  %121 = load i64, i64* %40, align 8
  %122 = or i64 %120, %121
  %123 = add i64 %122, -1
  %124 = xor i64 %116, -1
  %125 = xor i64 %117, 4294967295
  %126 = or i64 %125, %124
  %127 = or i64 %126, %119
  %128 = xor i64 %121, -4294967295
  %129 = or i64 %127, %128
  %130 = add i64 %129, -1
  %131 = insertelement <2 x i64> undef, i64 %130, i32 0
  %132 = insertelement <2 x i64> %131, i64 %123, i32 1
  %133 = shl <2 x i64> %132, <i64 32, i64 32>
  %134 = and <2 x i64> %133, %132
  %135 = shl <2 x i64> %134, <i64 16, i64 16>
  %136 = and <2 x i64> %135, %134
  %137 = shl <2 x i64> %136, <i64 8, i64 8>
  %138 = and <2 x i64> %137, %136
  %139 = shl <2 x i64> %138, <i64 4, i64 4>
  %140 = and <2 x i64> %139, %138
  %141 = shl <2 x i64> %140, <i64 2, i64 2>
  %142 = and <2 x i64> %141, %140
  %143 = shl <2 x i64> %142, <i64 1, i64 1>
  %144 = and <2 x i64> %143, %142
  %145 = extractelement <2 x i64> %144, i32 0
  %146 = extractelement <2 x i64> %144, i32 1
  %147 = or i64 %145, %146
  %148 = ashr i64 %147, 63
  %149 = zext i64 %148 to i128
  %150 = shl nuw i128 %149, 64
  %151 = or i128 %150, %149
  %152 = icmp eq i128 %151, 0
  br i1 %152, label %154, label %153

153:                                              ; preds = %115
  call void @__gmpz_set_si(%1* %2, i64 0) #7
  br label %155

154:                                              ; preds = %115
  call void @__gmpz_import(%1* %2, i64 4, i32 -1, i64 8, i32 0, i64 0, i8* nonnull %31) #7
  br label %155

155:                                              ; preds = %153, %154
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %31) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %21) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %20) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %19) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %18) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %17) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %16) #7
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jsmul_nistp256(%1* %0, %1* %1, %1* %2, %0* nocapture readnone %3, [1 x %1]* %4, [1 x %1]* %5, [1 x %1]* %6, [1 x %1]* %7, i64 %8) local_unnamed_addr #0 {
  %10 = alloca [4 x i64], align 16
  %11 = alloca [4 x i128], align 16
  %12 = alloca [4 x i128], align 16
  %13 = alloca [4 x i128], align 16
  %14 = bitcast [4 x i128]* %11 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %14) #7
  %15 = bitcast [4 x i128]* %12 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %15) #7
  %16 = bitcast [4 x i128]* %13 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %16) #7
  %17 = shl i64 %8, 5
  %18 = tail call noalias i8* @malloc(i64 %17) #7
  %19 = bitcast i8* %18 to [4 x i64]*
  %20 = icmp eq i64 %8, 0
  br i1 %20, label %51, label %21

21:                                               ; preds = %9, %21
  %22 = phi i64 [ %27, %21 ], [ 0, %9 ]
  %23 = getelementptr inbounds [4 x i64], [4 x i64]* %19, i64 %22, i64 0
  %24 = getelementptr inbounds [1 x %1], [1 x %1]* %4, i64 %22, i64 0
  %25 = bitcast i64* %23 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 8 %25, i8 0, i64 32, i1 false) #7
  %26 = tail call i8* @__gmpz_export(i8* %25, i64* null, i32 -1, i64 8, i32 0, i64 0, %1* %24) #7
  %27 = add nuw i64 %22, 1
  %28 = icmp eq i64 %27, %8
  br i1 %28, label %29, label %21

29:                                               ; preds = %21
  %30 = tail call noalias i8* @malloc(i64 %17) #7
  %31 = bitcast i8* %30 to [4 x i64]*
  br label %32

32:                                               ; preds = %29, %32
  %33 = phi i64 [ %38, %32 ], [ 0, %29 ]
  %34 = getelementptr inbounds [4 x i64], [4 x i64]* %31, i64 %33, i64 0
  %35 = getelementptr inbounds [1 x %1], [1 x %1]* %5, i64 %33, i64 0
  %36 = bitcast i64* %34 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 8 %36, i8 0, i64 32, i1 false) #7
  %37 = tail call i8* @__gmpz_export(i8* %36, i64* null, i32 -1, i64 8, i32 0, i64 0, %1* %35) #7
  %38 = add nuw i64 %33, 1
  %39 = icmp eq i64 %38, %8
  br i1 %39, label %40, label %32

40:                                               ; preds = %32
  %41 = tail call noalias i8* @malloc(i64 %17) #7
  %42 = bitcast i8* %41 to [4 x i64]*
  br label %43

43:                                               ; preds = %40, %43
  %44 = phi i64 [ %49, %43 ], [ 0, %40 ]
  %45 = getelementptr inbounds [4 x i64], [4 x i64]* %42, i64 %44, i64 0
  %46 = getelementptr inbounds [1 x %1], [1 x %1]* %6, i64 %44, i64 0
  %47 = bitcast i64* %45 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 8 %47, i8 0, i64 32, i1 false) #7
  %48 = tail call i8* @__gmpz_export(i8* %47, i64* null, i32 -1, i64 8, i32 0, i64 0, %1* %46) #7
  %49 = add nuw i64 %44, 1
  %50 = icmp eq i64 %49, %8
  br i1 %50, label %59, label %43

51:                                               ; preds = %9
  %52 = tail call noalias i8* @malloc(i64 %17) #7
  %53 = bitcast i8* %52 to [4 x i64]*
  %54 = tail call noalias i8* @malloc(i64 %17) #7
  %55 = bitcast i8* %54 to [4 x i64]*
  %56 = getelementptr inbounds [4 x i128], [4 x i128]* %11, i64 0, i64 0
  %57 = getelementptr inbounds [4 x i128], [4 x i128]* %12, i64 0, i64 0
  %58 = getelementptr inbounds [4 x i128], [4 x i128]* %13, i64 0, i64 0
  br label %72

59:                                               ; preds = %43
  %60 = getelementptr inbounds [4 x i128], [4 x i128]* %11, i64 0, i64 0
  %61 = getelementptr inbounds [4 x i128], [4 x i128]* %12, i64 0, i64 0
  %62 = getelementptr inbounds [4 x i128], [4 x i128]* %13, i64 0, i64 0
  br label %63

63:                                               ; preds = %59, %63
  %64 = phi i64 [ %70, %63 ], [ 0, %59 ]
  %65 = phi i64 [ %69, %63 ], [ 0, %59 ]
  %66 = getelementptr inbounds [1 x %1], [1 x %1]* %7, i64 %64, i64 0
  %67 = tail call i64 @__gmpz_sizeinbase(%1* %66, i32 2) #6
  %68 = icmp ugt i64 %67, %65
  %69 = select i1 %68, i64 %67, i64 %65
  %70 = add nuw i64 %64, 1
  %71 = icmp eq i64 %70, %8
  br i1 %71, label %72, label %63

72:                                               ; preds = %63, %51
  %73 = phi i128* [ %58, %51 ], [ %62, %63 ]
  %74 = phi i128* [ %57, %51 ], [ %61, %63 ]
  %75 = phi i128* [ %56, %51 ], [ %60, %63 ]
  %76 = phi [4 x i64]* [ %53, %51 ], [ %31, %63 ]
  %77 = phi i8* [ %52, %51 ], [ %30, %63 ]
  %78 = phi i8* [ %54, %51 ], [ %41, %63 ]
  %79 = phi [4 x i64]* [ %55, %51 ], [ %42, %63 ]
  %80 = phi i64 [ 0, %51 ], [ %69, %63 ]
  %81 = trunc i64 %80 to i32
  %82 = tail call i32 @vec_smul_block_width(i32 %81, i32 100) #7
  %83 = sext i32 %82 to i64
  call void @vec_jsmul_block_batch_nistp256_inner(i128* %75, i128* %74, i128* %73, %0* undef, [4 x i64]* %19, [4 x i64]* %76, [4 x i64]* %79, [1 x %1]* %7, i64 %8, i64 %83, i64 100, i64 %80) #7
  %84 = bitcast [4 x i64]* %10 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %84) #7
  %85 = getelementptr inbounds [4 x i64], [4 x i64]* %10, i64 0, i64 0
  call fastcc void @2(i64* nonnull %85, i128* %75) #7
  %86 = load i64, i64* %85, align 16
  %87 = getelementptr inbounds [4 x i64], [4 x i64]* %10, i64 0, i64 1
  %88 = load i64, i64* %87, align 8
  %89 = or i64 %88, %86
  %90 = getelementptr inbounds [4 x i64], [4 x i64]* %10, i64 0, i64 2
  %91 = load i64, i64* %90, align 16
  %92 = or i64 %89, %91
  %93 = getelementptr inbounds [4 x i64], [4 x i64]* %10, i64 0, i64 3
  %94 = load i64, i64* %93, align 8
  %95 = or i64 %92, %94
  %96 = add i64 %95, -1
  %97 = xor i64 %86, -1
  %98 = xor i64 %88, 4294967295
  %99 = or i64 %98, %97
  %100 = or i64 %99, %91
  %101 = xor i64 %94, -4294967295
  %102 = or i64 %100, %101
  %103 = add i64 %102, -1
  %104 = insertelement <2 x i64> undef, i64 %103, i32 0
  %105 = insertelement <2 x i64> %104, i64 %96, i32 1
  %106 = shl <2 x i64> %105, <i64 32, i64 32>
  %107 = and <2 x i64> %106, %105
  %108 = shl <2 x i64> %107, <i64 16, i64 16>
  %109 = and <2 x i64> %108, %107
  %110 = shl <2 x i64> %109, <i64 8, i64 8>
  %111 = and <2 x i64> %110, %109
  %112 = shl <2 x i64> %111, <i64 4, i64 4>
  %113 = and <2 x i64> %112, %111
  %114 = shl <2 x i64> %113, <i64 2, i64 2>
  %115 = and <2 x i64> %114, %113
  %116 = shl <2 x i64> %115, <i64 1, i64 1>
  %117 = and <2 x i64> %116, %115
  %118 = extractelement <2 x i64> %117, i32 0
  %119 = extractelement <2 x i64> %117, i32 1
  %120 = or i64 %118, %119
  %121 = ashr i64 %120, 63
  %122 = zext i64 %121 to i128
  %123 = shl nuw i128 %122, 64
  %124 = or i128 %123, %122
  %125 = icmp eq i128 %124, 0
  br i1 %125, label %127, label %126

126:                                              ; preds = %72
  call void @__gmpz_set_si(%1* %0, i64 0) #7
  br label %128

127:                                              ; preds = %72
  call void @__gmpz_import(%1* %0, i64 4, i32 -1, i64 8, i32 0, i64 0, i8* nonnull %84) #7
  br label %128

128:                                              ; preds = %126, %127
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %84) #7
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %84) #7
  call fastcc void @2(i64* nonnull %85, i128* %74) #7
  %129 = load i64, i64* %85, align 16
  %130 = load i64, i64* %87, align 8
  %131 = or i64 %130, %129
  %132 = load i64, i64* %90, align 16
  %133 = or i64 %131, %132
  %134 = load i64, i64* %93, align 8
  %135 = or i64 %133, %134
  %136 = add i64 %135, -1
  %137 = xor i64 %129, -1
  %138 = xor i64 %130, 4294967295
  %139 = or i64 %138, %137
  %140 = or i64 %139, %132
  %141 = xor i64 %134, -4294967295
  %142 = or i64 %140, %141
  %143 = add i64 %142, -1
  %144 = insertelement <2 x i64> undef, i64 %143, i32 0
  %145 = insertelement <2 x i64> %144, i64 %136, i32 1
  %146 = shl <2 x i64> %145, <i64 32, i64 32>
  %147 = and <2 x i64> %146, %145
  %148 = shl <2 x i64> %147, <i64 16, i64 16>
  %149 = and <2 x i64> %148, %147
  %150 = shl <2 x i64> %149, <i64 8, i64 8>
  %151 = and <2 x i64> %150, %149
  %152 = shl <2 x i64> %151, <i64 4, i64 4>
  %153 = and <2 x i64> %152, %151
  %154 = shl <2 x i64> %153, <i64 2, i64 2>
  %155 = and <2 x i64> %154, %153
  %156 = shl <2 x i64> %155, <i64 1, i64 1>
  %157 = and <2 x i64> %156, %155
  %158 = extractelement <2 x i64> %157, i32 0
  %159 = extractelement <2 x i64> %157, i32 1
  %160 = or i64 %158, %159
  %161 = ashr i64 %160, 63
  %162 = zext i64 %161 to i128
  %163 = shl nuw i128 %162, 64
  %164 = or i128 %163, %162
  %165 = icmp eq i128 %164, 0
  br i1 %165, label %167, label %166

166:                                              ; preds = %128
  call void @__gmpz_set_si(%1* %1, i64 0) #7
  br label %168

167:                                              ; preds = %128
  call void @__gmpz_import(%1* %1, i64 4, i32 -1, i64 8, i32 0, i64 0, i8* nonnull %84) #7
  br label %168

168:                                              ; preds = %166, %167
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %84) #7
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %84) #7
  call fastcc void @2(i64* nonnull %85, i128* %73) #7
  %169 = load i64, i64* %85, align 16
  %170 = load i64, i64* %87, align 8
  %171 = or i64 %170, %169
  %172 = load i64, i64* %90, align 16
  %173 = or i64 %171, %172
  %174 = load i64, i64* %93, align 8
  %175 = or i64 %173, %174
  %176 = add i64 %175, -1
  %177 = xor i64 %169, -1
  %178 = xor i64 %170, 4294967295
  %179 = or i64 %178, %177
  %180 = or i64 %179, %172
  %181 = xor i64 %174, -4294967295
  %182 = or i64 %180, %181
  %183 = add i64 %182, -1
  %184 = insertelement <2 x i64> undef, i64 %183, i32 0
  %185 = insertelement <2 x i64> %184, i64 %176, i32 1
  %186 = shl <2 x i64> %185, <i64 32, i64 32>
  %187 = and <2 x i64> %186, %185
  %188 = shl <2 x i64> %187, <i64 16, i64 16>
  %189 = and <2 x i64> %188, %187
  %190 = shl <2 x i64> %189, <i64 8, i64 8>
  %191 = and <2 x i64> %190, %189
  %192 = shl <2 x i64> %191, <i64 4, i64 4>
  %193 = and <2 x i64> %192, %191
  %194 = shl <2 x i64> %193, <i64 2, i64 2>
  %195 = and <2 x i64> %194, %193
  %196 = shl <2 x i64> %195, <i64 1, i64 1>
  %197 = and <2 x i64> %196, %195
  %198 = extractelement <2 x i64> %197, i32 0
  %199 = extractelement <2 x i64> %197, i32 1
  %200 = or i64 %198, %199
  %201 = ashr i64 %200, 63
  %202 = zext i64 %201 to i128
  %203 = shl nuw i128 %202, 64
  %204 = or i128 %203, %202
  %205 = icmp eq i128 %204, 0
  br i1 %205, label %207, label %206

206:                                              ; preds = %168
  call void @__gmpz_set_si(%1* %2, i64 0) #7
  br label %208

207:                                              ; preds = %168
  call void @__gmpz_import(%1* %2, i64 4, i32 -1, i64 8, i32 0, i64 0, i8* nonnull %84) #7
  br label %208

208:                                              ; preds = %206, %207
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %84) #7
  call void @free(i8* %18) #7
  call void @free(i8* %77) #7
  call void @free(i8* %78) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %16) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %15) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %14) #7
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local noalias %3* @vec_jfmul_precomp_nistp256(%0* %0, %1* %1, %1* %2, %1* %3, i64 %4) local_unnamed_addr #0 {
  %6 = alloca [4 x i64], align 16
  %7 = alloca [4 x i64], align 16
  %8 = alloca [4 x i64], align 16
  %9 = bitcast [4 x i64]* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %9) #7
  %10 = bitcast [4 x i64]* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %10) #7
  %11 = bitcast [4 x i64]* %8 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %11) #7
  %12 = tail call noalias i8* @malloc(i64 64) #7
  %13 = bitcast i8* %12 to %5*
  %14 = bitcast i8* %12 to %3*
  %15 = getelementptr inbounds [4 x i64], [4 x i64]* %6, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %9, i8 0, i64 32, i1 false) #7
  %16 = call i8* @__gmpz_export(i8* nonnull %9, i64* null, i32 -1, i64 8, i32 0, i64 0, %1* %1) #7
  %17 = getelementptr inbounds [4 x i64], [4 x i64]* %7, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %10, i8 0, i64 32, i1 false) #7
  %18 = call i8* @__gmpz_export(i8* nonnull %10, i64* null, i32 -1, i64 8, i32 0, i64 0, %1* %2) #7
  %19 = getelementptr inbounds [4 x i64], [4 x i64]* %8, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %11, i8 0, i64 32, i1 false) #7
  %20 = call i8* @__gmpz_export(i8* nonnull %11, i64* null, i32 -1, i64 8, i32 0, i64 0, %1* %3) #7
  %21 = getelementptr inbounds %0, %0* %0, i64 0, i32 6, i64 0
  %22 = call i64 @__gmpz_sizeinbase(%1* nonnull %21, i32 2) #6
  %23 = trunc i64 %22 to i32
  %24 = trunc i64 %4 to i32
  %25 = call i32 @vec_fmul_block_width(i32 %23, i32 %24) #7
  %26 = bitcast i8* %12 to %4*
  %27 = sext i32 %25 to i64
  call void @vec_jsmul_init_nistp256_inner(%4* %26, %0* undef, i64 %27, i64 %27) #7
  %28 = add nsw i32 %25, -1
  %29 = add nsw i32 %28, %23
  %30 = sdiv i32 %29, %25
  %31 = sext i32 %30 to i64
  %32 = getelementptr inbounds i8, i8* %12, i64 56
  %33 = bitcast i8* %32 to i64*
  store i64 %31, i64* %33, align 8
  call void @vec_jfmul_prcmp_nistp256_inner(%0* undef, %5* %13, i64* nonnull %15, i64* nonnull %17, i64* nonnull %19)
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %11) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %10) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %9) #7
  ret %3* %14
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jfmul_nistp256(%1* %0, %1* %1, %1* %2, %0* nocapture readnone %3, %3* nocapture readonly %4, %1* %5) local_unnamed_addr #0 {
  %7 = alloca [4 x i64], align 16
  %8 = alloca [4 x i64], align 16
  %9 = alloca [4 x i64], align 16
  %10 = bitcast [4 x i64]* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %10) #7
  %11 = bitcast [4 x i64]* %8 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %11) #7
  %12 = bitcast [4 x i64]* %9 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %12) #7
  %13 = getelementptr inbounds [4 x i64], [4 x i64]* %7, i64 0, i64 0
  %14 = getelementptr inbounds [4 x i64], [4 x i64]* %8, i64 0, i64 0
  %15 = getelementptr inbounds [4 x i64], [4 x i64]* %9, i64 0, i64 0
  %16 = bitcast %3* %4 to %5*
  call void @vec_jfmul_cmp_nistp256_inner(i64* nonnull %13, i64* nonnull %14, i64* nonnull %15, %0* undef, %5* %16, %1* %5)
  %17 = load i64, i64* %13, align 16
  %18 = getelementptr inbounds [4 x i64], [4 x i64]* %7, i64 0, i64 1
  %19 = load i64, i64* %18, align 8
  %20 = or i64 %19, %17
  %21 = getelementptr inbounds [4 x i64], [4 x i64]* %7, i64 0, i64 2
  %22 = load i64, i64* %21, align 16
  %23 = or i64 %20, %22
  %24 = getelementptr inbounds [4 x i64], [4 x i64]* %7, i64 0, i64 3
  %25 = load i64, i64* %24, align 8
  %26 = or i64 %23, %25
  %27 = add i64 %26, -1
  %28 = xor i64 %17, -1
  %29 = xor i64 %19, 4294967295
  %30 = or i64 %29, %28
  %31 = or i64 %30, %22
  %32 = xor i64 %25, -4294967295
  %33 = or i64 %31, %32
  %34 = add i64 %33, -1
  %35 = insertelement <2 x i64> undef, i64 %34, i32 0
  %36 = insertelement <2 x i64> %35, i64 %27, i32 1
  %37 = shl <2 x i64> %36, <i64 32, i64 32>
  %38 = and <2 x i64> %37, %36
  %39 = shl <2 x i64> %38, <i64 16, i64 16>
  %40 = and <2 x i64> %39, %38
  %41 = shl <2 x i64> %40, <i64 8, i64 8>
  %42 = and <2 x i64> %41, %40
  %43 = shl <2 x i64> %42, <i64 4, i64 4>
  %44 = and <2 x i64> %43, %42
  %45 = shl <2 x i64> %44, <i64 2, i64 2>
  %46 = and <2 x i64> %45, %44
  %47 = shl <2 x i64> %46, <i64 1, i64 1>
  %48 = and <2 x i64> %47, %46
  %49 = extractelement <2 x i64> %48, i32 0
  %50 = extractelement <2 x i64> %48, i32 1
  %51 = or i64 %49, %50
  %52 = ashr i64 %51, 63
  %53 = zext i64 %52 to i128
  %54 = shl nuw i128 %53, 64
  %55 = or i128 %54, %53
  %56 = icmp eq i128 %55, 0
  br i1 %56, label %58, label %57

57:                                               ; preds = %6
  tail call void @__gmpz_set_si(%1* %0, i64 0) #7
  br label %59

58:                                               ; preds = %6
  call void @__gmpz_import(%1* %0, i64 4, i32 -1, i64 8, i32 0, i64 0, i8* nonnull %10) #7
  br label %59

59:                                               ; preds = %57, %58
  %60 = load i64, i64* %14, align 16
  %61 = getelementptr inbounds [4 x i64], [4 x i64]* %8, i64 0, i64 1
  %62 = load i64, i64* %61, align 8
  %63 = or i64 %62, %60
  %64 = getelementptr inbounds [4 x i64], [4 x i64]* %8, i64 0, i64 2
  %65 = load i64, i64* %64, align 16
  %66 = or i64 %63, %65
  %67 = getelementptr inbounds [4 x i64], [4 x i64]* %8, i64 0, i64 3
  %68 = load i64, i64* %67, align 8
  %69 = or i64 %66, %68
  %70 = add i64 %69, -1
  %71 = xor i64 %60, -1
  %72 = xor i64 %62, 4294967295
  %73 = or i64 %72, %71
  %74 = or i64 %73, %65
  %75 = xor i64 %68, -4294967295
  %76 = or i64 %74, %75
  %77 = add i64 %76, -1
  %78 = insertelement <2 x i64> undef, i64 %77, i32 0
  %79 = insertelement <2 x i64> %78, i64 %70, i32 1
  %80 = shl <2 x i64> %79, <i64 32, i64 32>
  %81 = and <2 x i64> %80, %79
  %82 = shl <2 x i64> %81, <i64 16, i64 16>
  %83 = and <2 x i64> %82, %81
  %84 = shl <2 x i64> %83, <i64 8, i64 8>
  %85 = and <2 x i64> %84, %83
  %86 = shl <2 x i64> %85, <i64 4, i64 4>
  %87 = and <2 x i64> %86, %85
  %88 = shl <2 x i64> %87, <i64 2, i64 2>
  %89 = and <2 x i64> %88, %87
  %90 = shl <2 x i64> %89, <i64 1, i64 1>
  %91 = and <2 x i64> %90, %89
  %92 = extractelement <2 x i64> %91, i32 0
  %93 = extractelement <2 x i64> %91, i32 1
  %94 = or i64 %92, %93
  %95 = ashr i64 %94, 63
  %96 = zext i64 %95 to i128
  %97 = shl nuw i128 %96, 64
  %98 = or i128 %97, %96
  %99 = icmp eq i128 %98, 0
  br i1 %99, label %101, label %100

100:                                              ; preds = %59
  call void @__gmpz_set_si(%1* %1, i64 0) #7
  br label %102

101:                                              ; preds = %59
  call void @__gmpz_import(%1* %1, i64 4, i32 -1, i64 8, i32 0, i64 0, i8* nonnull %11) #7
  br label %102

102:                                              ; preds = %100, %101
  %103 = load i64, i64* %15, align 16
  %104 = getelementptr inbounds [4 x i64], [4 x i64]* %9, i64 0, i64 1
  %105 = load i64, i64* %104, align 8
  %106 = or i64 %105, %103
  %107 = getelementptr inbounds [4 x i64], [4 x i64]* %9, i64 0, i64 2
  %108 = load i64, i64* %107, align 16
  %109 = or i64 %106, %108
  %110 = getelementptr inbounds [4 x i64], [4 x i64]* %9, i64 0, i64 3
  %111 = load i64, i64* %110, align 8
  %112 = or i64 %109, %111
  %113 = add i64 %112, -1
  %114 = xor i64 %103, -1
  %115 = xor i64 %105, 4294967295
  %116 = or i64 %115, %114
  %117 = or i64 %116, %108
  %118 = xor i64 %111, -4294967295
  %119 = or i64 %117, %118
  %120 = add i64 %119, -1
  %121 = insertelement <2 x i64> undef, i64 %120, i32 0
  %122 = insertelement <2 x i64> %121, i64 %113, i32 1
  %123 = shl <2 x i64> %122, <i64 32, i64 32>
  %124 = and <2 x i64> %123, %122
  %125 = shl <2 x i64> %124, <i64 16, i64 16>
  %126 = and <2 x i64> %125, %124
  %127 = shl <2 x i64> %126, <i64 8, i64 8>
  %128 = and <2 x i64> %127, %126
  %129 = shl <2 x i64> %128, <i64 4, i64 4>
  %130 = and <2 x i64> %129, %128
  %131 = shl <2 x i64> %130, <i64 2, i64 2>
  %132 = and <2 x i64> %131, %130
  %133 = shl <2 x i64> %132, <i64 1, i64 1>
  %134 = and <2 x i64> %133, %132
  %135 = extractelement <2 x i64> %134, i32 0
  %136 = extractelement <2 x i64> %134, i32 1
  %137 = or i64 %135, %136
  %138 = ashr i64 %137, 63
  %139 = zext i64 %138 to i128
  %140 = shl nuw i128 %139, 64
  %141 = or i128 %140, %139
  %142 = icmp eq i128 %141, 0
  br i1 %142, label %144, label %143

143:                                              ; preds = %102
  call void @__gmpz_set_si(%1* %2, i64 0) #7
  br label %145

144:                                              ; preds = %102
  call void @__gmpz_import(%1* %2, i64 4, i32 -1, i64 8, i32 0, i64 0, i8* nonnull %12) #7
  br label %145

145:                                              ; preds = %143, %144
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %12) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %11) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %10) #7
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jfmul_free_nistp256(%3* nocapture %0) local_unnamed_addr #0 {
  %2 = bitcast %3* %0 to %4*
  %3 = tail call i32 @vec_jsmul_clear_nistp256_inner(%4* %2) #7
  %4 = bitcast %3* %0 to i8*
  tail call void @free(i8* %4) #7
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jdbl_nistp256(%2* nocapture readnone %0, %1* %1, %1* %2, %1* %3, %0* nocapture readnone %4, %1* %5, %1* %6, %1* %7) local_unnamed_addr #0 {
  %9 = alloca [4 x i64], align 16
  %10 = alloca [4 x i128], align 16
  %11 = alloca [4 x i128], align 16
  %12 = alloca [4 x i128], align 16
  %13 = alloca [4 x i128], align 16
  %14 = alloca [4 x i128], align 16
  %15 = alloca [4 x i128], align 16
  %16 = bitcast [4 x i128]* %10 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %16) #7
  %17 = bitcast [4 x i128]* %11 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %17) #7
  %18 = bitcast [4 x i128]* %12 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %18) #7
  %19 = bitcast [4 x i128]* %13 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %19) #7
  %20 = bitcast [4 x i128]* %14 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %20) #7
  %21 = bitcast [4 x i128]* %15 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %21) #7
  %22 = getelementptr inbounds [4 x i128], [4 x i128]* %10, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %16, i8 0, i64 64, i1 false) #7
  %23 = call i8* @__gmpz_export(i8* nonnull %16, i64* null, i32 -1, i64 16, i32 0, i64 64, %1* %5) #7
  %24 = getelementptr inbounds [4 x i128], [4 x i128]* %11, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %17, i8 0, i64 64, i1 false) #7
  %25 = call i8* @__gmpz_export(i8* nonnull %17, i64* null, i32 -1, i64 16, i32 0, i64 64, %1* %6) #7
  %26 = getelementptr inbounds [4 x i128], [4 x i128]* %12, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %18, i8 0, i64 64, i1 false) #7
  %27 = call i8* @__gmpz_export(i8* nonnull %18, i64* null, i32 -1, i64 16, i32 0, i64 64, %1* %7) #7
  %28 = getelementptr inbounds [4 x i128], [4 x i128]* %13, i64 0, i64 0
  %29 = getelementptr inbounds [4 x i128], [4 x i128]* %14, i64 0, i64 0
  %30 = getelementptr inbounds [4 x i128], [4 x i128]* %15, i64 0, i64 0
  call fastcc void @0(i128* nonnull %28, i128* nonnull %29, i128* nonnull %30, i128* nonnull %22, i128* nonnull %24, i128* nonnull %26)
  %31 = bitcast [4 x i64]* %9 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %31) #7
  %32 = getelementptr inbounds [4 x i64], [4 x i64]* %9, i64 0, i64 0
  call fastcc void @2(i64* nonnull %32, i128* nonnull %28) #7
  %33 = load i64, i64* %32, align 16
  %34 = getelementptr inbounds [4 x i64], [4 x i64]* %9, i64 0, i64 1
  %35 = load i64, i64* %34, align 8
  %36 = or i64 %35, %33
  %37 = getelementptr inbounds [4 x i64], [4 x i64]* %9, i64 0, i64 2
  %38 = load i64, i64* %37, align 16
  %39 = or i64 %36, %38
  %40 = getelementptr inbounds [4 x i64], [4 x i64]* %9, i64 0, i64 3
  %41 = load i64, i64* %40, align 8
  %42 = or i64 %39, %41
  %43 = add i64 %42, -1
  %44 = xor i64 %33, -1
  %45 = xor i64 %35, 4294967295
  %46 = or i64 %45, %44
  %47 = or i64 %46, %38
  %48 = xor i64 %41, -4294967295
  %49 = or i64 %47, %48
  %50 = add i64 %49, -1
  %51 = insertelement <2 x i64> undef, i64 %50, i32 0
  %52 = insertelement <2 x i64> %51, i64 %43, i32 1
  %53 = shl <2 x i64> %52, <i64 32, i64 32>
  %54 = and <2 x i64> %53, %52
  %55 = shl <2 x i64> %54, <i64 16, i64 16>
  %56 = and <2 x i64> %55, %54
  %57 = shl <2 x i64> %56, <i64 8, i64 8>
  %58 = and <2 x i64> %57, %56
  %59 = shl <2 x i64> %58, <i64 4, i64 4>
  %60 = and <2 x i64> %59, %58
  %61 = shl <2 x i64> %60, <i64 2, i64 2>
  %62 = and <2 x i64> %61, %60
  %63 = shl <2 x i64> %62, <i64 1, i64 1>
  %64 = and <2 x i64> %63, %62
  %65 = extractelement <2 x i64> %64, i32 0
  %66 = extractelement <2 x i64> %64, i32 1
  %67 = or i64 %65, %66
  %68 = ashr i64 %67, 63
  %69 = zext i64 %68 to i128
  %70 = shl nuw i128 %69, 64
  %71 = or i128 %70, %69
  %72 = icmp eq i128 %71, 0
  br i1 %72, label %74, label %73

73:                                               ; preds = %8
  call void @__gmpz_set_si(%1* %1, i64 0) #7
  br label %75

74:                                               ; preds = %8
  call void @__gmpz_import(%1* %1, i64 4, i32 -1, i64 8, i32 0, i64 0, i8* nonnull %31) #7
  br label %75

75:                                               ; preds = %73, %74
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %31) #7
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %31) #7
  call fastcc void @2(i64* nonnull %32, i128* nonnull %29) #7
  %76 = load i64, i64* %32, align 16
  %77 = load i64, i64* %34, align 8
  %78 = or i64 %77, %76
  %79 = load i64, i64* %37, align 16
  %80 = or i64 %78, %79
  %81 = load i64, i64* %40, align 8
  %82 = or i64 %80, %81
  %83 = add i64 %82, -1
  %84 = xor i64 %76, -1
  %85 = xor i64 %77, 4294967295
  %86 = or i64 %85, %84
  %87 = or i64 %86, %79
  %88 = xor i64 %81, -4294967295
  %89 = or i64 %87, %88
  %90 = add i64 %89, -1
  %91 = insertelement <2 x i64> undef, i64 %90, i32 0
  %92 = insertelement <2 x i64> %91, i64 %83, i32 1
  %93 = shl <2 x i64> %92, <i64 32, i64 32>
  %94 = and <2 x i64> %93, %92
  %95 = shl <2 x i64> %94, <i64 16, i64 16>
  %96 = and <2 x i64> %95, %94
  %97 = shl <2 x i64> %96, <i64 8, i64 8>
  %98 = and <2 x i64> %97, %96
  %99 = shl <2 x i64> %98, <i64 4, i64 4>
  %100 = and <2 x i64> %99, %98
  %101 = shl <2 x i64> %100, <i64 2, i64 2>
  %102 = and <2 x i64> %101, %100
  %103 = shl <2 x i64> %102, <i64 1, i64 1>
  %104 = and <2 x i64> %103, %102
  %105 = extractelement <2 x i64> %104, i32 0
  %106 = extractelement <2 x i64> %104, i32 1
  %107 = or i64 %105, %106
  %108 = ashr i64 %107, 63
  %109 = zext i64 %108 to i128
  %110 = shl nuw i128 %109, 64
  %111 = or i128 %110, %109
  %112 = icmp eq i128 %111, 0
  br i1 %112, label %114, label %113

113:                                              ; preds = %75
  call void @__gmpz_set_si(%1* %2, i64 0) #7
  br label %115

114:                                              ; preds = %75
  call void @__gmpz_import(%1* %2, i64 4, i32 -1, i64 8, i32 0, i64 0, i8* nonnull %31) #7
  br label %115

115:                                              ; preds = %113, %114
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %31) #7
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %31) #7
  call fastcc void @2(i64* nonnull %32, i128* nonnull %30) #7
  %116 = load i64, i64* %32, align 16
  %117 = load i64, i64* %34, align 8
  %118 = or i64 %117, %116
  %119 = load i64, i64* %37, align 16
  %120 = or i64 %118, %119
  %121 = load i64, i64* %40, align 8
  %122 = or i64 %120, %121
  %123 = add i64 %122, -1
  %124 = xor i64 %116, -1
  %125 = xor i64 %117, 4294967295
  %126 = or i64 %125, %124
  %127 = or i64 %126, %119
  %128 = xor i64 %121, -4294967295
  %129 = or i64 %127, %128
  %130 = add i64 %129, -1
  %131 = insertelement <2 x i64> undef, i64 %130, i32 0
  %132 = insertelement <2 x i64> %131, i64 %123, i32 1
  %133 = shl <2 x i64> %132, <i64 32, i64 32>
  %134 = and <2 x i64> %133, %132
  %135 = shl <2 x i64> %134, <i64 16, i64 16>
  %136 = and <2 x i64> %135, %134
  %137 = shl <2 x i64> %136, <i64 8, i64 8>
  %138 = and <2 x i64> %137, %136
  %139 = shl <2 x i64> %138, <i64 4, i64 4>
  %140 = and <2 x i64> %139, %138
  %141 = shl <2 x i64> %140, <i64 2, i64 2>
  %142 = and <2 x i64> %141, %140
  %143 = shl <2 x i64> %142, <i64 1, i64 1>
  %144 = and <2 x i64> %143, %142
  %145 = extractelement <2 x i64> %144, i32 0
  %146 = extractelement <2 x i64> %144, i32 1
  %147 = or i64 %145, %146
  %148 = ashr i64 %147, 63
  %149 = zext i64 %148 to i128
  %150 = shl nuw i128 %149, 64
  %151 = or i128 %150, %149
  %152 = icmp eq i128 %151, 0
  br i1 %152, label %154, label %153

153:                                              ; preds = %115
  call void @__gmpz_set_si(%1* %3, i64 0) #7
  br label %155

154:                                              ; preds = %115
  call void @__gmpz_import(%1* %3, i64 4, i32 -1, i64 8, i32 0, i64 0, i8* nonnull %31) #7
  br label %155

155:                                              ; preds = %153, %154
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %31) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %21) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %20) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %19) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %18) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %17) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %16) #7
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jadd_nistp256(%2* nocapture readnone %0, %1* %1, %1* %2, %1* %3, %0* nocapture readnone %4, %1* %5, %1* %6, %1* %7, %1* %8, %1* %9, %1* %10) local_unnamed_addr #0 {
  %12 = alloca [4 x i64], align 16
  %13 = alloca [4 x i128], align 16
  %14 = alloca [4 x i128], align 16
  %15 = alloca [4 x i128], align 16
  %16 = alloca [4 x i64], align 16
  %17 = alloca [4 x i64], align 16
  %18 = alloca [4 x i64], align 16
  %19 = alloca [4 x i128], align 16
  %20 = alloca [4 x i128], align 16
  %21 = alloca [4 x i128], align 16
  %22 = bitcast [4 x i128]* %13 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %22) #7
  %23 = bitcast [4 x i128]* %14 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %23) #7
  %24 = bitcast [4 x i128]* %15 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %24) #7
  %25 = bitcast [4 x i64]* %16 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %25) #7
  %26 = bitcast [4 x i64]* %17 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %26) #7
  %27 = bitcast [4 x i64]* %18 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %27) #7
  %28 = bitcast [4 x i128]* %19 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %28) #7
  %29 = bitcast [4 x i128]* %20 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %29) #7
  %30 = bitcast [4 x i128]* %21 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %30) #7
  %31 = getelementptr inbounds [4 x i128], [4 x i128]* %13, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %22, i8 0, i64 64, i1 false) #7
  %32 = call i8* @__gmpz_export(i8* nonnull %22, i64* null, i32 -1, i64 16, i32 0, i64 64, %1* %5) #7
  %33 = getelementptr inbounds [4 x i128], [4 x i128]* %14, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %23, i8 0, i64 64, i1 false) #7
  %34 = call i8* @__gmpz_export(i8* nonnull %23, i64* null, i32 -1, i64 16, i32 0, i64 64, %1* %6) #7
  %35 = getelementptr inbounds [4 x i128], [4 x i128]* %15, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %24, i8 0, i64 64, i1 false) #7
  %36 = call i8* @__gmpz_export(i8* nonnull %24, i64* null, i32 -1, i64 16, i32 0, i64 64, %1* %7) #7
  %37 = getelementptr inbounds [4 x i64], [4 x i64]* %16, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %25, i8 0, i64 32, i1 false) #7
  %38 = call i8* @__gmpz_export(i8* nonnull %25, i64* null, i32 -1, i64 8, i32 0, i64 0, %1* %8) #7
  %39 = getelementptr inbounds [4 x i64], [4 x i64]* %17, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %26, i8 0, i64 32, i1 false) #7
  %40 = call i8* @__gmpz_export(i8* nonnull %26, i64* null, i32 -1, i64 8, i32 0, i64 0, %1* %9) #7
  %41 = getelementptr inbounds [4 x i64], [4 x i64]* %18, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %27, i8 0, i64 32, i1 false) #7
  %42 = call i8* @__gmpz_export(i8* nonnull %27, i64* null, i32 -1, i64 8, i32 0, i64 0, %1* %10) #7
  %43 = getelementptr inbounds [4 x i128], [4 x i128]* %19, i64 0, i64 0
  %44 = getelementptr inbounds [4 x i128], [4 x i128]* %20, i64 0, i64 0
  %45 = getelementptr inbounds [4 x i128], [4 x i128]* %21, i64 0, i64 0
  call fastcc void @1(i128* nonnull %43, i128* nonnull %44, i128* nonnull %45, i128* nonnull %31, i128* nonnull %33, i128* nonnull %35, i64* nonnull %37, i64* nonnull %39, i64* nonnull %41)
  %46 = bitcast [4 x i64]* %12 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %46) #7
  %47 = getelementptr inbounds [4 x i64], [4 x i64]* %12, i64 0, i64 0
  call fastcc void @2(i64* nonnull %47, i128* nonnull %43) #7
  %48 = load i64, i64* %47, align 16
  %49 = getelementptr inbounds [4 x i64], [4 x i64]* %12, i64 0, i64 1
  %50 = load i64, i64* %49, align 8
  %51 = or i64 %50, %48
  %52 = getelementptr inbounds [4 x i64], [4 x i64]* %12, i64 0, i64 2
  %53 = load i64, i64* %52, align 16
  %54 = or i64 %51, %53
  %55 = getelementptr inbounds [4 x i64], [4 x i64]* %12, i64 0, i64 3
  %56 = load i64, i64* %55, align 8
  %57 = or i64 %54, %56
  %58 = add i64 %57, -1
  %59 = xor i64 %48, -1
  %60 = xor i64 %50, 4294967295
  %61 = or i64 %60, %59
  %62 = or i64 %61, %53
  %63 = xor i64 %56, -4294967295
  %64 = or i64 %62, %63
  %65 = add i64 %64, -1
  %66 = insertelement <2 x i64> undef, i64 %65, i32 0
  %67 = insertelement <2 x i64> %66, i64 %58, i32 1
  %68 = shl <2 x i64> %67, <i64 32, i64 32>
  %69 = and <2 x i64> %68, %67
  %70 = shl <2 x i64> %69, <i64 16, i64 16>
  %71 = and <2 x i64> %70, %69
  %72 = shl <2 x i64> %71, <i64 8, i64 8>
  %73 = and <2 x i64> %72, %71
  %74 = shl <2 x i64> %73, <i64 4, i64 4>
  %75 = and <2 x i64> %74, %73
  %76 = shl <2 x i64> %75, <i64 2, i64 2>
  %77 = and <2 x i64> %76, %75
  %78 = shl <2 x i64> %77, <i64 1, i64 1>
  %79 = and <2 x i64> %78, %77
  %80 = extractelement <2 x i64> %79, i32 0
  %81 = extractelement <2 x i64> %79, i32 1
  %82 = or i64 %80, %81
  %83 = ashr i64 %82, 63
  %84 = zext i64 %83 to i128
  %85 = shl nuw i128 %84, 64
  %86 = or i128 %85, %84
  %87 = icmp eq i128 %86, 0
  br i1 %87, label %89, label %88

88:                                               ; preds = %11
  call void @__gmpz_set_si(%1* %1, i64 0) #7
  br label %90

89:                                               ; preds = %11
  call void @__gmpz_import(%1* %1, i64 4, i32 -1, i64 8, i32 0, i64 0, i8* nonnull %46) #7
  br label %90

90:                                               ; preds = %88, %89
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %46) #7
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %46) #7
  call fastcc void @2(i64* nonnull %47, i128* nonnull %44) #7
  %91 = load i64, i64* %47, align 16
  %92 = load i64, i64* %49, align 8
  %93 = or i64 %92, %91
  %94 = load i64, i64* %52, align 16
  %95 = or i64 %93, %94
  %96 = load i64, i64* %55, align 8
  %97 = or i64 %95, %96
  %98 = add i64 %97, -1
  %99 = xor i64 %91, -1
  %100 = xor i64 %92, 4294967295
  %101 = or i64 %100, %99
  %102 = or i64 %101, %94
  %103 = xor i64 %96, -4294967295
  %104 = or i64 %102, %103
  %105 = add i64 %104, -1
  %106 = insertelement <2 x i64> undef, i64 %105, i32 0
  %107 = insertelement <2 x i64> %106, i64 %98, i32 1
  %108 = shl <2 x i64> %107, <i64 32, i64 32>
  %109 = and <2 x i64> %108, %107
  %110 = shl <2 x i64> %109, <i64 16, i64 16>
  %111 = and <2 x i64> %110, %109
  %112 = shl <2 x i64> %111, <i64 8, i64 8>
  %113 = and <2 x i64> %112, %111
  %114 = shl <2 x i64> %113, <i64 4, i64 4>
  %115 = and <2 x i64> %114, %113
  %116 = shl <2 x i64> %115, <i64 2, i64 2>
  %117 = and <2 x i64> %116, %115
  %118 = shl <2 x i64> %117, <i64 1, i64 1>
  %119 = and <2 x i64> %118, %117
  %120 = extractelement <2 x i64> %119, i32 0
  %121 = extractelement <2 x i64> %119, i32 1
  %122 = or i64 %120, %121
  %123 = ashr i64 %122, 63
  %124 = zext i64 %123 to i128
  %125 = shl nuw i128 %124, 64
  %126 = or i128 %125, %124
  %127 = icmp eq i128 %126, 0
  br i1 %127, label %129, label %128

128:                                              ; preds = %90
  call void @__gmpz_set_si(%1* %2, i64 0) #7
  br label %130

129:                                              ; preds = %90
  call void @__gmpz_import(%1* %2, i64 4, i32 -1, i64 8, i32 0, i64 0, i8* nonnull %46) #7
  br label %130

130:                                              ; preds = %128, %129
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %46) #7
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %46) #7
  call fastcc void @2(i64* nonnull %47, i128* nonnull %45) #7
  %131 = load i64, i64* %47, align 16
  %132 = load i64, i64* %49, align 8
  %133 = or i64 %132, %131
  %134 = load i64, i64* %52, align 16
  %135 = or i64 %133, %134
  %136 = load i64, i64* %55, align 8
  %137 = or i64 %135, %136
  %138 = add i64 %137, -1
  %139 = xor i64 %131, -1
  %140 = xor i64 %132, 4294967295
  %141 = or i64 %140, %139
  %142 = or i64 %141, %134
  %143 = xor i64 %136, -4294967295
  %144 = or i64 %142, %143
  %145 = add i64 %144, -1
  %146 = insertelement <2 x i64> undef, i64 %145, i32 0
  %147 = insertelement <2 x i64> %146, i64 %138, i32 1
  %148 = shl <2 x i64> %147, <i64 32, i64 32>
  %149 = and <2 x i64> %148, %147
  %150 = shl <2 x i64> %149, <i64 16, i64 16>
  %151 = and <2 x i64> %150, %149
  %152 = shl <2 x i64> %151, <i64 8, i64 8>
  %153 = and <2 x i64> %152, %151
  %154 = shl <2 x i64> %153, <i64 4, i64 4>
  %155 = and <2 x i64> %154, %153
  %156 = shl <2 x i64> %155, <i64 2, i64 2>
  %157 = and <2 x i64> %156, %155
  %158 = shl <2 x i64> %157, <i64 1, i64 1>
  %159 = and <2 x i64> %158, %157
  %160 = extractelement <2 x i64> %159, i32 0
  %161 = extractelement <2 x i64> %159, i32 1
  %162 = or i64 %160, %161
  %163 = ashr i64 %162, 63
  %164 = zext i64 %163 to i128
  %165 = shl nuw i128 %164, 64
  %166 = or i128 %165, %164
  %167 = icmp eq i128 %166, 0
  br i1 %167, label %169, label %168

168:                                              ; preds = %130
  call void @__gmpz_set_si(%1* %3, i64 0) #7
  br label %170

169:                                              ; preds = %130
  call void @__gmpz_import(%1* %3, i64 4, i32 -1, i64 8, i32 0, i64 0, i8* nonnull %46) #7
  br label %170

170:                                              ; preds = %168, %169
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %46) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %30) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %29) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %28) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %27) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %26) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %25) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %24) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %23) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %22) #7
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @time_jdbl_nistp256(i32 %0, %1* %1, %1* %2) local_unnamed_addr #0 {
  %4 = alloca [4 x i128], align 16
  %5 = alloca [4 x i128], align 16
  %6 = alloca [4 x i128], align 16
  %7 = alloca [1 x %1], align 16
  %8 = bitcast [4 x i128]* %4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %8) #7
  %9 = bitcast [4 x i128]* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %9) #7
  %10 = bitcast [4 x i128]* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %10) #7
  %11 = bitcast [1 x %1]* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 16, i8* nonnull %11) #7
  %12 = getelementptr inbounds [4 x i128], [4 x i128]* %4, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %8, i8 0, i64 64, i1 false) #7
  %13 = call i8* @__gmpz_export(i8* nonnull %8, i64* null, i32 -1, i64 16, i32 0, i64 64, %1* %1) #7
  %14 = getelementptr inbounds [4 x i128], [4 x i128]* %5, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %9, i8 0, i64 64, i1 false) #7
  %15 = call i8* @__gmpz_export(i8* nonnull %9, i64* null, i32 -1, i64 16, i32 0, i64 64, %1* %2) #7
  %16 = getelementptr inbounds [1 x %1], [1 x %1]* %7, i64 0, i64 0
  call void @__gmpz_init(%1* nonnull %16) #7
  call void @__gmpz_set_ui(%1* nonnull %16, i64 1) #7
  %17 = getelementptr inbounds [4 x i128], [4 x i128]* %6, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %10, i8 0, i64 64, i1 false) #7
  %18 = call i8* @__gmpz_export(i8* nonnull %10, i64* null, i32 -1, i64 16, i32 0, i64 64, %1* nonnull %16) #7
  call void @__gmpz_clear(%1* nonnull %16) #7
  %19 = call i64 @clock() #7
  %20 = shl i64 %19, 32
  %21 = ashr exact i64 %20, 32
  %22 = sext i32 %0 to i64
  br label %23

23:                                               ; preds = %23, %3
  %24 = phi i64 [ 0, %3 ], [ %25, %23 ]
  call fastcc void @0(i128* nonnull %12, i128* nonnull %14, i128* nonnull %17, i128* nonnull %12, i128* nonnull %14, i128* nonnull %17)
  %25 = add nuw nsw i64 %24, 1
  %26 = call i32 @vec_done(i64 %21, i64 %22) #7
  %27 = icmp eq i32 %26, 0
  br i1 %27, label %23, label %28

28:                                               ; preds = %23
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %11) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %10) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %9) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %8) #7
  ret i64 %25
}

declare dso_local void @__gmpz_set_ui(%1*, i64) local_unnamed_addr #5

; Function Attrs: nounwind
declare dso_local i64 @clock() local_unnamed_addr #4

declare dso_local i32 @vec_done(i64, i64) local_unnamed_addr #5

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @time_jadd_nistp256(i32 %0, %1* %1, %1* %2) local_unnamed_addr #0 {
  %4 = alloca [4 x i128], align 16
  %5 = alloca [4 x i128], align 16
  %6 = alloca [4 x i128], align 16
  %7 = alloca [4 x i64], align 16
  %8 = alloca [4 x i64], align 16
  %9 = alloca [4 x i64], align 16
  %10 = alloca [1 x %1], align 16
  %11 = bitcast [4 x i128]* %4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %11) #7
  %12 = bitcast [4 x i128]* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %12) #7
  %13 = bitcast [4 x i128]* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %13) #7
  %14 = bitcast [4 x i64]* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %14) #7
  %15 = bitcast [4 x i64]* %8 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %15) #7
  %16 = bitcast [4 x i64]* %9 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %16) #7
  %17 = bitcast [1 x %1]* %10 to i8*
  call void @llvm.lifetime.start.p0i8(i64 16, i8* nonnull %17) #7
  %18 = getelementptr inbounds [4 x i64], [4 x i64]* %7, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %14, i8 0, i64 32, i1 false) #7
  %19 = call i8* @__gmpz_export(i8* nonnull %14, i64* null, i32 -1, i64 8, i32 0, i64 0, %1* %1) #7
  %20 = getelementptr inbounds [4 x i64], [4 x i64]* %8, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %15, i8 0, i64 32, i1 false) #7
  %21 = call i8* @__gmpz_export(i8* nonnull %15, i64* null, i32 -1, i64 8, i32 0, i64 0, %1* %2) #7
  %22 = getelementptr inbounds [1 x %1], [1 x %1]* %10, i64 0, i64 0
  call void @__gmpz_init(%1* nonnull %22) #7
  call void @__gmpz_set_ui(%1* nonnull %22, i64 1) #7
  %23 = getelementptr inbounds [4 x i64], [4 x i64]* %9, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %16, i8 0, i64 32, i1 false) #7
  %24 = call i8* @__gmpz_export(i8* nonnull %16, i64* null, i32 -1, i64 8, i32 0, i64 0, %1* nonnull %22) #7
  call void @__gmpz_clear(%1* nonnull %22) #7
  %25 = getelementptr inbounds [4 x i128], [4 x i128]* %4, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %11, i8 0, i64 64, i1 false)
  %26 = getelementptr inbounds [4 x i128], [4 x i128]* %5, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %12, i8 0, i64 64, i1 false)
  %27 = getelementptr inbounds [4 x i128], [4 x i128]* %6, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %13, i8 0, i64 64, i1 false)
  %28 = call i64 @clock() #7
  %29 = shl i64 %28, 32
  %30 = ashr exact i64 %29, 32
  %31 = sext i32 %0 to i64
  br label %32

32:                                               ; preds = %32, %3
  %33 = phi i64 [ 0, %3 ], [ %34, %32 ]
  call fastcc void @1(i128* nonnull %25, i128* nonnull %26, i128* nonnull %27, i128* nonnull %25, i128* nonnull %26, i128* nonnull %27, i64* nonnull %18, i64* nonnull %20, i64* nonnull %23)
  %34 = add nuw nsw i64 %33, 1
  %35 = call i32 @vec_done(i64 %30, i64 %31) #7
  %36 = icmp eq i32 %35, 0
  br i1 %36, label %32, label %37

37:                                               ; preds = %32
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %17) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %16) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %15) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %14) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %13) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %12) #7
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %11) #7
  ret i64 %34
}

declare dso_local i8* @__gmpz_export(i8*, i64*, i32, i64, i32, i64, %1*) local_unnamed_addr #5

declare dso_local void @__gmpz_set_si(%1*, i64) local_unnamed_addr #5

declare dso_local void @__gmpz_import(%1*, i64, i32, i64, i32, i64, i8*) local_unnamed_addr #5

attributes #0 = { nounwind sspstrong uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "frame-pointer"="none" "less-precise-fpmad"="false" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+fxsr,+mmx,+sse,+sse2,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { argmemonly nounwind willreturn }
attributes #2 = { argmemonly nounwind willreturn writeonly }
attributes #3 = { nounwind readonly "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "frame-pointer"="none" "less-precise-fpmad"="false" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+fxsr,+mmx,+sse,+sse2,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #4 = { nounwind "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "frame-pointer"="none" "less-precise-fpmad"="false" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+fxsr,+mmx,+sse,+sse2,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #5 = { "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "frame-pointer"="none" "less-precise-fpmad"="false" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+fxsr,+mmx,+sse,+sse2,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #6 = { nounwind readonly }
attributes #7 = { nounwind }

!llvm.module.flags = !{!0, !1, !2}
!llvm.ident = !{!3}

!0 = !{i32 2, !"Dwarf Version", i32 4}
!1 = !{i32 2, !"Debug Info Version", i32 3}
!2 = !{i32 1, !"wchar_size", i32 4}
!3 = !{!"clang version 7.0.0 (tags/RELEASE_700/final)"}
