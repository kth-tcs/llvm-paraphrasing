; ModuleID = 'nistp224-strip-O3-renamed.bc'
source_filename = "nistp224.c"
target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

%0 = type { i8*, [1 x %1], [1 x %1], [1 x %1], [1 x %1], [1 x %1], [1 x %1], void (%2*, %1*, %1*, %1*, %0*, %1*, %1*, %1*)*, void (%2*, %1*, %1*, %1*, %0*, %1*, %1*, %1*, %1*, %1*, %1*)*, void (%1*, %1*, %1*, %0*, %1*, %1*, %1*, %1*)*, void (%1*, %1*, %1*, %0*, [1 x %1]*, [1 x %1]*, [1 x %1]*, [1 x %1]*, i64)*, %3* (%0*, %1*, %1*, %1*, i64)*, void (%1*, %1*, %1*, %0*, %3*, %1*)*, void (%3*)*, i64 (i32, %1*, %1*)*, i64 (i32, %1*, %1*)* }
%1 = type { i32, i32, i64* }
%2 = type { [1 x %1], [1 x %1], [1 x %1], [1 x %1], [1 x %1], [1 x %1], [1 x %1], [1 x %1], [1 x %1] }
%3 = type opaque
%4 = type { %0*, i64, i64, i64, [4 x i64]**, [4 x i64]**, [4 x i64]** }
%5 = type { [1 x %4], i64 }

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jmul_nistp224_inner(i64* nocapture %0, i64* nocapture %1, i64* %2, %0* nocapture readnone %3, i64* nocapture readonly %4, i64* nocapture readonly %5, i64* nocapture readonly %6, %1* %7) local_unnamed_addr #0 {
  %9 = bitcast i64* %0 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 8 %9, i8 0, i64 32, i1 false)
  %10 = bitcast i64* %1 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 8 %10, i8 0, i64 32, i1 false)
  %11 = bitcast i64* %2 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 8 %11, i8 0, i64 32, i1 false)
  %12 = tail call i64 @__gmpz_sizeinbase(%1* %7, i32 2) #6
  %13 = trunc i64 %12 to i32
  %14 = icmp sgt i32 %13, -1
  br i1 %14, label %15, label %26

15:                                               ; preds = %8
  %16 = shl i64 %12, 32
  %17 = ashr exact i64 %16, 32
  br label %18

18:                                               ; preds = %15, %23
  %19 = phi i64 [ %17, %15 ], [ %24, %23 ]
  tail call fastcc void @0(i64* %0, i64* %1, i64* %2, i64* %0, i64* %1, i64* %2)
  %20 = tail call i32 @__gmpz_tstbit(%1* %7, i64 %19) #6
  %21 = icmp eq i32 %20, 0
  br i1 %21, label %23, label %22

22:                                               ; preds = %18
  tail call fastcc void @1(i64* %0, i64* %1, i64* %2, i64* %0, i64* %1, i64* %2, i64* %4, i64* %5, i64* %6)
  br label %23

23:                                               ; preds = %18, %22
  %24 = add nsw i64 %19, -1
  %25 = icmp sgt i64 %19, 0
  br i1 %25, label %18, label %26

26:                                               ; preds = %23, %8
  ret void
}

; Function Attrs: argmemonly nounwind willreturn
declare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #1

; Function Attrs: argmemonly nounwind willreturn writeonly
declare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i1 immarg) #2

; Function Attrs: nounwind readonly
declare dso_local i64 @__gmpz_sizeinbase(%1*, i32) local_unnamed_addr #3

; Function Attrs: nounwind sspstrong uwtable
define internal fastcc void @0(i64* nocapture %0, i64* nocapture %1, i64* nocapture %2, i64* nocapture readonly %3, i64* nocapture readonly %4, i64* nocapture readonly %5) unnamed_addr #0 {
  %7 = load i64, i64* %3, align 8
  %8 = getelementptr inbounds i64, i64* %3, i64 1
  %9 = load i64, i64* %8, align 8
  %10 = getelementptr inbounds i64, i64* %3, i64 2
  %11 = load i64, i64* %10, align 8
  %12 = getelementptr inbounds i64, i64* %3, i64 3
  %13 = load i64, i64* %12, align 8
  %14 = load i64, i64* %5, align 8
  %15 = shl i64 %14, 1
  %16 = getelementptr inbounds i64, i64* %5, i64 1
  %17 = load i64, i64* %16, align 8
  %18 = shl i64 %17, 1
  %19 = getelementptr inbounds i64, i64* %5, i64 2
  %20 = load i64, i64* %19, align 8
  %21 = shl i64 %20, 1
  %22 = zext i64 %14 to i128
  %23 = mul nuw i128 %22, %22
  %24 = zext i64 %18 to i128
  %25 = mul nuw i128 %24, %22
  %26 = zext i64 %21 to i128
  %27 = mul nuw i128 %26, %22
  %28 = zext i64 %17 to i128
  %29 = mul nuw i128 %28, %28
  %30 = getelementptr inbounds i64, i64* %5, i64 3
  %31 = load i64, i64* %30, align 8
  %32 = zext i64 %31 to i128
  %33 = zext i64 %15 to i128
  %34 = mul nuw i128 %32, %33
  %35 = mul nuw i128 %26, %28
  %36 = add i128 %34, %35
  %37 = mul nuw i128 %32, %24
  %38 = zext i64 %20 to i128
  %39 = mul nuw i128 %38, %38
  %40 = add i128 %37, %39
  %41 = mul nuw i128 %32, %26
  %42 = mul nuw i128 %32, %32
  %43 = add i128 %23, -170141183460469231731687303715884072960
  %44 = add i128 %25, 170141183460469229370468033484042534912
  %45 = lshr i128 %42, 16
  %46 = add i128 %40, %45
  %47 = shl i128 %42, 40
  %48 = and i128 %47, 72056494526300160
  %49 = lshr i128 %41, 16
  %50 = shl i128 %41, 40
  %51 = and i128 %50, 72055395014672384
  %52 = sub i128 %44, %41
  %53 = lshr i128 %46, 16
  %54 = add i128 %29, 170141183460469229370504062281061498880
  %55 = add i128 %54, %27
  %56 = sub i128 %55, %42
  %57 = add i128 %56, %51
  %58 = add i128 %57, %53
  %59 = shl i128 %46, 40
  %60 = and i128 %59, 72056494526300160
  %61 = add i128 %52, %60
  %62 = sub i128 %43, %46
  %63 = lshr i128 %58, 56
  %64 = add i128 %36, %49
  %65 = add i128 %64, %48
  %66 = add i128 %65, %63
  %67 = and i128 %58, 72057594037927935
  %68 = lshr i128 %66, 56
  %69 = and i128 %66, 72057594037927935
  %70 = lshr i128 %66, 72
  %71 = add nuw nsw i128 %70, %67
  %72 = shl nuw nsw i128 %68, 40
  %73 = and i128 %72, 72056494526300160
  %74 = add i128 %61, %73
  %75 = sub i128 %62, %68
  %76 = lshr i128 %75, 56
  %77 = add i128 %74, %76
  %78 = trunc i128 %75 to i64
  %79 = and i64 %78, 72057594037927935
  %80 = lshr i128 %77, 56
  %81 = add nuw nsw i128 %71, %80
  %82 = trunc i128 %77 to i64
  %83 = and i64 %82, 72057594037927935
  %84 = lshr i128 %81, 56
  %85 = add nuw nsw i128 %84, %69
  %86 = trunc i128 %81 to i64
  %87 = and i64 %86, 72057594037927935
  %88 = trunc i128 %85 to i64
  %89 = load i64, i64* %4, align 8
  %90 = shl i64 %89, 1
  %91 = getelementptr inbounds i64, i64* %4, i64 1
  %92 = load i64, i64* %91, align 8
  %93 = shl i64 %92, 1
  %94 = getelementptr inbounds i64, i64* %4, i64 2
  %95 = load i64, i64* %94, align 8
  %96 = shl i64 %95, 1
  %97 = zext i64 %89 to i128
  %98 = mul nuw i128 %97, %97
  %99 = zext i64 %93 to i128
  %100 = mul nuw i128 %99, %97
  %101 = zext i64 %96 to i128
  %102 = mul nuw i128 %101, %97
  %103 = zext i64 %92 to i128
  %104 = mul nuw i128 %103, %103
  %105 = getelementptr inbounds i64, i64* %4, i64 3
  %106 = load i64, i64* %105, align 8
  %107 = zext i64 %106 to i128
  %108 = zext i64 %90 to i128
  %109 = mul nuw i128 %107, %108
  %110 = mul nuw i128 %101, %103
  %111 = add i128 %109, %110
  %112 = mul nuw i128 %107, %99
  %113 = zext i64 %95 to i128
  %114 = mul nuw i128 %113, %113
  %115 = add i128 %112, %114
  %116 = mul nuw i128 %107, %101
  %117 = mul nuw i128 %107, %107
  %118 = add i128 %98, -170141183460469231731687303715884072960
  %119 = add i128 %100, 170141183460469229370468033484042534912
  %120 = lshr i128 %117, 16
  %121 = add i128 %115, %120
  %122 = shl i128 %117, 40
  %123 = and i128 %122, 72056494526300160
  %124 = lshr i128 %116, 16
  %125 = shl i128 %116, 40
  %126 = and i128 %125, 72055395014672384
  %127 = sub i128 %119, %116
  %128 = lshr i128 %121, 16
  %129 = add i128 %104, 170141183460469229370504062281061498880
  %130 = add i128 %129, %102
  %131 = sub i128 %130, %117
  %132 = add i128 %131, %126
  %133 = add i128 %132, %128
  %134 = shl i128 %121, 40
  %135 = and i128 %134, 72056494526300160
  %136 = add i128 %127, %135
  %137 = sub i128 %118, %121
  %138 = lshr i128 %133, 56
  %139 = add i128 %111, %124
  %140 = add i128 %139, %123
  %141 = add i128 %140, %138
  %142 = and i128 %133, 72057594037927935
  %143 = lshr i128 %141, 56
  %144 = and i128 %141, 72057594037927935
  %145 = lshr i128 %141, 72
  %146 = add nuw nsw i128 %145, %142
  %147 = shl nuw nsw i128 %143, 40
  %148 = and i128 %147, 72056494526300160
  %149 = add i128 %136, %148
  %150 = sub i128 %137, %143
  %151 = lshr i128 %150, 56
  %152 = add i128 %149, %151
  %153 = trunc i128 %150 to i64
  %154 = and i64 %153, 72057594037927935
  %155 = lshr i128 %152, 56
  %156 = add nuw nsw i128 %146, %155
  %157 = trunc i128 %152 to i64
  %158 = and i64 %157, 72057594037927935
  %159 = lshr i128 %156, 56
  %160 = add nuw nsw i128 %159, %144
  %161 = trunc i128 %156 to i64
  %162 = and i64 %161, 72057594037927935
  %163 = trunc i128 %160 to i64
  %164 = zext i64 %7 to i128
  %165 = zext i64 %154 to i128
  %166 = mul nuw nsw i128 %165, %164
  %167 = zext i64 %158 to i128
  %168 = mul nuw nsw i128 %167, %164
  %169 = zext i64 %9 to i128
  %170 = mul nuw nsw i128 %165, %169
  %171 = zext i64 %162 to i128
  %172 = mul nuw nsw i128 %171, %164
  %173 = mul nuw nsw i128 %167, %169
  %174 = zext i64 %11 to i128
  %175 = mul nuw nsw i128 %165, %174
  %176 = mul nuw nsw i128 %160, %164
  %177 = mul nuw nsw i128 %171, %169
  %178 = mul nuw nsw i128 %167, %174
  %179 = zext i64 %13 to i128
  %180 = mul nuw nsw i128 %165, %179
  %181 = mul nuw nsw i128 %160, %169
  %182 = mul nuw nsw i128 %171, %174
  %183 = mul nuw nsw i128 %167, %179
  %184 = mul nuw nsw i128 %160, %174
  %185 = mul nuw nsw i128 %171, %179
  %186 = add nuw nsw i128 %185, %184
  %187 = mul nuw nsw i128 %160, %179
  %188 = add nsw i128 %166, -170141183460469231731687303715884072960
  %189 = lshr i128 %187, 16
  %190 = add nuw nsw i128 %181, %183
  %191 = add nuw i128 %190, %182
  %192 = add i128 %191, %189
  %193 = shl i128 %187, 40
  %194 = and i128 %193, 72056494526300160
  %195 = lshr i128 %186, 16
  %196 = shl i128 %186, 40
  %197 = and i128 %196, 72056494526300160
  %198 = lshr i128 %192, 16
  %199 = add nuw i128 %175, 170141183460469229370504062281061498880
  %200 = add i128 %199, %173
  %201 = sub i128 %200, %187
  %202 = add i128 %201, %172
  %203 = add i128 %202, %197
  %204 = add i128 %203, %198
  %205 = shl i128 %192, 40
  %206 = and i128 %205, 72056494526300160
  %207 = sub i128 %188, %192
  %208 = lshr i128 %204, 56
  %209 = add nuw nsw i128 %178, %180
  %210 = add nuw nsw i128 %209, %176
  %211 = add nuw i128 %210, %177
  %212 = add i128 %211, %194
  %213 = add i128 %212, %195
  %214 = add i128 %213, %208
  %215 = and i128 %204, 72057594037927935
  %216 = lshr i128 %214, 56
  %217 = and i128 %214, 72057594037927935
  %218 = lshr i128 %214, 72
  %219 = add nuw nsw i128 %218, %215
  %220 = shl nuw nsw i128 %216, 40
  %221 = and i128 %220, 72056494526300160
  %222 = sub i128 %207, %216
  %223 = lshr i128 %222, 56
  %224 = add nuw i128 %170, 170141183460469229370468033484042534912
  %225 = add i128 %224, %168
  %226 = sub i128 %225, %186
  %227 = add i128 %226, %206
  %228 = add i128 %227, %221
  %229 = add i128 %228, %223
  %230 = trunc i128 %222 to i64
  %231 = and i64 %230, 72057594037927935
  %232 = lshr i128 %229, 56
  %233 = add nuw nsw i128 %219, %232
  %234 = trunc i128 %229 to i64
  %235 = and i64 %234, 72057594037927935
  %236 = lshr i128 %233, 56
  %237 = add nuw nsw i128 %236, %217
  %238 = trunc i128 %233 to i64
  %239 = and i64 %238, 72057594037927935
  %240 = trunc i128 %237 to i64
  %241 = add i64 %7, 288230376151711748
  %242 = add i64 %9, 288225978105200636
  %243 = add i64 %11, 288230376151711740
  %244 = add i64 %13, 288230376151711740
  %245 = sub i64 %241, %79
  %246 = sub i64 %242, %83
  %247 = sub i64 %243, %87
  %248 = sub i64 %244, %88
  %249 = add i64 %79, %7
  %250 = add i64 %83, %9
  %251 = add i64 %87, %11
  %252 = add i64 %13, %88
  %253 = mul i64 %249, 3
  %254 = mul i64 %250, 3
  %255 = mul i64 %251, 3
  %256 = mul i64 %252, 3
  %257 = zext i64 %245 to i128
  %258 = zext i64 %253 to i128
  %259 = mul nuw i128 %258, %257
  %260 = zext i64 %254 to i128
  %261 = mul nuw i128 %260, %257
  %262 = zext i64 %246 to i128
  %263 = mul nuw i128 %262, %258
  %264 = zext i64 %255 to i128
  %265 = mul nuw i128 %264, %257
  %266 = mul nuw i128 %260, %262
  %267 = zext i64 %247 to i128
  %268 = mul nuw i128 %267, %258
  %269 = zext i64 %256 to i128
  %270 = mul nuw i128 %269, %257
  %271 = mul nuw i128 %264, %262
  %272 = mul nuw i128 %267, %260
  %273 = zext i64 %248 to i128
  %274 = mul nuw i128 %273, %258
  %275 = add i128 %271, %272
  %276 = mul nuw i128 %269, %262
  %277 = mul nuw i128 %264, %267
  %278 = mul nuw i128 %273, %260
  %279 = mul nuw i128 %269, %267
  %280 = mul nuw i128 %273, %264
  %281 = add i128 %279, %280
  %282 = mul nuw i128 %269, %273
  %283 = add i128 %259, -170141183460469231731687303715884072960
  %284 = lshr i128 %282, 16
  %285 = add i128 %278, %277
  %286 = add i128 %285, %276
  %287 = add i128 %286, %284
  %288 = shl i128 %282, 40
  %289 = and i128 %288, 72056494526300160
  %290 = lshr i128 %281, 16
  %291 = shl i128 %281, 40
  %292 = and i128 %291, 72056494526300160
  %293 = lshr i128 %287, 16
  %294 = add i128 %266, 170141183460469229370504062281061498880
  %295 = add i128 %294, %268
  %296 = add i128 %295, %265
  %297 = sub i128 %296, %282
  %298 = add i128 %297, %292
  %299 = add i128 %298, %293
  %300 = shl i128 %287, 40
  %301 = and i128 %300, 72056494526300160
  %302 = sub i128 %283, %287
  %303 = lshr i128 %299, 56
  %304 = add i128 %275, %274
  %305 = add i128 %304, %270
  %306 = add i128 %305, %289
  %307 = add i128 %306, %290
  %308 = add i128 %307, %303
  %309 = and i128 %299, 72057594037927935
  %310 = lshr i128 %308, 56
  %311 = and i128 %308, 72057594037927935
  %312 = lshr i128 %308, 72
  %313 = add nuw nsw i128 %312, %309
  %314 = shl nuw nsw i128 %310, 40
  %315 = and i128 %314, 72056494526300160
  %316 = sub i128 %302, %310
  %317 = lshr i128 %316, 56
  %318 = add i128 %263, 170141183460469229370468033484042534912
  %319 = add i128 %318, %261
  %320 = sub i128 %319, %281
  %321 = add i128 %320, %301
  %322 = add i128 %321, %315
  %323 = add i128 %322, %317
  %324 = trunc i128 %316 to i64
  %325 = and i64 %324, 72057594037927935
  %326 = lshr i128 %323, 56
  %327 = add nuw nsw i128 %313, %326
  %328 = trunc i128 %323 to i64
  %329 = and i64 %328, 72057594037927935
  %330 = lshr i128 %327, 56
  %331 = add nuw nsw i128 %330, %311
  %332 = trunc i128 %327 to i64
  %333 = and i64 %332, 72057594037927935
  %334 = shl nuw nsw i64 %325, 1
  %335 = shl nuw nsw i64 %329, 1
  %336 = shl nuw nsw i64 %333, 1
  %337 = zext i64 %325 to i128
  %338 = mul nuw nsw i128 %337, %337
  %339 = zext i64 %335 to i128
  %340 = mul nuw nsw i128 %339, %337
  %341 = zext i64 %336 to i128
  %342 = mul nuw nsw i128 %341, %337
  %343 = zext i64 %329 to i128
  %344 = mul nuw nsw i128 %343, %343
  %345 = zext i64 %334 to i128
  %346 = mul nuw nsw i128 %331, %345
  %347 = mul nuw nsw i128 %341, %343
  %348 = mul nuw nsw i128 %331, %339
  %349 = zext i64 %333 to i128
  %350 = mul nuw nsw i128 %349, %349
  %351 = add nuw nsw i128 %350, %348
  %352 = mul nuw nsw i128 %331, %341
  %353 = mul nuw nsw i128 %331, %331
  %354 = shl nuw nsw i64 %231, 3
  %355 = shl nuw nsw i64 %235, 3
  %356 = shl nuw nsw i64 %239, 3
  %357 = shl nuw nsw i64 %240, 3
  %358 = zext i64 %354 to i128
  %359 = zext i64 %355 to i128
  %360 = zext i64 %356 to i128
  %361 = zext i64 %357 to i128
  %362 = lshr i128 %353, 16
  %363 = add nuw i128 %351, %362
  %364 = shl i128 %353, 40
  %365 = and i128 %364, 72056494526300160
  %366 = lshr i128 %352, 16
  %367 = shl i128 %352, 40
  %368 = and i128 %367, 72055395014672384
  %369 = lshr i128 %363, 16
  %370 = add nuw i128 %344, 170141183460469229388950806354771050240
  %371 = sub i128 %370, %353
  %372 = add i128 %371, %342
  %373 = sub i128 %372, %360
  %374 = add i128 %373, %368
  %375 = add i128 %374, %369
  %376 = shl i128 %363, 40
  %377 = and i128 %376, 72056494526300160
  %378 = lshr i128 %375, 56
  %379 = add nuw nsw i128 %346, 18446744073709551360
  %380 = add nuw i128 %379, %347
  %381 = add i128 %380, %365
  %382 = add i128 %381, %366
  %383 = sub i128 %382, %361
  %384 = add i128 %383, %378
  %385 = and i128 %375, 72057594037927935
  %386 = lshr i128 %384, 56
  %387 = and i128 %384, 72057594037927935
  %388 = lshr i128 %384, 72
  %389 = add nuw nsw i128 %388, %385
  %390 = shl nuw nsw i128 %386, 40
  %391 = and i128 %390, 72056494526300160
  %392 = add nsw i128 %338, -170141183460469231713240559642174521088
  %393 = sub nuw i128 %392, %358
  %394 = sub i128 %393, %363
  %395 = sub i128 %394, %386
  %396 = lshr i128 %395, 56
  %397 = add nuw i128 %340, 170141183460469229388914496082775375616
  %398 = sub i128 %397, %359
  %399 = sub i128 %398, %352
  %400 = add i128 %399, %377
  %401 = add i128 %400, %391
  %402 = add i128 %401, %396
  %403 = lshr i128 %402, 56
  %404 = add nuw nsw i128 %389, %403
  %405 = insertelement <2 x i128> undef, i128 %395, i32 0
  %406 = insertelement <2 x i128> %405, i128 %402, i32 1
  %407 = trunc <2 x i128> %406 to <2 x i64>
  %408 = and <2 x i64> %407, <i64 72057594037927935, i64 72057594037927935>
  %409 = getelementptr inbounds i64, i64* %0, i64 1
  %410 = bitcast i64* %0 to <2 x i64>*
  store <2 x i64> %408, <2 x i64>* %410, align 8
  %411 = lshr i128 %404, 56
  %412 = add nuw nsw i128 %411, %387
  %413 = trunc i128 %404 to i64
  %414 = and i64 %413, 72057594037927935
  %415 = getelementptr inbounds i64, i64* %0, i64 2
  store i64 %414, i64* %415, align 8
  %416 = trunc i128 %412 to i64
  %417 = getelementptr inbounds i64, i64* %0, i64 3
  store i64 %416, i64* %417, align 8
  %418 = add nuw nsw i64 %154, %79
  %419 = add nuw nsw i64 %158, %83
  %420 = add nuw nsw i64 %162, %87
  %421 = add nuw nsw i64 %163, %88
  %422 = load i64, i64* %4, align 8
  %423 = load i64, i64* %91, align 8
  %424 = load i64, i64* %94, align 8
  %425 = load i64, i64* %105, align 8
  %426 = load i64, i64* %5, align 8
  %427 = add i64 %426, %422
  %428 = load i64, i64* %16, align 8
  %429 = add i64 %428, %423
  %430 = load i64, i64* %19, align 8
  %431 = add i64 %430, %424
  %432 = load i64, i64* %30, align 8
  %433 = add i64 %432, %425
  %434 = shl i64 %427, 1
  %435 = shl i64 %429, 1
  %436 = shl i64 %431, 1
  %437 = zext i64 %427 to i128
  %438 = mul nuw i128 %437, %437
  %439 = zext i64 %435 to i128
  %440 = mul nuw i128 %439, %437
  %441 = zext i64 %436 to i128
  %442 = mul nuw i128 %441, %437
  %443 = zext i64 %429 to i128
  %444 = mul nuw i128 %443, %443
  %445 = zext i64 %433 to i128
  %446 = zext i64 %434 to i128
  %447 = mul nuw i128 %445, %446
  %448 = mul nuw i128 %441, %443
  %449 = mul nuw i128 %445, %439
  %450 = zext i64 %431 to i128
  %451 = mul nuw i128 %450, %450
  %452 = add i128 %449, %451
  %453 = mul nuw i128 %445, %441
  %454 = mul nuw i128 %445, %445
  %455 = zext i64 %418 to i128
  %456 = zext i64 %419 to i128
  %457 = zext i64 %420 to i128
  %458 = zext i64 %421 to i128
  %459 = lshr i128 %454, 16
  %460 = add i128 %452, %459
  %461 = shl i128 %454, 40
  %462 = and i128 %461, 72056494526300160
  %463 = lshr i128 %453, 16
  %464 = shl i128 %453, 40
  %465 = and i128 %464, 72055395014672384
  %466 = lshr i128 %460, 16
  %467 = add i128 %444, 170141183460469229388950806354771050240
  %468 = add i128 %467, %442
  %469 = sub i128 %468, %454
  %470 = add i128 %469, %465
  %471 = add i128 %470, %466
  %472 = sub i128 %471, %457
  %473 = shl i128 %460, 40
  %474 = and i128 %473, 72056494526300160
  %475 = lshr i128 %472, 56
  %476 = add i128 %448, 18446744073709551360
  %477 = add i128 %476, %447
  %478 = add i128 %477, %463
  %479 = add i128 %478, %462
  %480 = sub i128 %479, %458
  %481 = add i128 %480, %475
  %482 = and i128 %472, 72057594037927935
  %483 = lshr i128 %481, 56
  %484 = and i128 %481, 72057594037927935
  %485 = lshr i128 %481, 72
  %486 = add nuw nsw i128 %485, %482
  %487 = shl nuw nsw i128 %483, 40
  %488 = and i128 %487, 72056494526300160
  %489 = add i128 %438, -170141183460469231713240559642174521088
  %490 = sub i128 %489, %455
  %491 = sub i128 %490, %460
  %492 = sub i128 %491, %483
  %493 = lshr i128 %492, 56
  %494 = add i128 %440, 170141183460469229388914496082775375616
  %495 = sub i128 %494, %453
  %496 = sub i128 %495, %456
  %497 = add i128 %496, %474
  %498 = add i128 %497, %488
  %499 = add i128 %498, %493
  %500 = lshr i128 %499, 56
  %501 = add nuw nsw i128 %486, %500
  %502 = insertelement <2 x i128> undef, i128 %492, i32 0
  %503 = insertelement <2 x i128> %502, i128 %499, i32 1
  %504 = trunc <2 x i128> %503 to <2 x i64>
  %505 = and <2 x i64> %504, <i64 72057594037927935, i64 72057594037927935>
  %506 = bitcast i64* %2 to <2 x i64>*
  store <2 x i64> %505, <2 x i64>* %506, align 8
  %507 = lshr i128 %501, 56
  %508 = add nuw nsw i128 %507, %484
  %509 = trunc i128 %501 to i64
  %510 = and i64 %509, 72057594037927935
  %511 = getelementptr inbounds i64, i64* %2, i64 2
  store i64 %510, i64* %511, align 8
  %512 = trunc i128 %508 to i64
  %513 = getelementptr inbounds i64, i64* %2, i64 3
  store i64 %512, i64* %513, align 8
  %514 = shl nuw nsw i64 %231, 2
  %515 = shl nuw nsw i64 %235, 2
  %516 = shl nuw nsw i64 %239, 2
  %517 = shl nuw nsw i64 %240, 2
  %518 = load i64, i64* %0, align 8
  %519 = sub i64 288230376151711748, %518
  %520 = add i64 %519, %514
  %521 = load i64, i64* %409, align 8
  %522 = sub i64 288225978105200636, %521
  %523 = add i64 %522, %515
  %524 = load i64, i64* %415, align 8
  %525 = sub i64 288230376151711740, %524
  %526 = add i64 %525, %516
  %527 = load i64, i64* %417, align 8
  %528 = sub i64 288230376151711740, %527
  %529 = add i64 %528, %517
  %530 = zext i64 %520 to i128
  %531 = mul nuw nsw i128 %530, %337
  %532 = zext i64 %523 to i128
  %533 = mul nuw nsw i128 %532, %337
  %534 = mul nuw nsw i128 %530, %343
  %535 = zext i64 %526 to i128
  %536 = mul nuw nsw i128 %535, %337
  %537 = mul nuw nsw i128 %532, %343
  %538 = mul nuw nsw i128 %349, %530
  %539 = zext i64 %529 to i128
  %540 = mul nuw nsw i128 %539, %337
  %541 = mul nuw nsw i128 %535, %343
  %542 = mul nuw nsw i128 %532, %349
  %543 = mul nuw nsw i128 %331, %530
  %544 = mul nuw nsw i128 %539, %343
  %545 = mul nuw nsw i128 %535, %349
  %546 = mul nuw nsw i128 %331, %532
  %547 = mul nuw nsw i128 %539, %349
  %548 = mul nuw nsw i128 %331, %535
  %549 = mul nuw nsw i128 %331, %539
  %550 = shl nuw nsw i64 %154, 1
  %551 = shl nuw nsw i64 %158, 1
  %552 = shl nuw nsw i64 %162, 1
  %553 = zext i64 %551 to i128
  %554 = zext i64 %552 to i128
  %555 = mul nuw nsw i128 %554, %165
  %556 = mul nuw nsw i128 %167, %167
  %557 = add nuw nsw i128 %555, %556
  %558 = zext i64 %550 to i128
  %559 = mul nuw nsw i128 %160, %558
  %560 = mul nuw nsw i128 %554, %167
  %561 = add nuw nsw i128 %560, %559
  %562 = mul nuw nsw i128 %160, %553
  %563 = mul nuw nsw i128 %171, %171
  %564 = add nuw nsw i128 %563, %562
  %565 = shl nuw nsw i128 %557, 3
  %566 = shl i128 %561, 3
  %567 = shl i128 %564, 3
  %568 = or i128 %531, 1329227995784915872903807060280344576
  %569 = mul nsw i128 %165, -8
  %570 = mul nsw i128 %569, %165
  %571 = mul nsw i128 %569, %553
  %572 = mul nsw i128 %160, -8
  %573 = mul nsw i128 %572, %554
  %574 = add i128 %573, 1329227995784915854457062986570792960
  %575 = add i128 %574, %548
  %576 = add i128 %575, %547
  %577 = mul nsw i128 %572, %160
  %578 = add i128 %577, 1329227995784915854457062986570792960
  %579 = add i128 %578, %549
  %580 = lshr i128 %579, 16
  %581 = sub i128 1329207713375312202786639039319506944, %567
  %582 = add i128 %581, %546
  %583 = add i128 %582, %545
  %584 = add i128 %583, %544
  %585 = add i128 %584, %580
  %586 = shl i128 %579, 40
  %587 = and i128 %586, 72056494526300160
  %588 = lshr i128 %576, 16
  %589 = shl i128 %576, 40
  %590 = and i128 %589, 72056494526300160
  %591 = lshr i128 %585, 16
  %592 = sub nuw i128 -168811955464684318238413482164135919616, %565
  %593 = add i128 %592, %538
  %594 = add i128 %593, %537
  %595 = add i128 %594, %536
  %596 = sub i128 %595, %579
  %597 = add i128 %596, %590
  %598 = add i128 %597, %591
  %599 = shl i128 %585, 40
  %600 = and i128 %599, 72056494526300160
  %601 = lshr i128 %598, 56
  %602 = sub i128 1329227995784915872903807060280344576, %566
  %603 = add i128 %602, %543
  %604 = add i128 %603, %542
  %605 = add i128 %604, %541
  %606 = add i128 %605, %540
  %607 = add i128 %606, %588
  %608 = add i128 %607, %587
  %609 = add i128 %608, %601
  %610 = and i128 %598, 72057594037927935
  %611 = lshr i128 %609, 56
  %612 = and i128 %609, 72057594037927935
  %613 = lshr i128 %609, 72
  %614 = add nuw nsw i128 %613, %610
  %615 = shl nuw nsw i128 %611, 40
  %616 = and i128 %615, 72056494526300160
  %617 = add i128 %570, -170141183460469231731687303715884072960
  %618 = add i128 %617, %568
  %619 = sub i128 %618, %585
  %620 = sub i128 %619, %611
  %621 = lshr i128 %620, 56
  %622 = add i128 %571, -168811955464684318238449510961154883584
  %623 = add i128 %622, %534
  %624 = add i128 %623, %533
  %625 = sub i128 %624, %576
  %626 = add i128 %625, %600
  %627 = add i128 %626, %616
  %628 = add i128 %627, %621
  %629 = lshr i128 %628, 56
  %630 = add nuw nsw i128 %614, %629
  %631 = insertelement <2 x i128> undef, i128 %620, i32 0
  %632 = insertelement <2 x i128> %631, i128 %628, i32 1
  %633 = trunc <2 x i128> %632 to <2 x i64>
  %634 = and <2 x i64> %633, <i64 72057594037927935, i64 72057594037927935>
  %635 = bitcast i64* %1 to <2 x i64>*
  store <2 x i64> %634, <2 x i64>* %635, align 8
  %636 = lshr i128 %630, 56
  %637 = add nuw nsw i128 %636, %612
  %638 = trunc i128 %630 to i64
  %639 = and i64 %638, 72057594037927935
  %640 = getelementptr inbounds i64, i64* %1, i64 2
  store i64 %639, i64* %640, align 8
  %641 = trunc i128 %637 to i64
  %642 = getelementptr inbounds i64, i64* %1, i64 3
  store i64 %641, i64* %642, align 8
  ret void
}

; Function Attrs: nounwind readonly
declare dso_local i32 @__gmpz_tstbit(%1*, i64) local_unnamed_addr #3

; Function Attrs: nounwind sspstrong uwtable
define internal fastcc void @1(i64* nocapture %0, i64* nocapture %1, i64* nocapture %2, i64* nocapture readonly %3, i64* nocapture readonly %4, i64* readonly %5, i64* nocapture readonly %6, i64* nocapture readonly %7, i64* nocapture readonly %8) unnamed_addr #0 {
  %10 = bitcast i64* %8 to <2 x i64>*
  %11 = load <2 x i64>, <2 x i64>* %10, align 8
  %12 = extractelement <2 x i64> %11, i32 0
  %13 = shl i64 %12, 1
  %14 = extractelement <2 x i64> %11, i32 1
  %15 = shl i64 %14, 1
  %16 = getelementptr inbounds i64, i64* %8, i64 2
  %17 = bitcast i64* %16 to <2 x i64>*
  %18 = load <2 x i64>, <2 x i64>* %17, align 8
  %19 = extractelement <2 x i64> %18, i32 0
  %20 = shl i64 %19, 1
  %21 = zext i64 %12 to i128
  %22 = mul nuw i128 %21, %21
  %23 = zext i64 %15 to i128
  %24 = mul nuw i128 %23, %21
  %25 = zext i64 %20 to i128
  %26 = mul nuw i128 %25, %21
  %27 = zext i64 %14 to i128
  %28 = mul nuw i128 %27, %27
  %29 = extractelement <2 x i64> %18, i32 1
  %30 = zext i64 %29 to i128
  %31 = zext i64 %13 to i128
  %32 = mul nuw i128 %30, %31
  %33 = mul nuw i128 %25, %27
  %34 = add i128 %32, %33
  %35 = mul nuw i128 %30, %23
  %36 = zext i64 %19 to i128
  %37 = mul nuw i128 %36, %36
  %38 = add i128 %35, %37
  %39 = mul nuw i128 %30, %25
  %40 = mul nuw i128 %30, %30
  %41 = add i128 %22, -170141183460469231731687303715884072960
  %42 = add i128 %24, 170141183460469229370468033484042534912
  %43 = lshr i128 %40, 16
  %44 = add i128 %38, %43
  %45 = shl i128 %40, 40
  %46 = and i128 %45, 72056494526300160
  %47 = lshr i128 %39, 16
  %48 = shl i128 %39, 40
  %49 = and i128 %48, 72055395014672384
  %50 = sub i128 %42, %39
  %51 = lshr i128 %44, 16
  %52 = add i128 %28, 170141183460469229370504062281061498880
  %53 = add i128 %52, %26
  %54 = sub i128 %53, %40
  %55 = add i128 %54, %49
  %56 = add i128 %55, %51
  %57 = shl i128 %44, 40
  %58 = and i128 %57, 72056494526300160
  %59 = add i128 %50, %58
  %60 = sub i128 %41, %44
  %61 = lshr i128 %56, 56
  %62 = add i128 %34, %47
  %63 = add i128 %62, %46
  %64 = add i128 %63, %61
  %65 = and i128 %56, 72057594037927935
  %66 = lshr i128 %64, 56
  %67 = and i128 %64, 72057594037927935
  %68 = lshr i128 %64, 72
  %69 = add nuw nsw i128 %68, %65
  %70 = shl nuw nsw i128 %66, 40
  %71 = and i128 %70, 72056494526300160
  %72 = add i128 %59, %71
  %73 = sub i128 %60, %66
  %74 = lshr i128 %73, 56
  %75 = add i128 %72, %74
  %76 = lshr i128 %75, 56
  %77 = add nuw nsw i128 %69, %76
  %78 = lshr i128 %77, 56
  %79 = add nuw nsw i128 %78, %67
  %80 = and i128 %73, 72057594037927935
  %81 = mul nuw nsw i128 %80, %21
  %82 = mul nuw nsw i128 %80, %27
  %83 = and i128 %75, 72057594037927935
  %84 = mul nuw nsw i128 %83, %21
  %85 = mul nuw nsw i128 %80, %36
  %86 = mul nuw nsw i128 %83, %27
  %87 = and i128 %77, 72057594037927935
  %88 = mul nuw nsw i128 %87, %21
  %89 = mul nuw nsw i128 %80, %30
  %90 = mul nuw nsw i128 %83, %36
  %91 = mul nuw nsw i128 %87, %27
  %92 = mul nuw nsw i128 %79, %21
  %93 = mul nuw nsw i128 %83, %30
  %94 = mul nuw nsw i128 %87, %36
  %95 = add nuw nsw i128 %94, %93
  %96 = mul nuw nsw i128 %79, %27
  %97 = add nuw nsw i128 %95, %96
  %98 = mul nuw nsw i128 %87, %30
  %99 = mul nuw nsw i128 %79, %36
  %100 = add nuw nsw i128 %99, %98
  %101 = mul nuw nsw i128 %79, %30
  %102 = add nsw i128 %81, -170141183460469231731687303715884072960
  %103 = lshr i128 %101, 16
  %104 = add nuw i128 %97, %103
  %105 = shl i128 %101, 40
  %106 = and i128 %105, 72056494526300160
  %107 = lshr i128 %100, 16
  %108 = shl i128 %100, 40
  %109 = and i128 %108, 72056494526300160
  %110 = lshr i128 %104, 16
  %111 = add nuw i128 %85, 170141183460469229370504062281061498880
  %112 = add i128 %111, %86
  %113 = add i128 %112, %88
  %114 = sub i128 %113, %101
  %115 = add i128 %114, %109
  %116 = add i128 %115, %110
  %117 = shl i128 %104, 40
  %118 = and i128 %117, 72056494526300160
  %119 = sub i128 %102, %104
  %120 = lshr i128 %116, 56
  %121 = add nuw nsw i128 %90, %89
  %122 = add nuw nsw i128 %121, %91
  %123 = add nuw nsw i128 %122, %92
  %124 = add nuw i128 %123, %106
  %125 = add i128 %124, %107
  %126 = add i128 %125, %120
  %127 = and i128 %116, 72057594037927935
  %128 = lshr i128 %126, 56
  %129 = and i128 %126, 72057594037927935
  %130 = lshr i128 %126, 72
  %131 = add nuw nsw i128 %130, %127
  %132 = shl nuw nsw i128 %128, 40
  %133 = and i128 %132, 72056494526300160
  %134 = sub i128 %119, %128
  %135 = lshr i128 %134, 56
  %136 = add nuw i128 %82, 170141183460469229370468033484042534912
  %137 = add i128 %136, %84
  %138 = sub i128 %137, %100
  %139 = add i128 %138, %118
  %140 = add i128 %139, %133
  %141 = add i128 %140, %135
  %142 = lshr i128 %141, 56
  %143 = add nuw nsw i128 %131, %142
  %144 = lshr i128 %143, 56
  %145 = add nuw nsw i128 %144, %129
  %146 = and i128 %134, 72057594037927935
  %147 = bitcast i64* %4 to <2 x i64>*
  %148 = load <2 x i64>, <2 x i64>* %147, align 8
  %149 = extractelement <2 x i64> %148, i32 0
  %150 = zext i64 %149 to i128
  %151 = mul nuw nsw i128 %146, %150
  %152 = extractelement <2 x i64> %148, i32 1
  %153 = zext i64 %152 to i128
  %154 = mul nuw nsw i128 %146, %153
  %155 = and i128 %141, 72057594037927935
  %156 = mul nuw nsw i128 %155, %150
  %157 = getelementptr inbounds i64, i64* %4, i64 2
  %158 = bitcast i64* %157 to <2 x i64>*
  %159 = load <2 x i64>, <2 x i64>* %158, align 8
  %160 = extractelement <2 x i64> %159, i32 0
  %161 = zext i64 %160 to i128
  %162 = mul nuw nsw i128 %146, %161
  %163 = mul nuw nsw i128 %155, %153
  %164 = and i128 %143, 72057594037927935
  %165 = mul nuw nsw i128 %164, %150
  %166 = extractelement <2 x i64> %159, i32 1
  %167 = zext i64 %166 to i128
  %168 = mul nuw nsw i128 %146, %167
  %169 = mul nuw nsw i128 %155, %161
  %170 = mul nuw nsw i128 %164, %153
  %171 = mul nuw nsw i128 %145, %150
  %172 = mul nuw nsw i128 %155, %167
  %173 = mul nuw nsw i128 %164, %161
  %174 = add nuw nsw i128 %173, %172
  %175 = mul nuw nsw i128 %145, %153
  %176 = add nuw nsw i128 %174, %175
  %177 = mul nuw nsw i128 %164, %167
  %178 = mul nuw nsw i128 %145, %161
  %179 = add nuw nsw i128 %178, %177
  %180 = mul nuw nsw i128 %145, %167
  %181 = add nsw i128 %151, -170141183460469231731687303715884072960
  %182 = lshr i128 %180, 16
  %183 = add nuw i128 %176, %182
  %184 = shl i128 %180, 40
  %185 = and i128 %184, 72056494526300160
  %186 = lshr i128 %179, 16
  %187 = shl i128 %179, 40
  %188 = and i128 %187, 72056494526300160
  %189 = lshr i128 %183, 16
  %190 = add nuw i128 %162, 170141183460469229370504062281061498880
  %191 = add i128 %190, %163
  %192 = add i128 %191, %165
  %193 = sub i128 %192, %180
  %194 = add i128 %193, %188
  %195 = add i128 %194, %189
  %196 = shl i128 %183, 40
  %197 = and i128 %196, 72056494526300160
  %198 = sub i128 %181, %183
  %199 = lshr i128 %195, 56
  %200 = add nuw nsw i128 %169, %168
  %201 = add nuw nsw i128 %200, %170
  %202 = add nuw nsw i128 %201, %171
  %203 = add nuw i128 %202, %185
  %204 = add i128 %203, %186
  %205 = add i128 %204, %199
  %206 = and i128 %195, 72057594037927935
  %207 = lshr i128 %205, 56
  %208 = and i128 %205, 72057594037927935
  %209 = lshr i128 %205, 72
  %210 = add nuw nsw i128 %209, %206
  %211 = shl nuw nsw i128 %207, 40
  %212 = and i128 %211, 72056494526300160
  %213 = sub i128 %198, %207
  %214 = lshr i128 %213, 56
  %215 = add nuw i128 %154, 170141183460469229370468033484042534912
  %216 = add i128 %215, %156
  %217 = sub i128 %216, %179
  %218 = add i128 %217, %197
  %219 = add i128 %218, %212
  %220 = add i128 %219, %214
  %221 = lshr i128 %220, 56
  %222 = add nuw nsw i128 %210, %221
  %223 = lshr i128 %222, 56
  %224 = add nuw nsw i128 %223, %208
  %225 = bitcast i64* %3 to <2 x i64>*
  %226 = load <2 x i64>, <2 x i64>* %225, align 8
  %227 = extractelement <2 x i64> %226, i32 0
  %228 = zext i64 %227 to i128
  %229 = mul nuw nsw i128 %80, %228
  %230 = extractelement <2 x i64> %226, i32 1
  %231 = zext i64 %230 to i128
  %232 = mul nuw nsw i128 %80, %231
  %233 = mul nuw nsw i128 %83, %228
  %234 = getelementptr inbounds i64, i64* %3, i64 2
  %235 = bitcast i64* %234 to <2 x i64>*
  %236 = load <2 x i64>, <2 x i64>* %235, align 8
  %237 = extractelement <2 x i64> %236, i32 0
  %238 = zext i64 %237 to i128
  %239 = mul nuw nsw i128 %80, %238
  %240 = mul nuw nsw i128 %83, %231
  %241 = mul nuw nsw i128 %87, %228
  %242 = extractelement <2 x i64> %236, i32 1
  %243 = zext i64 %242 to i128
  %244 = mul nuw nsw i128 %80, %243
  %245 = mul nuw nsw i128 %83, %238
  %246 = mul nuw nsw i128 %87, %231
  %247 = mul nuw nsw i128 %79, %228
  %248 = mul nuw nsw i128 %83, %243
  %249 = mul nuw nsw i128 %87, %238
  %250 = add nuw nsw i128 %249, %248
  %251 = mul nuw nsw i128 %79, %231
  %252 = add nuw nsw i128 %250, %251
  %253 = mul nuw nsw i128 %87, %243
  %254 = mul nuw nsw i128 %79, %238
  %255 = add nuw nsw i128 %254, %253
  %256 = mul nuw nsw i128 %79, %243
  %257 = add nsw i128 %229, -170141183460469231731687303715884072960
  %258 = lshr i128 %256, 16
  %259 = add nuw i128 %252, %258
  %260 = shl i128 %256, 40
  %261 = and i128 %260, 72056494526300160
  %262 = lshr i128 %255, 16
  %263 = shl i128 %255, 40
  %264 = and i128 %263, 72056494526300160
  %265 = lshr i128 %259, 16
  %266 = add nuw i128 %239, 170141183460469229370504062281061498880
  %267 = add i128 %266, %240
  %268 = add i128 %267, %241
  %269 = sub i128 %268, %256
  %270 = add i128 %269, %264
  %271 = add i128 %270, %265
  %272 = shl i128 %259, 40
  %273 = and i128 %272, 72056494526300160
  %274 = sub i128 %257, %259
  %275 = lshr i128 %271, 56
  %276 = add nuw nsw i128 %245, %244
  %277 = add nuw nsw i128 %276, %246
  %278 = add nuw nsw i128 %277, %247
  %279 = add nuw i128 %278, %261
  %280 = add i128 %279, %262
  %281 = add i128 %280, %275
  %282 = and i128 %271, 72057594037927935
  %283 = lshr i128 %281, 56
  %284 = and i128 %281, 72057594037927935
  %285 = lshr i128 %281, 72
  %286 = add nuw nsw i128 %285, %282
  %287 = shl nuw nsw i128 %283, 40
  %288 = and i128 %287, 72056494526300160
  %289 = sub i128 %274, %283
  %290 = lshr i128 %289, 56
  %291 = add nuw i128 %232, 170141183460469229370468033484042534912
  %292 = add i128 %291, %233
  %293 = sub i128 %292, %255
  %294 = add i128 %293, %273
  %295 = add i128 %294, %288
  %296 = add i128 %295, %290
  %297 = lshr i128 %296, 56
  %298 = add nuw nsw i128 %286, %297
  %299 = lshr i128 %298, 56
  %300 = add nuw nsw i128 %299, %284
  %301 = bitcast i64* %5 to <2 x i64>*
  %302 = load <2 x i64>, <2 x i64>* %301, align 8
  %303 = extractelement <2 x i64> %302, i32 0
  %304 = shl i64 %303, 1
  %305 = extractelement <2 x i64> %302, i32 1
  %306 = shl i64 %305, 1
  %307 = getelementptr inbounds i64, i64* %5, i64 2
  %308 = bitcast i64* %307 to <2 x i64>*
  %309 = load <2 x i64>, <2 x i64>* %308, align 8
  %310 = extractelement <2 x i64> %309, i32 0
  %311 = shl i64 %310, 1
  %312 = zext i64 %303 to i128
  %313 = mul nuw i128 %312, %312
  %314 = zext i64 %306 to i128
  %315 = mul nuw i128 %314, %312
  %316 = zext i64 %311 to i128
  %317 = mul nuw i128 %316, %312
  %318 = zext i64 %305 to i128
  %319 = mul nuw i128 %318, %318
  %320 = extractelement <2 x i64> %309, i32 1
  %321 = zext i64 %320 to i128
  %322 = zext i64 %304 to i128
  %323 = mul nuw i128 %321, %322
  %324 = mul nuw i128 %316, %318
  %325 = add i128 %323, %324
  %326 = mul nuw i128 %321, %314
  %327 = zext i64 %310 to i128
  %328 = mul nuw i128 %327, %327
  %329 = add i128 %326, %328
  %330 = mul nuw i128 %321, %316
  %331 = mul nuw i128 %321, %321
  %332 = add i128 %313, -170141183460469231731687303715884072960
  %333 = add i128 %315, 170141183460469229370468033484042534912
  %334 = lshr i128 %331, 16
  %335 = add i128 %329, %334
  %336 = shl i128 %331, 40
  %337 = and i128 %336, 72056494526300160
  %338 = lshr i128 %330, 16
  %339 = shl i128 %330, 40
  %340 = and i128 %339, 72055395014672384
  %341 = sub i128 %333, %330
  %342 = lshr i128 %335, 16
  %343 = add i128 %319, 170141183460469229370504062281061498880
  %344 = add i128 %343, %317
  %345 = sub i128 %344, %331
  %346 = add i128 %345, %340
  %347 = add i128 %346, %342
  %348 = shl i128 %335, 40
  %349 = and i128 %348, 72056494526300160
  %350 = add i128 %341, %349
  %351 = sub i128 %332, %335
  %352 = lshr i128 %347, 56
  %353 = add i128 %325, %338
  %354 = add i128 %353, %337
  %355 = add i128 %354, %352
  %356 = and i128 %347, 72057594037927935
  %357 = lshr i128 %355, 56
  %358 = and i128 %355, 72057594037927935
  %359 = lshr i128 %355, 72
  %360 = add nuw nsw i128 %359, %356
  %361 = shl nuw nsw i128 %357, 40
  %362 = and i128 %361, 72056494526300160
  %363 = add i128 %350, %362
  %364 = sub i128 %351, %357
  %365 = lshr i128 %364, 56
  %366 = add i128 %363, %365
  %367 = lshr i128 %366, 56
  %368 = add nuw nsw i128 %360, %367
  %369 = lshr i128 %368, 56
  %370 = add nuw nsw i128 %369, %358
  %371 = and i128 %364, 72057594037927935
  %372 = mul nuw nsw i128 %371, %312
  %373 = mul nuw nsw i128 %371, %318
  %374 = and i128 %366, 72057594037927935
  %375 = mul nuw nsw i128 %374, %312
  %376 = mul nuw nsw i128 %371, %327
  %377 = mul nuw nsw i128 %374, %318
  %378 = and i128 %368, 72057594037927935
  %379 = mul nuw nsw i128 %378, %312
  %380 = mul nuw nsw i128 %371, %321
  %381 = mul nuw nsw i128 %374, %327
  %382 = mul nuw nsw i128 %378, %318
  %383 = mul nuw nsw i128 %370, %312
  %384 = mul nuw nsw i128 %374, %321
  %385 = mul nuw nsw i128 %378, %327
  %386 = add nuw nsw i128 %385, %384
  %387 = mul nuw nsw i128 %370, %318
  %388 = add nuw nsw i128 %386, %387
  %389 = mul nuw nsw i128 %378, %321
  %390 = mul nuw nsw i128 %370, %327
  %391 = add nuw nsw i128 %390, %389
  %392 = mul nuw nsw i128 %370, %321
  %393 = add nsw i128 %372, -170141183460469231731687303715884072960
  %394 = lshr i128 %392, 16
  %395 = add nuw i128 %388, %394
  %396 = shl i128 %392, 40
  %397 = and i128 %396, 72056494526300160
  %398 = lshr i128 %391, 16
  %399 = shl i128 %391, 40
  %400 = and i128 %399, 72056494526300160
  %401 = lshr i128 %395, 16
  %402 = add nuw i128 %376, 170141183460469229370504062281061498880
  %403 = add i128 %402, %377
  %404 = add i128 %403, %379
  %405 = sub i128 %404, %392
  %406 = add i128 %405, %400
  %407 = add i128 %406, %401
  %408 = shl i128 %395, 40
  %409 = and i128 %408, 72056494526300160
  %410 = sub i128 %393, %395
  %411 = lshr i128 %407, 56
  %412 = add nuw nsw i128 %381, %380
  %413 = add nuw nsw i128 %412, %382
  %414 = add nuw nsw i128 %413, %383
  %415 = add nuw i128 %414, %397
  %416 = add i128 %415, %398
  %417 = add i128 %416, %411
  %418 = and i128 %407, 72057594037927935
  %419 = lshr i128 %417, 56
  %420 = and i128 %417, 72057594037927935
  %421 = lshr i128 %417, 72
  %422 = add nuw nsw i128 %421, %418
  %423 = shl nuw nsw i128 %419, 40
  %424 = and i128 %423, 72056494526300160
  %425 = sub i128 %410, %419
  %426 = lshr i128 %425, 56
  %427 = add nuw i128 %373, 170141183460469229370468033484042534912
  %428 = add i128 %427, %375
  %429 = sub i128 %428, %391
  %430 = add i128 %429, %409
  %431 = add i128 %430, %424
  %432 = add i128 %431, %426
  %433 = lshr i128 %432, 56
  %434 = add nuw nsw i128 %422, %433
  %435 = lshr i128 %434, 56
  %436 = add nuw nsw i128 %435, %420
  %437 = and i128 %425, 72057594037927935
  %438 = bitcast i64* %7 to <2 x i64>*
  %439 = load <2 x i64>, <2 x i64>* %438, align 8
  %440 = extractelement <2 x i64> %439, i32 0
  %441 = zext i64 %440 to i128
  %442 = mul nuw nsw i128 %437, %441
  %443 = extractelement <2 x i64> %439, i32 1
  %444 = zext i64 %443 to i128
  %445 = mul nuw nsw i128 %437, %444
  %446 = and i128 %432, 72057594037927935
  %447 = mul nuw nsw i128 %446, %441
  %448 = getelementptr inbounds i64, i64* %7, i64 2
  %449 = bitcast i64* %448 to <2 x i64>*
  %450 = load <2 x i64>, <2 x i64>* %449, align 8
  %451 = extractelement <2 x i64> %450, i32 0
  %452 = zext i64 %451 to i128
  %453 = mul nuw nsw i128 %437, %452
  %454 = mul nuw nsw i128 %446, %444
  %455 = and i128 %434, 72057594037927935
  %456 = mul nuw nsw i128 %455, %441
  %457 = extractelement <2 x i64> %450, i32 1
  %458 = zext i64 %457 to i128
  %459 = mul nuw nsw i128 %437, %458
  %460 = mul nuw nsw i128 %446, %452
  %461 = mul nuw nsw i128 %455, %444
  %462 = mul nuw nsw i128 %436, %441
  %463 = mul nuw nsw i128 %446, %458
  %464 = mul nuw nsw i128 %455, %452
  %465 = add nuw nsw i128 %464, %463
  %466 = mul nuw nsw i128 %436, %444
  %467 = add nuw nsw i128 %465, %466
  %468 = mul nuw nsw i128 %455, %458
  %469 = mul nuw nsw i128 %436, %452
  %470 = add nuw nsw i128 %469, %468
  %471 = mul nuw nsw i128 %436, %458
  %472 = and i128 %213, 72057594037927935
  %473 = and i128 %220, 72057594037927935
  %474 = and i128 %222, 72057594037927935
  %475 = lshr i128 %471, 16
  %476 = add nuw i128 %467, %475
  %477 = shl i128 %471, 40
  %478 = and i128 %477, 72056494526300160
  %479 = lshr i128 %470, 16
  %480 = shl i128 %470, 40
  %481 = and i128 %480, 72056494526300160
  %482 = lshr i128 %476, 16
  %483 = add nuw i128 %453, 170141183460469229388950806354771050240
  %484 = add i128 %483, %454
  %485 = add i128 %484, %456
  %486 = sub i128 %485, %471
  %487 = sub i128 %486, %474
  %488 = add i128 %487, %481
  %489 = shl i128 %476, 40
  %490 = and i128 %489, 72056494526300160
  %491 = bitcast i64* %6 to <2 x i64>*
  %492 = load <2 x i64>, <2 x i64>* %491, align 8
  %493 = extractelement <2 x i64> %492, i32 0
  %494 = zext i64 %493 to i128
  %495 = extractelement <2 x i64> %492, i32 1
  %496 = zext i64 %495 to i128
  %497 = getelementptr inbounds i64, i64* %6, i64 2
  %498 = bitcast i64* %497 to <2 x i64>*
  %499 = load <2 x i64>, <2 x i64>* %498, align 8
  %500 = extractelement <2 x i64> %499, i32 0
  %501 = zext i64 %500 to i128
  %502 = mul nuw nsw i128 %371, %501
  %503 = mul nuw nsw i128 %374, %496
  %504 = mul nuw nsw i128 %378, %494
  %505 = extractelement <2 x i64> %499, i32 1
  %506 = zext i64 %505 to i128
  %507 = mul nuw nsw i128 %374, %506
  %508 = mul nuw nsw i128 %378, %501
  %509 = add nuw nsw i128 %508, %507
  %510 = mul nuw nsw i128 %370, %496
  %511 = add nuw nsw i128 %509, %510
  %512 = mul nuw nsw i128 %378, %506
  %513 = mul nuw nsw i128 %370, %501
  %514 = add nuw nsw i128 %513, %512
  %515 = mul nuw nsw i128 %370, %506
  %516 = and i128 %298, 72057594037927935
  %517 = lshr i128 %515, 16
  %518 = add nuw i128 %511, %517
  %519 = shl i128 %514, 40
  %520 = and i128 %519, 72056494526300160
  %521 = lshr i128 %518, 16
  %522 = add nuw i128 %502, 170141183460469229388950806354771050240
  %523 = add i128 %522, %503
  %524 = add i128 %523, %504
  %525 = sub i128 %524, %515
  %526 = sub i128 %525, %516
  %527 = add i128 %526, %520
  %528 = insertelement <2 x i128> undef, i128 %488, i32 0
  %529 = insertelement <2 x i128> %528, i128 %527, i32 1
  %530 = insertelement <2 x i128> undef, i128 %482, i32 0
  %531 = insertelement <2 x i128> %530, i128 %521, i32 1
  %532 = add <2 x i128> %529, %531
  %533 = extractelement <2 x i128> %532, i32 0
  %534 = lshr i128 %533, 56
  %535 = add nuw nsw i128 %459, 18446744073709551360
  %536 = add nuw nsw i128 %535, %460
  %537 = add nuw nsw i128 %536, %461
  %538 = add nuw nsw i128 %537, %462
  %539 = add nuw i128 %538, %478
  %540 = add i128 %539, %479
  %541 = sub i128 %540, %224
  %542 = mul nuw nsw i128 %371, %506
  %543 = mul nuw nsw i128 %374, %501
  %544 = mul nuw nsw i128 %378, %496
  %545 = mul nuw nsw i128 %370, %494
  %546 = shl i128 %515, 40
  %547 = and i128 %546, 72056494526300160
  %548 = lshr i128 %514, 16
  %549 = extractelement <2 x i128> %532, i32 1
  %550 = lshr i128 %549, 56
  %551 = add nuw nsw i128 %542, 18446744073709551360
  %552 = add nuw nsw i128 %551, %543
  %553 = add nuw nsw i128 %552, %544
  %554 = add nuw nsw i128 %553, %545
  %555 = add nuw i128 %554, %547
  %556 = add i128 %555, %548
  %557 = sub i128 %556, %300
  %558 = insertelement <2 x i128> undef, i128 %541, i32 0
  %559 = insertelement <2 x i128> %558, i128 %557, i32 1
  %560 = insertelement <2 x i128> undef, i128 %534, i32 0
  %561 = insertelement <2 x i128> %560, i128 %550, i32 1
  %562 = add <2 x i128> %559, %561
  %563 = extractelement <2 x i128> %562, i32 0
  %564 = lshr i128 %563, 56
  %565 = shl nuw nsw i128 %564, 40
  %566 = add nsw i128 %442, -170141183460469231713240559642174521088
  %567 = sub nuw i128 %566, %472
  %568 = sub i128 %567, %476
  %569 = add nuw i128 %445, 170141183460469229388914496082775375616
  %570 = add i128 %569, %447
  %571 = sub i128 %570, %473
  %572 = sub i128 %571, %470
  %573 = mul nuw nsw i128 %371, %494
  %574 = mul nuw nsw i128 %371, %496
  %575 = mul nuw nsw i128 %374, %494
  %576 = and i128 %289, 72057594037927935
  %577 = and i128 %296, 72057594037927935
  %578 = shl i128 %518, 40
  %579 = and i128 %578, 72056494526300160
  %580 = and <2 x i128> %532, <i128 72057594037927935, i128 72057594037927935>
  %581 = extractelement <2 x i128> %562, i32 1
  %582 = lshr i128 %581, 56
  %583 = and <2 x i128> %562, <i128 72057594037927935, i128 72057594037927935>
  %584 = lshr <2 x i128> %562, <i128 72, i128 72>
  %585 = add nuw nsw <2 x i128> %584, %580
  %586 = shl nuw nsw i128 %582, 40
  %587 = insertelement <2 x i128> undef, i128 %565, i32 0
  %588 = insertelement <2 x i128> %587, i128 %586, i32 1
  %589 = and <2 x i128> %588, <i128 72056494526300160, i128 72056494526300160>
  %590 = add nsw i128 %573, -170141183460469231713240559642174521088
  %591 = sub nuw i128 %590, %576
  %592 = sub i128 %591, %518
  %593 = insertelement <2 x i128> undef, i128 %568, i32 0
  %594 = insertelement <2 x i128> %593, i128 %592, i32 1
  %595 = insertelement <2 x i128> undef, i128 %564, i32 0
  %596 = insertelement <2 x i128> %595, i128 %582, i32 1
  %597 = sub <2 x i128> %594, %596
  %598 = lshr <2 x i128> %597, <i128 56, i128 56>
  %599 = add nuw i128 %574, 170141183460469229388914496082775375616
  %600 = add i128 %599, %575
  %601 = sub i128 %600, %577
  %602 = sub i128 %601, %514
  %603 = insertelement <2 x i128> undef, i128 %572, i32 0
  %604 = insertelement <2 x i128> %603, i128 %602, i32 1
  %605 = insertelement <2 x i128> undef, i128 %490, i32 0
  %606 = insertelement <2 x i128> %605, i128 %579, i32 1
  %607 = add <2 x i128> %604, %606
  %608 = add <2 x i128> %607, %589
  %609 = add <2 x i128> %608, %598
  %610 = trunc <2 x i128> %597 to <2 x i64>
  %611 = and <2 x i64> %610, <i64 72057594037927935, i64 72057594037927935>
  %612 = lshr <2 x i128> %609, <i128 56, i128 56>
  %613 = add nuw nsw <2 x i128> %585, %612
  %614 = trunc <2 x i128> %609 to <2 x i64>
  %615 = and <2 x i64> %614, <i64 72057594037927935, i64 72057594037927935>
  %616 = lshr <2 x i128> %613, <i128 56, i128 56>
  %617 = add nuw nsw <2 x i128> %616, %583
  %618 = trunc <2 x i128> %613 to <2 x i64>
  %619 = and <2 x i64> %618, <i64 72057594037927935, i64 72057594037927935>
  %620 = trunc <2 x i128> %617 to <2 x i64>
  %621 = or <2 x i64> %615, %611
  %622 = or <2 x i64> %621, %619
  %623 = or <2 x i64> %622, %620
  %624 = add nsw <2 x i64> %623, <i64 -1, i64 -1>
  %625 = xor <2 x i64> %611, <i64 1, i64 1>
  %626 = xor <2 x i64> %615, <i64 72056494526300160, i64 72056494526300160>
  %627 = or <2 x i64> %626, %625
  %628 = xor <2 x i64> %619, <i64 72057594037927935, i64 72057594037927935>
  %629 = or <2 x i64> %627, %628
  %630 = xor <2 x i64> %620, <i64 72057594037927935, i64 72057594037927935>
  %631 = or <2 x i64> %629, %630
  %632 = add nsw <2 x i64> %631, <i64 -1, i64 -1>
  %633 = xor <2 x i64> %611, <i64 2, i64 2>
  %634 = xor <2 x i64> %615, <i64 72055395014672384, i64 72055395014672384>
  %635 = or <2 x i64> %634, %633
  %636 = or <2 x i64> %635, %628
  %637 = xor <2 x i64> %620, <i64 144115188075855871, i64 144115188075855871>
  %638 = or <2 x i64> %636, %637
  %639 = add nsw <2 x i64> %638, <i64 -1, i64 -1>
  %640 = or <2 x i64> %632, %624
  %641 = or <2 x i64> %640, %639
  %642 = shufflevector <2 x i64> %302, <2 x i64> %11, <2 x i32> <i32 1, i32 3>
  %643 = shufflevector <2 x i64> %302, <2 x i64> %11, <2 x i32> <i32 0, i32 2>
  %644 = or <2 x i64> %642, %643
  %645 = shufflevector <2 x i64> %309, <2 x i64> %18, <2 x i32> <i32 0, i32 2>
  %646 = or <2 x i64> %644, %645
  %647 = shufflevector <2 x i64> %309, <2 x i64> %18, <2 x i32> <i32 1, i32 3>
  %648 = or <2 x i64> %646, %647
  %649 = add nsw <2 x i64> %648, <i64 -1, i64 -1>
  %650 = xor <2 x i64> %643, <i64 1, i64 1>
  %651 = xor <2 x i64> %642, <i64 72056494526300160, i64 72056494526300160>
  %652 = or <2 x i64> %651, %650
  %653 = xor <2 x i64> %645, <i64 72057594037927935, i64 72057594037927935>
  %654 = or <2 x i64> %652, %653
  %655 = xor <2 x i64> %647, <i64 72057594037927935, i64 72057594037927935>
  %656 = or <2 x i64> %654, %655
  %657 = add nsw <2 x i64> %656, <i64 -1, i64 -1>
  %658 = xor <2 x i64> %643, <i64 2, i64 2>
  %659 = xor <2 x i64> %642, <i64 72055395014672384, i64 72055395014672384>
  %660 = or <2 x i64> %659, %658
  %661 = or <2 x i64> %660, %653
  %662 = xor <2 x i64> %647, <i64 144115188075855871, i64 144115188075855871>
  %663 = or <2 x i64> %661, %662
  %664 = add nsw <2 x i64> %663, <i64 -1, i64 -1>
  %665 = or <2 x i64> %657, %649
  %666 = or <2 x i64> %665, %664
  %667 = extractelement <2 x i64> %641, i32 0
  %668 = extractelement <2 x i64> %641, i32 1
  %669 = and i64 %667, %668
  %670 = icmp sgt i64 %669, -1
  %671 = extractelement <2 x i64> %666, i32 0
  %672 = extractelement <2 x i64> %666, i32 1
  %673 = or i64 %671, %672
  %674 = icmp slt i64 %673, 0
  %675 = or i1 %674, %670
  br i1 %675, label %677, label %676

676:                                              ; preds = %9
  tail call fastcc void @0(i64* %0, i64* %1, i64* %2, i64* nonnull %3, i64* nonnull %4, i64* nonnull %5)
  br label %1283

677:                                              ; preds = %9
  %678 = mul nuw i128 %312, %21
  %679 = mul nuw i128 %312, %27
  %680 = mul nuw i128 %318, %21
  %681 = mul nuw i128 %312, %36
  %682 = mul nuw i128 %318, %27
  %683 = mul nuw i128 %327, %21
  %684 = mul nuw i128 %312, %30
  %685 = mul nuw i128 %318, %36
  %686 = mul nuw i128 %327, %27
  %687 = mul nuw i128 %321, %21
  %688 = mul nuw i128 %318, %30
  %689 = mul nuw i128 %327, %36
  %690 = add i128 %689, %688
  %691 = mul nuw i128 %321, %27
  %692 = add i128 %690, %691
  %693 = mul nuw i128 %327, %30
  %694 = mul nuw i128 %321, %36
  %695 = add i128 %694, %693
  %696 = mul nuw i128 %321, %30
  %697 = add i128 %678, -170141183460469231731687303715884072960
  %698 = lshr i128 %696, 16
  %699 = add i128 %692, %698
  %700 = shl i128 %696, 40
  %701 = and i128 %700, 72056494526300160
  %702 = lshr i128 %695, 16
  %703 = shl i128 %695, 40
  %704 = and i128 %703, 72056494526300160
  %705 = lshr i128 %699, 16
  %706 = add i128 %681, 170141183460469229370504062281061498880
  %707 = add i128 %706, %682
  %708 = add i128 %707, %683
  %709 = sub i128 %708, %696
  %710 = add i128 %709, %704
  %711 = add i128 %710, %705
  %712 = shl i128 %699, 40
  %713 = and i128 %712, 72056494526300160
  %714 = sub i128 %697, %699
  %715 = lshr i128 %711, 56
  %716 = add i128 %685, %684
  %717 = add i128 %716, %686
  %718 = add i128 %717, %687
  %719 = add i128 %718, %701
  %720 = add i128 %719, %702
  %721 = add i128 %720, %715
  %722 = and i128 %711, 72057594037927935
  %723 = lshr i128 %721, 56
  %724 = and i128 %721, 72057594037927935
  %725 = lshr i128 %721, 72
  %726 = add nuw nsw i128 %725, %722
  %727 = shl nuw nsw i128 %723, 40
  %728 = and i128 %727, 72056494526300160
  %729 = sub i128 %714, %723
  %730 = lshr i128 %729, 56
  %731 = add i128 %679, 170141183460469229370468033484042534912
  %732 = add i128 %731, %680
  %733 = sub i128 %732, %695
  %734 = add i128 %733, %713
  %735 = add i128 %734, %728
  %736 = add i128 %735, %730
  %737 = lshr i128 %736, 56
  %738 = add nuw nsw i128 %726, %737
  %739 = lshr i128 %738, 56
  %740 = add nuw nsw i128 %739, %724
  %741 = extractelement <2 x i64> %611, i32 1
  %742 = zext i64 %741 to i128
  %743 = and i128 %729, 72057594037927935
  %744 = mul nuw nsw i128 %743, %742
  %745 = and i128 %736, 72057594037927935
  %746 = mul nuw nsw i128 %745, %742
  %747 = extractelement <2 x i64> %615, i32 1
  %748 = zext i64 %747 to i128
  %749 = mul nuw nsw i128 %743, %748
  %750 = and i128 %738, 72057594037927935
  %751 = mul nuw nsw i128 %750, %742
  %752 = mul nuw nsw i128 %745, %748
  %753 = extractelement <2 x i64> %619, i32 1
  %754 = zext i64 %753 to i128
  %755 = mul nuw nsw i128 %743, %754
  %756 = mul nuw nsw i128 %740, %742
  %757 = mul nuw nsw i128 %750, %748
  %758 = mul nuw nsw i128 %745, %754
  %759 = extractelement <2 x i128> %617, i32 1
  %760 = mul nuw nsw i128 %759, %743
  %761 = mul nuw nsw i128 %740, %748
  %762 = mul nuw nsw i128 %750, %754
  %763 = mul nuw nsw i128 %759, %745
  %764 = mul nuw nsw i128 %740, %754
  %765 = mul nuw nsw i128 %759, %750
  %766 = add nuw nsw i128 %764, %765
  %767 = mul nuw nsw i128 %759, %740
  %768 = add nsw i128 %744, -170141183460469231731687303715884072960
  %769 = lshr i128 %767, 16
  %770 = add nuw nsw i128 %763, %761
  %771 = add nuw i128 %770, %762
  %772 = add i128 %771, %769
  %773 = shl i128 %767, 40
  %774 = and i128 %773, 72056494526300160
  %775 = lshr i128 %766, 16
  %776 = shl i128 %766, 40
  %777 = and i128 %776, 72056494526300160
  %778 = lshr i128 %772, 16
  %779 = add nuw i128 %751, 170141183460469229370504062281061498880
  %780 = add i128 %779, %752
  %781 = sub i128 %780, %767
  %782 = add i128 %781, %755
  %783 = add i128 %782, %777
  %784 = add i128 %783, %778
  %785 = shl i128 %772, 40
  %786 = and i128 %785, 72056494526300160
  %787 = sub i128 %768, %772
  %788 = lshr i128 %784, 56
  %789 = add nuw nsw i128 %757, %756
  %790 = add nuw i128 %789, %760
  %791 = add i128 %790, %758
  %792 = add i128 %791, %774
  %793 = add i128 %792, %775
  %794 = add i128 %793, %788
  %795 = and i128 %784, 72057594037927935
  %796 = lshr i128 %794, 56
  %797 = and i128 %794, 72057594037927935
  %798 = lshr i128 %794, 72
  %799 = add nuw nsw i128 %798, %795
  %800 = shl nuw nsw i128 %796, 40
  %801 = and i128 %800, 72056494526300160
  %802 = sub i128 %787, %796
  %803 = lshr i128 %802, 56
  %804 = add nuw i128 %746, 170141183460469229370468033484042534912
  %805 = add i128 %804, %749
  %806 = sub i128 %805, %766
  %807 = add i128 %806, %786
  %808 = add i128 %807, %801
  %809 = add i128 %808, %803
  %810 = lshr i128 %809, 56
  %811 = add nuw nsw i128 %799, %810
  %812 = insertelement <2 x i128> undef, i128 %802, i32 0
  %813 = insertelement <2 x i128> %812, i128 %809, i32 1
  %814 = trunc <2 x i128> %813 to <2 x i64>
  %815 = and <2 x i64> %814, <i64 72057594037927935, i64 72057594037927935>
  %816 = lshr i128 %811, 56
  %817 = add nuw nsw i128 %816, %797
  %818 = trunc i128 %811 to i64
  %819 = and i64 %818, 72057594037927935
  %820 = trunc i128 %817 to i64
  %821 = shl nuw nsw i64 %741, 1
  %822 = shl nuw nsw i64 %747, 1
  %823 = shl nuw nsw i64 %753, 1
  %824 = mul nuw nsw i128 %742, %742
  %825 = zext i64 %822 to i128
  %826 = mul nuw nsw i128 %825, %742
  %827 = zext i64 %823 to i128
  %828 = mul nuw nsw i128 %827, %742
  %829 = mul nuw nsw i128 %748, %748
  %830 = zext i64 %821 to i128
  %831 = mul nuw nsw i128 %759, %830
  %832 = mul nuw nsw i128 %827, %748
  %833 = add nuw nsw i128 %832, %831
  %834 = mul nuw nsw i128 %759, %825
  %835 = mul nuw nsw i128 %754, %754
  %836 = add nuw nsw i128 %835, %834
  %837 = mul nuw nsw i128 %759, %827
  %838 = mul nuw nsw i128 %759, %759
  %839 = add nsw i128 %824, -170141183460469231731687303715884072960
  %840 = add nuw i128 %826, 170141183460469229370468033484042534912
  %841 = lshr i128 %838, 16
  %842 = add nuw i128 %836, %841
  %843 = shl i128 %838, 40
  %844 = and i128 %843, 72056494526300160
  %845 = add nuw i128 %833, %844
  %846 = lshr i128 %837, 16
  %847 = add i128 %845, %846
  %848 = shl i128 %837, 40
  %849 = and i128 %848, 72055395014672384
  %850 = sub i128 %840, %837
  %851 = lshr i128 %842, 16
  %852 = add nuw i128 %829, 170141183460469229370504062281061498880
  %853 = sub i128 %852, %838
  %854 = add i128 %853, %828
  %855 = add i128 %854, %849
  %856 = add i128 %855, %851
  %857 = shl i128 %842, 40
  %858 = and i128 %857, 72056494526300160
  %859 = add i128 %850, %858
  %860 = sub i128 %839, %842
  %861 = lshr i128 %856, 56
  %862 = add i128 %847, %861
  %863 = and i128 %856, 72057594037927935
  %864 = lshr i128 %862, 56
  %865 = and i128 %862, 72057594037927935
  %866 = lshr i128 %862, 72
  %867 = add nuw nsw i128 %866, %863
  %868 = shl nuw nsw i128 %864, 40
  %869 = and i128 %868, 72056494526300160
  %870 = add i128 %859, %869
  %871 = sub i128 %860, %864
  %872 = lshr i128 %871, 56
  %873 = add i128 %870, %872
  %874 = lshr i128 %873, 56
  %875 = add nuw nsw i128 %867, %874
  %876 = lshr i128 %875, 56
  %877 = add nuw nsw i128 %876, %865
  %878 = and i128 %871, 72057594037927935
  %879 = mul nuw nsw i128 %878, %742
  %880 = mul nuw nsw i128 %878, %748
  %881 = and i128 %873, 72057594037927935
  %882 = mul nuw nsw i128 %881, %742
  %883 = mul nuw nsw i128 %878, %754
  %884 = mul nuw nsw i128 %881, %748
  %885 = and i128 %875, 72057594037927935
  %886 = mul nuw nsw i128 %885, %742
  %887 = mul nuw nsw i128 %878, %759
  %888 = mul nuw nsw i128 %881, %754
  %889 = mul nuw nsw i128 %885, %748
  %890 = mul nuw nsw i128 %877, %742
  %891 = mul nuw nsw i128 %881, %759
  %892 = mul nuw nsw i128 %885, %754
  %893 = add nuw nsw i128 %892, %891
  %894 = mul nuw nsw i128 %877, %748
  %895 = add nuw i128 %893, %894
  %896 = mul nuw nsw i128 %885, %759
  %897 = mul nuw nsw i128 %877, %754
  %898 = add nuw nsw i128 %897, %896
  %899 = mul nuw nsw i128 %877, %759
  %900 = add nsw i128 %879, -170141183460469231731687303715884072960
  %901 = lshr i128 %899, 16
  %902 = add i128 %895, %901
  %903 = shl i128 %899, 40
  %904 = and i128 %903, 72056494526300160
  %905 = lshr i128 %898, 16
  %906 = shl i128 %898, 40
  %907 = and i128 %906, 72056494526300160
  %908 = lshr i128 %902, 16
  %909 = add nuw i128 %883, 170141183460469229370504062281061498880
  %910 = add i128 %909, %884
  %911 = add i128 %910, %886
  %912 = sub i128 %911, %899
  %913 = add i128 %912, %907
  %914 = add i128 %913, %908
  %915 = shl i128 %902, 40
  %916 = and i128 %915, 72056494526300160
  %917 = sub i128 %900, %902
  %918 = lshr i128 %914, 56
  %919 = add nuw nsw i128 %888, %887
  %920 = add nuw i128 %919, %889
  %921 = add i128 %920, %890
  %922 = add i128 %921, %904
  %923 = add i128 %922, %905
  %924 = add i128 %923, %918
  %925 = and i128 %914, 72057594037927935
  %926 = lshr i128 %924, 56
  %927 = and i128 %924, 72057594037927935
  %928 = lshr i128 %924, 72
  %929 = add nuw nsw i128 %928, %925
  %930 = shl nuw nsw i128 %926, 40
  %931 = and i128 %930, 72056494526300160
  %932 = sub i128 %917, %926
  %933 = lshr i128 %932, 56
  %934 = add nuw i128 %880, 170141183460469229370468033484042534912
  %935 = add i128 %934, %882
  %936 = sub i128 %935, %898
  %937 = add i128 %936, %916
  %938 = add i128 %937, %931
  %939 = add i128 %938, %933
  %940 = lshr i128 %939, 56
  %941 = add nuw nsw i128 %929, %940
  %942 = lshr i128 %941, 56
  %943 = add nuw nsw i128 %942, %927
  %944 = mul nuw nsw i128 %878, %576
  %945 = mul nuw nsw i128 %881, %576
  %946 = mul nuw nsw i128 %878, %577
  %947 = mul nuw nsw i128 %885, %576
  %948 = mul nuw nsw i128 %881, %577
  %949 = mul nuw nsw i128 %878, %516
  %950 = mul nuw nsw i128 %877, %576
  %951 = mul nuw nsw i128 %885, %577
  %952 = mul nuw nsw i128 %881, %516
  %953 = mul nuw nsw i128 %878, %300
  %954 = mul nuw nsw i128 %877, %577
  %955 = mul nuw nsw i128 %885, %516
  %956 = mul nuw nsw i128 %881, %300
  %957 = mul nuw nsw i128 %877, %516
  %958 = mul nuw nsw i128 %885, %300
  %959 = add nuw nsw i128 %957, %958
  %960 = mul nuw nsw i128 %877, %300
  %961 = add nsw i128 %944, -170141183460469231731687303715884072960
  %962 = lshr i128 %960, 16
  %963 = add nuw nsw i128 %955, %956
  %964 = add nuw i128 %963, %954
  %965 = add i128 %964, %962
  %966 = shl i128 %960, 40
  %967 = and i128 %966, 72056494526300160
  %968 = lshr i128 %959, 16
  %969 = shl i128 %959, 40
  %970 = and i128 %969, 72056494526300160
  %971 = lshr i128 %965, 16
  %972 = add nuw i128 %949, 170141183460469229370504062281061498880
  %973 = add i128 %972, %948
  %974 = add i128 %973, %947
  %975 = sub i128 %974, %960
  %976 = add i128 %975, %970
  %977 = add i128 %976, %971
  %978 = shl i128 %965, 40
  %979 = and i128 %978, 72056494526300160
  %980 = sub i128 %961, %965
  %981 = lshr i128 %977, 56
  %982 = add nuw nsw i128 %952, %953
  %983 = add nuw i128 %982, %951
  %984 = add i128 %983, %950
  %985 = add i128 %984, %967
  %986 = add i128 %985, %968
  %987 = add i128 %986, %981
  %988 = and i128 %977, 72057594037927935
  %989 = lshr i128 %987, 56
  %990 = and i128 %987, 72057594037927935
  %991 = lshr i128 %987, 72
  %992 = add nuw nsw i128 %991, %988
  %993 = shl nuw nsw i128 %989, 40
  %994 = and i128 %993, 72056494526300160
  %995 = sub i128 %980, %989
  %996 = lshr i128 %995, 56
  %997 = add nuw i128 %946, 170141183460469229370468033484042534912
  %998 = add i128 %997, %945
  %999 = sub i128 %998, %959
  %1000 = add i128 %999, %979
  %1001 = add i128 %1000, %994
  %1002 = add i128 %1001, %996
  %1003 = trunc i128 %995 to i64
  %1004 = and i64 %1003, 72057594037927935
  %1005 = lshr i128 %1002, 56
  %1006 = add nuw nsw i128 %992, %1005
  %1007 = trunc i128 %1002 to i64
  %1008 = and i64 %1007, 72057594037927935
  %1009 = lshr i128 %1006, 56
  %1010 = add nuw nsw i128 %1009, %990
  %1011 = trunc i128 %1006 to i64
  %1012 = and i64 %1011, 72057594037927935
  %1013 = trunc i128 %1010 to i64
  %1014 = and i128 %932, 72057594037927935
  %1015 = mul nuw nsw i128 %1014, %472
  %1016 = and i128 %939, 72057594037927935
  %1017 = and i128 %941, 72057594037927935
  %1018 = mul nuw nsw i128 %943, %224
  %1019 = extractelement <2 x i64> %611, i32 0
  %1020 = shl nuw nsw i64 %1019, 1
  %1021 = extractelement <2 x i64> %615, i32 0
  %1022 = shl nuw nsw i64 %1021, 1
  %1023 = extractelement <2 x i64> %619, i32 0
  %1024 = shl nuw nsw i64 %1023, 1
  %1025 = zext i64 %1019 to i128
  %1026 = mul nuw nsw i128 %1025, %1025
  %1027 = zext i64 %1022 to i128
  %1028 = mul nuw nsw i128 %1027, %1025
  %1029 = zext i64 %1024 to i128
  %1030 = mul nuw nsw i128 %1029, %1025
  %1031 = zext i64 %1021 to i128
  %1032 = mul nuw nsw i128 %1031, %1031
  %1033 = zext i64 %1020 to i128
  %1034 = extractelement <2 x i128> %617, i32 0
  %1035 = mul nuw nsw i128 %1034, %1033
  %1036 = mul nuw nsw i128 %1029, %1031
  %1037 = mul nuw nsw i128 %1034, %1027
  %1038 = zext i64 %1023 to i128
  %1039 = mul nuw nsw i128 %1038, %1038
  %1040 = add nuw nsw i128 %1039, %1037
  %1041 = mul nuw nsw i128 %1034, %1029
  %1042 = mul nuw nsw i128 %1034, %1034
  %1043 = shl nuw nsw i64 %1004, 1
  %1044 = shl nuw nsw i64 %1008, 1
  %1045 = shl nuw nsw i64 %1012, 1
  %1046 = shl nuw nsw i64 %1013, 1
  %1047 = zext i64 %1043 to i128
  %1048 = zext i64 %1044 to i128
  %1049 = zext i64 %1045 to i128
  %1050 = zext i64 %1046 to i128
  %1051 = lshr i128 %1042, 16
  %1052 = add nuw i128 %1040, %1051
  %1053 = shl i128 %1042, 40
  %1054 = and i128 %1053, 72056494526300160
  %1055 = lshr i128 %1041, 16
  %1056 = shl i128 %1041, 40
  %1057 = and i128 %1056, 72055395014672384
  %1058 = lshr i128 %1052, 16
  %1059 = add nuw i128 %1032, 170141183460469229407397550428480601600
  %1060 = sub i128 %1059, %1042
  %1061 = add i128 %1060, %1030
  %1062 = add i128 %1061, %1057
  %1063 = add i128 %1062, %1058
  %1064 = sub i128 %1063, %1017
  %1065 = sub i128 %1064, %1049
  %1066 = shl i128 %1052, 40
  %1067 = and i128 %1066, 72056494526300160
  %1068 = lshr i128 %1065, 56
  %1069 = add nuw nsw i128 %1035, 36893488147419102720
  %1070 = add nuw i128 %1069, %1036
  %1071 = add i128 %1070, %1054
  %1072 = add i128 %1071, %1055
  %1073 = sub i128 %1072, %943
  %1074 = sub i128 %1073, %1050
  %1075 = add i128 %1074, %1068
  %1076 = and i128 %1065, 72057594037927935
  %1077 = lshr i128 %1075, 56
  %1078 = and i128 %1075, 72057594037927935
  %1079 = lshr i128 %1075, 72
  %1080 = add nuw nsw i128 %1079, %1076
  %1081 = shl nuw nsw i128 %1077, 40
  %1082 = and i128 %1081, 72056494526300160
  %1083 = add nsw i128 %1026, -170141183460469231694793815568464969216
  %1084 = sub i128 %1083, %1052
  %1085 = sub i128 %1084, %1014
  %1086 = sub i128 %1085, %1047
  %1087 = sub i128 %1086, %1077
  %1088 = lshr i128 %1087, 56
  %1089 = add nuw i128 %1028, 170141183460469229407360958681508216320
  %1090 = sub i128 %1089, %1041
  %1091 = add i128 %1090, %1067
  %1092 = sub i128 %1091, %1016
  %1093 = sub i128 %1092, %1048
  %1094 = add i128 %1093, %1082
  %1095 = add i128 %1094, %1088
  %1096 = lshr i128 %1095, 56
  %1097 = add nuw nsw i128 %1080, %1096
  %1098 = insertelement <2 x i128> undef, i128 %1087, i32 0
  %1099 = insertelement <2 x i128> %1098, i128 %1095, i32 1
  %1100 = trunc <2 x i128> %1099 to <2 x i64>
  %1101 = and <2 x i64> %1100, <i64 72057594037927935, i64 72057594037927935>
  %1102 = lshr i128 %1097, 56
  %1103 = add nuw nsw i128 %1102, %1078
  %1104 = trunc i128 %1097 to i64
  %1105 = and i64 %1104, 72057594037927935
  %1106 = trunc i128 %1103 to i64
  %1107 = add nuw nsw i64 %1004, 288230376151711748
  %1108 = add nuw nsw i64 %1008, 288225978105200636
  %1109 = add nuw nsw i64 %1012, 288230376151711740
  %1110 = add nuw nsw i64 %1013, 288230376151711740
  %1111 = extractelement <2 x i64> %1101, i32 0
  %1112 = sub nsw i64 %1107, %1111
  %1113 = extractelement <2 x i64> %1101, i32 1
  %1114 = sub nsw i64 %1108, %1113
  %1115 = sub nsw i64 %1109, %1105
  %1116 = sub i64 %1110, %1106
  %1117 = zext i64 %1112 to i128
  %1118 = mul nuw nsw i128 %1117, %1025
  %1119 = zext i64 %1114 to i128
  %1120 = mul nuw nsw i128 %1119, %1025
  %1121 = mul nuw nsw i128 %1117, %1031
  %1122 = zext i64 %1115 to i128
  %1123 = mul nuw nsw i128 %1122, %1025
  %1124 = mul nuw nsw i128 %1119, %1031
  %1125 = mul nuw nsw i128 %1117, %1038
  %1126 = zext i64 %1116 to i128
  %1127 = mul nuw nsw i128 %1126, %1025
  %1128 = mul nuw nsw i128 %1122, %1031
  %1129 = mul nuw nsw i128 %1119, %1038
  %1130 = mul nuw nsw i128 %1034, %1117
  %1131 = mul nuw nsw i128 %1126, %1031
  %1132 = mul nuw nsw i128 %1122, %1038
  %1133 = mul nuw nsw i128 %1034, %1119
  %1134 = mul nuw nsw i128 %1126, %1038
  %1135 = mul nuw nsw i128 %1034, %1122
  %1136 = mul nuw nsw i128 %1034, %1126
  %1137 = or i128 %1118, 1329227995784915872903807060280344576
  %1138 = mul nuw nsw i128 %1014, %473
  %1139 = mul nuw nsw i128 %1016, %472
  %1140 = mul nuw nsw i128 %1014, %474
  %1141 = mul nuw nsw i128 %1016, %473
  %1142 = mul nuw nsw i128 %1017, %472
  %1143 = mul nuw nsw i128 %1014, %224
  %1144 = mul nuw nsw i128 %1016, %474
  %1145 = mul nuw nsw i128 %1017, %473
  %1146 = mul nuw nsw i128 %943, %472
  %1147 = mul nuw nsw i128 %1016, %224
  %1148 = mul nuw nsw i128 %1017, %474
  %1149 = mul nuw nsw i128 %943, %473
  %1150 = mul nuw nsw i128 %1017, %224
  %1151 = mul nuw nsw i128 %943, %474
  %1152 = add nuw nsw i128 %1151, %1150
  %1153 = sub nsw i128 1329227995784915854457062986570792960, %1152
  %1154 = add i128 %1153, %1135
  %1155 = add i128 %1154, %1134
  %1156 = sub nsw i128 1329227995784915854457062986570792960, %1018
  %1157 = add i128 %1156, %1136
  %1158 = lshr i128 %1157, 16
  %1159 = add nuw nsw i128 %1148, %1147
  %1160 = add nuw i128 %1159, %1149
  %1161 = sub i128 1329207713375312202786639039319506944, %1160
  %1162 = add i128 %1161, %1133
  %1163 = add i128 %1162, %1132
  %1164 = add i128 %1163, %1131
  %1165 = add i128 %1164, %1158
  %1166 = shl i128 %1157, 40
  %1167 = and i128 %1166, 72056494526300160
  %1168 = lshr i128 %1155, 16
  %1169 = shl i128 %1155, 40
  %1170 = and i128 %1169, 72056494526300160
  %1171 = lshr i128 %1165, 16
  %1172 = add nuw nsw i128 %1141, %1140
  %1173 = add nuw nsw i128 %1172, %1142
  %1174 = sub nuw i128 -168811955464684318238413482164135919616, %1173
  %1175 = add nsw i128 %1174, %1125
  %1176 = add nsw i128 %1175, %1124
  %1177 = add i128 %1176, %1123
  %1178 = sub i128 %1177, %1157
  %1179 = add i128 %1178, %1170
  %1180 = add i128 %1179, %1171
  %1181 = shl i128 %1165, 40
  %1182 = and i128 %1181, 72056494526300160
  %1183 = lshr i128 %1180, 56
  %1184 = add nuw nsw i128 %1144, %1143
  %1185 = add nuw i128 %1184, %1145
  %1186 = add i128 %1185, %1146
  %1187 = sub i128 1329227995784915872903807060280344576, %1186
  %1188 = add i128 %1187, %1130
  %1189 = add i128 %1188, %1129
  %1190 = add i128 %1189, %1128
  %1191 = add i128 %1190, %1127
  %1192 = add i128 %1191, %1168
  %1193 = add i128 %1192, %1167
  %1194 = add i128 %1193, %1183
  %1195 = and i128 %1180, 72057594037927935
  %1196 = lshr i128 %1194, 56
  %1197 = and i128 %1194, 72057594037927935
  %1198 = lshr i128 %1194, 72
  %1199 = add nuw nsw i128 %1198, %1195
  %1200 = shl nuw nsw i128 %1196, 40
  %1201 = and i128 %1200, 72056494526300160
  %1202 = sub nuw i128 -170141183460469231731687303715884072960, %1015
  %1203 = add i128 %1202, %1137
  %1204 = sub i128 %1203, %1165
  %1205 = sub i128 %1204, %1196
  %1206 = lshr i128 %1205, 56
  %1207 = add nuw nsw i128 %1139, %1138
  %1208 = sub nuw i128 -168811955464684318238449510961154883584, %1207
  %1209 = add nsw i128 %1208, %1121
  %1210 = add nsw i128 %1209, %1120
  %1211 = sub i128 %1210, %1155
  %1212 = add i128 %1211, %1182
  %1213 = add i128 %1212, %1201
  %1214 = add i128 %1213, %1206
  %1215 = lshr i128 %1214, 56
  %1216 = add nuw nsw i128 %1199, %1215
  %1217 = insertelement <2 x i128> undef, i128 %1205, i32 0
  %1218 = insertelement <2 x i128> %1217, i128 %1214, i32 1
  %1219 = trunc <2 x i128> %1218 to <2 x i64>
  %1220 = and <2 x i64> %1219, <i64 72057594037927935, i64 72057594037927935>
  %1221 = lshr i128 %1216, 56
  %1222 = add nuw nsw i128 %1221, %1197
  %1223 = trunc i128 %1216 to i64
  %1224 = and i64 %1223, 72057594037927935
  %1225 = trunc i128 %1222 to i64
  %1226 = ashr i64 %671, 63
  %1227 = xor <2 x i64> %1101, %492
  %1228 = insertelement <2 x i64> undef, i64 %1226, i32 0
  %1229 = shufflevector <2 x i64> %1228, <2 x i64> undef, <2 x i32> zeroinitializer
  %1230 = and <2 x i64> %1227, %1229
  %1231 = xor <2 x i64> %1230, %1101
  %1232 = insertelement <2 x i64> undef, i64 %1105, i32 0
  %1233 = insertelement <2 x i64> %1232, i64 %1106, i32 1
  %1234 = xor <2 x i64> %1233, %499
  %1235 = and <2 x i64> %1234, %1229
  %1236 = xor <2 x i64> %1235, %1233
  %1237 = ashr i64 %672, 63
  %1238 = xor <2 x i64> %1231, %226
  %1239 = insertelement <2 x i64> undef, i64 %1237, i32 0
  %1240 = shufflevector <2 x i64> %1239, <2 x i64> undef, <2 x i32> zeroinitializer
  %1241 = and <2 x i64> %1238, %1240
  %1242 = xor <2 x i64> %1241, %1231
  %1243 = xor <2 x i64> %1236, %236
  %1244 = and <2 x i64> %1243, %1240
  %1245 = xor <2 x i64> %1244, %1236
  %1246 = xor <2 x i64> %1220, %439
  %1247 = and <2 x i64> %1246, %1229
  %1248 = xor <2 x i64> %1247, %1220
  %1249 = insertelement <2 x i64> undef, i64 %1224, i32 0
  %1250 = insertelement <2 x i64> %1249, i64 %1225, i32 1
  %1251 = xor <2 x i64> %1250, %450
  %1252 = and <2 x i64> %1251, %1229
  %1253 = xor <2 x i64> %1252, %1250
  %1254 = xor <2 x i64> %1248, %148
  %1255 = and <2 x i64> %1254, %1240
  %1256 = xor <2 x i64> %1255, %1248
  %1257 = xor <2 x i64> %1253, %159
  %1258 = and <2 x i64> %1257, %1240
  %1259 = xor <2 x i64> %1258, %1253
  %1260 = xor <2 x i64> %815, %11
  %1261 = and <2 x i64> %1260, %1229
  %1262 = xor <2 x i64> %1261, %815
  %1263 = insertelement <2 x i64> undef, i64 %819, i32 0
  %1264 = insertelement <2 x i64> %1263, i64 %820, i32 1
  %1265 = xor <2 x i64> %1264, %18
  %1266 = and <2 x i64> %1265, %1229
  %1267 = xor <2 x i64> %1266, %1264
  %1268 = xor <2 x i64> %1262, %302
  %1269 = and <2 x i64> %1268, %1240
  %1270 = xor <2 x i64> %1269, %1262
  %1271 = xor <2 x i64> %1267, %309
  %1272 = and <2 x i64> %1271, %1240
  %1273 = xor <2 x i64> %1272, %1267
  %1274 = bitcast i64* %0 to <2 x i64>*
  store <2 x i64> %1242, <2 x i64>* %1274, align 8
  %1275 = getelementptr inbounds i64, i64* %0, i64 2
  %1276 = bitcast i64* %1275 to <2 x i64>*
  store <2 x i64> %1245, <2 x i64>* %1276, align 8
  %1277 = bitcast i64* %1 to <2 x i64>*
  store <2 x i64> %1256, <2 x i64>* %1277, align 8
  %1278 = getelementptr inbounds i64, i64* %1, i64 2
  %1279 = bitcast i64* %1278 to <2 x i64>*
  store <2 x i64> %1259, <2 x i64>* %1279, align 8
  %1280 = bitcast i64* %2 to <2 x i64>*
  store <2 x i64> %1270, <2 x i64>* %1280, align 8
  %1281 = getelementptr inbounds i64, i64* %2, i64 2
  %1282 = bitcast i64* %1281 to <2 x i64>*
  store <2 x i64> %1273, <2 x i64>* %1282, align 8
  br label %1283

1283:                                             ; preds = %677, %676
  ret void
}

; Function Attrs: argmemonly nounwind willreturn
declare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #1

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jmulsw_nistp224_inner(i64* nocapture %0, i64* nocapture %1, i64* %2, %0* nocapture readnone %3, i64* nocapture readonly %4, i64* nocapture readonly %5, i64* nocapture readonly %6, %1* %7) local_unnamed_addr #0 {
  %9 = tail call i64 @__gmpz_sizeinbase(%1* %7, i32 2) #6
  %10 = trunc i64 %9 to i32
  %11 = sdiv i32 %10, 2
  %12 = sitofp i32 %11 to float
  %13 = sitofp i32 %10 to float
  br label %14

14:                                               ; preds = %14, %8
  %15 = phi float [ %12, %8 ], [ %27, %14 ]
  %16 = phi i32 [ 1, %8 ], [ %17, %14 ]
  %17 = add nuw nsw i32 %16, 1
  %18 = shl i32 1, %16
  %19 = sitofp i32 %18 to float
  %20 = shl i32 2, %16
  %21 = add nsw i32 %20, -1
  %22 = sitofp i32 %21 to float
  %23 = fmul float %13, %22
  %24 = mul nsw i32 %20, %17
  %25 = sitofp i32 %24 to float
  %26 = fdiv float %23, %25
  %27 = fadd float %26, %19
  %28 = fcmp olt float %27, %15
  br i1 %28, label %14, label %29

29:                                               ; preds = %14
  %30 = add nsw i32 %16, -1
  %31 = shl i32 1, %30
  %32 = sext i32 %31 to i64
  %33 = shl nsw i64 %32, 5
  %34 = tail call noalias i8* @malloc(i64 %33) #7
  %35 = bitcast i8* %34 to [4 x i64]*
  %36 = tail call noalias i8* @malloc(i64 %33) #7
  %37 = bitcast i8* %36 to [4 x i64]*
  %38 = tail call noalias i8* @malloc(i64 %33) #7
  %39 = bitcast i8* %38 to [4 x i64]*
  %40 = add nsw i32 %31, -1
  %41 = sext i32 %40 to i64
  %42 = getelementptr inbounds [4 x i64], [4 x i64]* %35, i64 %41, i64 0
  %43 = getelementptr inbounds [4 x i64], [4 x i64]* %37, i64 %41, i64 0
  %44 = getelementptr inbounds [4 x i64], [4 x i64]* %39, i64 %41, i64 0
  tail call fastcc void @0(i64* %42, i64* %43, i64* %44, i64* %4, i64* %5, i64* %6)
  %45 = bitcast i64* %4 to <2 x i64>*
  %46 = load <2 x i64>, <2 x i64>* %45, align 8
  %47 = bitcast i8* %34 to <2 x i64>*
  store <2 x i64> %46, <2 x i64>* %47, align 8
  %48 = getelementptr inbounds i64, i64* %4, i64 2
  %49 = getelementptr inbounds i8, i8* %34, i64 16
  %50 = bitcast i64* %48 to <2 x i64>*
  %51 = load <2 x i64>, <2 x i64>* %50, align 8
  %52 = bitcast i8* %49 to <2 x i64>*
  store <2 x i64> %51, <2 x i64>* %52, align 8
  %53 = bitcast i64* %5 to <2 x i64>*
  %54 = load <2 x i64>, <2 x i64>* %53, align 8
  %55 = bitcast i8* %36 to <2 x i64>*
  store <2 x i64> %54, <2 x i64>* %55, align 8
  %56 = getelementptr inbounds i64, i64* %5, i64 2
  %57 = getelementptr inbounds i8, i8* %36, i64 16
  %58 = bitcast i64* %56 to <2 x i64>*
  %59 = load <2 x i64>, <2 x i64>* %58, align 8
  %60 = bitcast i8* %57 to <2 x i64>*
  store <2 x i64> %59, <2 x i64>* %60, align 8
  %61 = bitcast i64* %6 to <2 x i64>*
  %62 = load <2 x i64>, <2 x i64>* %61, align 8
  %63 = bitcast i8* %38 to <2 x i64>*
  store <2 x i64> %62, <2 x i64>* %63, align 8
  %64 = getelementptr inbounds i64, i64* %6, i64 2
  %65 = getelementptr inbounds i8, i8* %38, i64 16
  %66 = bitcast i64* %64 to <2 x i64>*
  %67 = load <2 x i64>, <2 x i64>* %66, align 8
  %68 = bitcast i8* %65 to <2 x i64>*
  store <2 x i64> %67, <2 x i64>* %68, align 8
  %69 = icmp sgt i32 %31, 1
  br i1 %69, label %70, label %83

70:                                               ; preds = %29
  %71 = zext i32 %31 to i64
  br label %72

72:                                               ; preds = %72, %70
  %73 = phi i64 [ 1, %70 ], [ %81, %72 ]
  %74 = getelementptr inbounds [4 x i64], [4 x i64]* %35, i64 %73, i64 0
  %75 = getelementptr inbounds [4 x i64], [4 x i64]* %37, i64 %73, i64 0
  %76 = getelementptr inbounds [4 x i64], [4 x i64]* %39, i64 %73, i64 0
  %77 = add nsw i64 %73, -1
  %78 = getelementptr inbounds [4 x i64], [4 x i64]* %35, i64 %77, i64 0
  %79 = getelementptr inbounds [4 x i64], [4 x i64]* %37, i64 %77, i64 0
  %80 = getelementptr inbounds [4 x i64], [4 x i64]* %39, i64 %77, i64 0
  tail call fastcc void @1(i64* nonnull %74, i64* nonnull %75, i64* nonnull %76, i64* %78, i64* %79, i64* %80, i64* %42, i64* %43, i64* %44)
  %81 = add nuw nsw i64 %73, 1
  %82 = icmp eq i64 %81, %71
  br i1 %82, label %83, label %72

83:                                               ; preds = %72, %29
  %84 = bitcast i64* %0 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 8 %84, i8 0, i64 32, i1 false)
  %85 = bitcast i64* %1 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 8 %85, i8 0, i64 32, i1 false)
  %86 = bitcast i64* %2 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 8 %86, i8 0, i64 32, i1 false)
  %87 = icmp sgt i32 %10, -1
  br i1 %87, label %88, label %149

88:                                               ; preds = %83, %142
  %89 = phi i32 [ %143, %142 ], [ %10, %83 ]
  %90 = sext i32 %89 to i64
  br label %91

91:                                               ; preds = %88, %95
  %92 = phi i64 [ %90, %88 ], [ %96, %95 ]
  %93 = tail call i32 @__gmpz_tstbit(%1* %7, i64 %92) #6
  %94 = icmp eq i32 %93, 0
  br i1 %94, label %95, label %98

95:                                               ; preds = %91
  tail call fastcc void @0(i64* %0, i64* %1, i64* %2, i64* %0, i64* %1, i64* %2)
  %96 = add nsw i64 %92, -1
  %97 = icmp sgt i64 %92, 0
  br i1 %97, label %91, label %149

98:                                               ; preds = %91
  %99 = trunc i64 %92 to i32
  %100 = sub nsw i32 %99, %16
  %101 = add nsw i32 %100, 1
  %102 = icmp sgt i32 %101, 0
  %103 = select i1 %102, i32 %101, i32 0
  %104 = icmp slt i32 %103, %99
  br i1 %104, label %105, label %120

105:                                              ; preds = %98
  %106 = zext i32 %103 to i64
  %107 = shl i64 %92, 32
  %108 = ashr exact i64 %107, 32
  br label %109

109:                                              ; preds = %105, %114
  %110 = phi i64 [ %106, %105 ], [ %115, %114 ]
  %111 = phi i32 [ %103, %105 ], [ %116, %114 ]
  %112 = tail call i32 @__gmpz_tstbit(%1* %7, i64 %110) #6
  %113 = icmp eq i32 %112, 0
  br i1 %113, label %114, label %118

114:                                              ; preds = %109
  %115 = add nuw nsw i64 %110, 1
  %116 = add nuw nsw i32 %111, 1
  %117 = icmp slt i64 %115, %108
  br i1 %117, label %109, label %120

118:                                              ; preds = %109
  %119 = trunc i64 %110 to i32
  br label %120

120:                                              ; preds = %114, %118, %98
  %121 = phi i32 [ %103, %98 ], [ %119, %118 ], [ %116, %114 ]
  %122 = icmp slt i32 %121, %99
  br i1 %122, label %123, label %127

123:                                              ; preds = %120
  %124 = shl i64 %92, 32
  %125 = ashr exact i64 %124, 32
  %126 = sext i32 %121 to i64
  br label %130

127:                                              ; preds = %130, %120
  %128 = phi i32 [ 0, %120 ], [ %135, %130 ]
  %129 = icmp sgt i32 %121, %99
  br i1 %129, label %142, label %138

130:                                              ; preds = %123, %130
  %131 = phi i64 [ %125, %123 ], [ %136, %130 ]
  %132 = phi i32 [ 0, %123 ], [ %135, %130 ]
  %133 = shl i32 %132, 1
  %134 = tail call i32 @__gmpz_tstbit(%1* %7, i64 %131) #6
  %135 = or i32 %134, %133
  %136 = add nsw i64 %131, -1
  %137 = icmp sgt i64 %136, %126
  br i1 %137, label %130, label %127

138:                                              ; preds = %127, %138
  %139 = phi i32 [ %140, %138 ], [ %99, %127 ]
  tail call fastcc void @0(i64* %0, i64* %1, i64* %2, i64* %0, i64* %1, i64* %2)
  %140 = add nsw i32 %139, -1
  %141 = icmp sgt i32 %139, %121
  br i1 %141, label %138, label %142

142:                                              ; preds = %138, %127
  %143 = phi i32 [ %99, %127 ], [ %140, %138 ]
  %144 = sext i32 %128 to i64
  %145 = getelementptr inbounds [4 x i64], [4 x i64]* %35, i64 %144, i64 0
  %146 = getelementptr inbounds [4 x i64], [4 x i64]* %37, i64 %144, i64 0
  %147 = getelementptr inbounds [4 x i64], [4 x i64]* %39, i64 %144, i64 0
  tail call fastcc void @1(i64* %0, i64* %1, i64* %2, i64* %0, i64* %1, i64* %2, i64* %145, i64* %146, i64* %147)
  %148 = icmp sgt i32 %143, -1
  br i1 %148, label %88, label %149

149:                                              ; preds = %142, %95, %83
  tail call void @free(i8* %38) #7
  tail call void @free(i8* %36) #7
  tail call void @free(i8* %34) #7
  ret void
}

; Function Attrs: nounwind
declare dso_local noalias i8* @malloc(i64) local_unnamed_addr #4

; Function Attrs: nounwind
declare dso_local void @free(i8* nocapture) local_unnamed_addr #4

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jsmul_init_nistp224_inner(%4* nocapture %0, %0* nocapture readnone %1, i64 %2, i64 %3) local_unnamed_addr #0 {
  %5 = getelementptr inbounds %4, %4* %0, i64 0, i32 1
  store i64 %2, i64* %5, align 8
  %6 = getelementptr inbounds %4, %4* %0, i64 0, i32 2
  %7 = icmp ult i64 %2, %3
  %8 = select i1 %7, i64 %2, i64 %3
  store i64 %8, i64* %6, align 8
  %9 = add i64 %2, -1
  %10 = add i64 %9, %3
  %11 = udiv i64 %10, %3
  %12 = getelementptr inbounds %4, %4* %0, i64 0, i32 3
  store i64 %11, i64* %12, align 8
  %13 = shl i64 %11, 3
  %14 = tail call noalias i8* @malloc(i64 %13) #7
  %15 = getelementptr inbounds %4, %4* %0, i64 0, i32 4
  %16 = bitcast [4 x i64]*** %15 to i8**
  store i8* %14, i8** %16, align 8
  %17 = tail call noalias i8* @malloc(i64 %13) #7
  %18 = getelementptr inbounds %4, %4* %0, i64 0, i32 5
  %19 = bitcast [4 x i64]*** %18 to i8**
  store i8* %17, i8** %19, align 8
  %20 = tail call noalias i8* @malloc(i64 %13) #7
  %21 = getelementptr inbounds %4, %4* %0, i64 0, i32 6
  %22 = bitcast [4 x i64]*** %21 to i8**
  store i8* %20, i8** %22, align 8
  %23 = icmp ult i64 %10, %3
  br i1 %23, label %60, label %24

24:                                               ; preds = %4
  %25 = trunc i64 %3 to i32
  %26 = shl i32 1, %25
  %27 = sext i32 %26 to i64
  %28 = add i64 %11, -1
  br label %29

29:                                               ; preds = %24, %42
  %30 = phi i64 [ %27, %24 ], [ %44, %42 ]
  %31 = phi i64 [ 0, %24 ], [ %58, %42 ]
  %32 = phi i64 [ %3, %24 ], [ %43, %42 ]
  %33 = icmp eq i64 %31, %28
  br i1 %33, label %34, label %42

34:                                               ; preds = %29
  %35 = mul i64 %32, %28
  %36 = sub i64 %2, %35
  %37 = icmp ult i64 %36, %32
  br i1 %37, label %38, label %42

38:                                               ; preds = %34
  %39 = trunc i64 %36 to i32
  %40 = shl i32 1, %39
  %41 = sext i32 %40 to i64
  br label %42

42:                                               ; preds = %38, %34, %29
  %43 = phi i64 [ %36, %38 ], [ %32, %34 ], [ %32, %29 ]
  %44 = phi i64 [ %41, %38 ], [ %30, %34 ], [ %30, %29 ]
  %45 = shl i64 %44, 5
  %46 = tail call noalias i8* @malloc(i64 %45) #7
  %47 = load [4 x i64]**, [4 x i64]*** %15, align 8
  %48 = getelementptr inbounds [4 x i64]*, [4 x i64]** %47, i64 %31
  %49 = bitcast [4 x i64]** %48 to i8**
  store i8* %46, i8** %49, align 8
  %50 = tail call noalias i8* @malloc(i64 %45) #7
  %51 = load [4 x i64]**, [4 x i64]*** %18, align 8
  %52 = getelementptr inbounds [4 x i64]*, [4 x i64]** %51, i64 %31
  %53 = bitcast [4 x i64]** %52 to i8**
  store i8* %50, i8** %53, align 8
  %54 = tail call noalias i8* @malloc(i64 %45) #7
  %55 = load [4 x i64]**, [4 x i64]*** %21, align 8
  %56 = getelementptr inbounds [4 x i64]*, [4 x i64]** %55, i64 %31
  %57 = bitcast [4 x i64]** %56 to i8**
  store i8* %54, i8** %57, align 8
  %58 = add nuw i64 %31, 1
  %59 = icmp ult i64 %58, %11
  br i1 %59, label %29, label %60

60:                                               ; preds = %42, %4
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i32 @vec_jsmul_clear_nistp224_inner(%4* nocapture readonly %0) local_unnamed_addr #0 {
  %2 = getelementptr inbounds %4, %4* %0, i64 0, i32 3
  %3 = load i64, i64* %2, align 8
  %4 = getelementptr inbounds %4, %4* %0, i64 0, i32 2
  %5 = load i64, i64* %4, align 8
  %6 = trunc i64 %5 to i32
  %7 = shl i32 1, %6
  %8 = sext i32 %7 to i64
  %9 = icmp eq i64 %3, 0
  br i1 %9, label %10, label %14

10:                                               ; preds = %1
  %11 = getelementptr inbounds %4, %4* %0, i64 0, i32 4
  %12 = getelementptr inbounds %4, %4* %0, i64 0, i32 5
  %13 = getelementptr inbounds %4, %4* %0, i64 0, i32 6
  br label %49

14:                                               ; preds = %1
  %15 = add i64 %3, -1
  %16 = getelementptr inbounds %4, %4* %0, i64 0, i32 1
  %17 = getelementptr inbounds %4, %4* %0, i64 0, i32 4
  %18 = getelementptr inbounds %4, %4* %0, i64 0, i32 5
  %19 = getelementptr inbounds %4, %4* %0, i64 0, i32 6
  br label %20

20:                                               ; preds = %32, %14
  %21 = phi i64 [ %8, %14 ], [ %34, %32 ]
  %22 = phi i64 [ %5, %14 ], [ %33, %32 ]
  %23 = phi i64 [ 0, %14 ], [ %47, %32 ]
  %24 = icmp eq i64 %23, %15
  br i1 %24, label %25, label %32

25:                                               ; preds = %20
  %26 = load i64, i64* %16, align 8
  %27 = mul i64 %22, %15
  %28 = sub i64 %26, %27
  %29 = trunc i64 %28 to i32
  %30 = shl i32 1, %29
  %31 = sext i32 %30 to i64
  br label %32

32:                                               ; preds = %25, %20
  %33 = phi i64 [ %28, %25 ], [ %22, %20 ]
  %34 = phi i64 [ %31, %25 ], [ %21, %20 ]
  %35 = load [4 x i64]**, [4 x i64]*** %17, align 8
  %36 = getelementptr inbounds [4 x i64]*, [4 x i64]** %35, i64 %23
  %37 = bitcast [4 x i64]** %36 to i8**
  %38 = load i8*, i8** %37, align 8
  tail call void @free(i8* %38) #7
  %39 = load [4 x i64]**, [4 x i64]*** %18, align 8
  %40 = getelementptr inbounds [4 x i64]*, [4 x i64]** %39, i64 %23
  %41 = bitcast [4 x i64]** %40 to i8**
  %42 = load i8*, i8** %41, align 8
  tail call void @free(i8* %42) #7
  %43 = load [4 x i64]**, [4 x i64]*** %19, align 8
  %44 = getelementptr inbounds [4 x i64]*, [4 x i64]** %43, i64 %23
  %45 = bitcast [4 x i64]** %44 to i8**
  %46 = load i8*, i8** %45, align 8
  tail call void @free(i8* %46) #7
  %47 = add nuw i64 %23, 1
  %48 = icmp eq i64 %47, %3
  br i1 %48, label %49, label %20

49:                                               ; preds = %32, %10
  %50 = phi [4 x i64]*** [ %13, %10 ], [ %19, %32 ]
  %51 = phi [4 x i64]*** [ %12, %10 ], [ %18, %32 ]
  %52 = phi [4 x i64]*** [ %11, %10 ], [ %17, %32 ]
  %53 = phi i64 [ %8, %10 ], [ %34, %32 ]
  %54 = bitcast [4 x i64]*** %52 to i8**
  %55 = load i8*, i8** %54, align 8
  tail call void @free(i8* %55) #7
  %56 = bitcast [4 x i64]*** %51 to i8**
  %57 = load i8*, i8** %56, align 8
  tail call void @free(i8* %57) #7
  %58 = bitcast [4 x i64]*** %50 to i8**
  %59 = load i8*, i8** %58, align 8
  tail call void @free(i8* %59) #7
  %60 = trunc i64 %53 to i32
  ret i32 %60
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jsmul_precomp_nistp224_inner(%4* nocapture readonly %0, %0* nocapture readnone %1, [4 x i64]* nocapture readonly %2, [4 x i64]* nocapture readonly %3, [4 x i64]* nocapture readonly %4) local_unnamed_addr #0 {
  %6 = getelementptr inbounds %4, %4* %0, i64 0, i32 2
  %7 = load i64, i64* %6, align 8
  %8 = getelementptr inbounds %4, %4* %0, i64 0, i32 3
  %9 = load i64, i64* %8, align 8
  %10 = icmp eq i64 %9, 0
  br i1 %10, label %123, label %11

11:                                               ; preds = %5
  %12 = trunc i64 %7 to i32
  %13 = shl i32 1, %12
  %14 = sext i32 %13 to i64
  %15 = getelementptr inbounds %4, %4* %0, i64 0, i32 1
  %16 = getelementptr inbounds %4, %4* %0, i64 0, i32 4
  %17 = getelementptr inbounds %4, %4* %0, i64 0, i32 5
  %18 = getelementptr inbounds %4, %4* %0, i64 0, i32 6
  br label %19

19:                                               ; preds = %11, %116
  %20 = phi i64 [ %9, %11 ], [ %121, %116 ]
  %21 = phi [4 x i64]* [ %2, %11 ], [ %117, %116 ]
  %22 = phi [4 x i64]* [ %3, %11 ], [ %118, %116 ]
  %23 = phi [4 x i64]* [ %4, %11 ], [ %119, %116 ]
  %24 = phi i64 [ 0, %11 ], [ %120, %116 ]
  %25 = phi i64 [ %14, %11 ], [ %38, %116 ]
  %26 = phi i64 [ %7, %11 ], [ %37, %116 ]
  %27 = add i64 %20, -1
  %28 = icmp eq i64 %24, %27
  br i1 %28, label %29, label %36

29:                                               ; preds = %19
  %30 = load i64, i64* %15, align 8
  %31 = mul i64 %24, %26
  %32 = sub i64 %30, %31
  %33 = trunc i64 %32 to i32
  %34 = shl i32 1, %33
  %35 = sext i32 %34 to i64
  br label %36

36:                                               ; preds = %29, %19
  %37 = phi i64 [ %32, %29 ], [ %26, %19 ]
  %38 = phi i64 [ %35, %29 ], [ %25, %19 ]
  %39 = load [4 x i64]**, [4 x i64]*** %16, align 8
  %40 = getelementptr inbounds [4 x i64]*, [4 x i64]** %39, i64 %24
  %41 = load [4 x i64]*, [4 x i64]** %40, align 8
  %42 = load [4 x i64]**, [4 x i64]*** %17, align 8
  %43 = getelementptr inbounds [4 x i64]*, [4 x i64]** %42, i64 %24
  %44 = load [4 x i64]*, [4 x i64]** %43, align 8
  %45 = load [4 x i64]**, [4 x i64]*** %18, align 8
  %46 = getelementptr inbounds [4 x i64]*, [4 x i64]** %45, i64 %24
  %47 = load [4 x i64]*, [4 x i64]** %46, align 8
  %48 = bitcast [4 x i64]* %41 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 8 %48, i8 0, i64 32, i1 false)
  %49 = bitcast [4 x i64]* %44 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 8 %49, i8 0, i64 32, i1 false)
  %50 = bitcast [4 x i64]* %47 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 8 %50, i8 0, i64 32, i1 false)
  %51 = icmp eq i64 %37, 0
  br i1 %51, label %52, label %54

52:                                               ; preds = %54, %36
  %53 = icmp ugt i64 %38, 1
  br i1 %53, label %97, label %116

54:                                               ; preds = %36, %54
  %55 = phi i32 [ %94, %54 ], [ 1, %36 ]
  %56 = phi i64 [ %95, %54 ], [ 0, %36 ]
  %57 = sext i32 %55 to i64
  %58 = getelementptr inbounds [4 x i64], [4 x i64]* %41, i64 %57, i64 0
  %59 = getelementptr inbounds [4 x i64], [4 x i64]* %21, i64 %56, i64 0
  %60 = load i64, i64* %59, align 8
  store i64 %60, i64* %58, align 8
  %61 = getelementptr inbounds [4 x i64], [4 x i64]* %21, i64 %56, i64 1
  %62 = load i64, i64* %61, align 8
  %63 = getelementptr inbounds [4 x i64], [4 x i64]* %41, i64 %57, i64 1
  store i64 %62, i64* %63, align 8
  %64 = getelementptr inbounds [4 x i64], [4 x i64]* %21, i64 %56, i64 2
  %65 = load i64, i64* %64, align 8
  %66 = getelementptr inbounds [4 x i64], [4 x i64]* %41, i64 %57, i64 2
  store i64 %65, i64* %66, align 8
  %67 = getelementptr inbounds [4 x i64], [4 x i64]* %21, i64 %56, i64 3
  %68 = load i64, i64* %67, align 8
  %69 = getelementptr inbounds [4 x i64], [4 x i64]* %41, i64 %57, i64 3
  store i64 %68, i64* %69, align 8
  %70 = getelementptr inbounds [4 x i64], [4 x i64]* %44, i64 %57, i64 0
  %71 = getelementptr inbounds [4 x i64], [4 x i64]* %22, i64 %56, i64 0
  %72 = load i64, i64* %71, align 8
  store i64 %72, i64* %70, align 8
  %73 = getelementptr inbounds [4 x i64], [4 x i64]* %22, i64 %56, i64 1
  %74 = load i64, i64* %73, align 8
  %75 = getelementptr inbounds [4 x i64], [4 x i64]* %44, i64 %57, i64 1
  store i64 %74, i64* %75, align 8
  %76 = getelementptr inbounds [4 x i64], [4 x i64]* %22, i64 %56, i64 2
  %77 = load i64, i64* %76, align 8
  %78 = getelementptr inbounds [4 x i64], [4 x i64]* %44, i64 %57, i64 2
  store i64 %77, i64* %78, align 8
  %79 = getelementptr inbounds [4 x i64], [4 x i64]* %22, i64 %56, i64 3
  %80 = load i64, i64* %79, align 8
  %81 = getelementptr inbounds [4 x i64], [4 x i64]* %44, i64 %57, i64 3
  store i64 %80, i64* %81, align 8
  %82 = getelementptr inbounds [4 x i64], [4 x i64]* %47, i64 %57, i64 0
  %83 = getelementptr inbounds [4 x i64], [4 x i64]* %23, i64 %56, i64 0
  %84 = load i64, i64* %83, align 8
  store i64 %84, i64* %82, align 8
  %85 = getelementptr inbounds [4 x i64], [4 x i64]* %23, i64 %56, i64 1
  %86 = load i64, i64* %85, align 8
  %87 = getelementptr inbounds [4 x i64], [4 x i64]* %47, i64 %57, i64 1
  store i64 %86, i64* %87, align 8
  %88 = getelementptr inbounds [4 x i64], [4 x i64]* %23, i64 %56, i64 2
  %89 = load i64, i64* %88, align 8
  %90 = getelementptr inbounds [4 x i64], [4 x i64]* %47, i64 %57, i64 2
  store i64 %89, i64* %90, align 8
  %91 = getelementptr inbounds [4 x i64], [4 x i64]* %23, i64 %56, i64 3
  %92 = load i64, i64* %91, align 8
  %93 = getelementptr inbounds [4 x i64], [4 x i64]* %47, i64 %57, i64 3
  store i64 %92, i64* %93, align 8
  %94 = shl i32 %55, 1
  %95 = add nuw i64 %56, 1
  %96 = icmp eq i64 %95, %37
  br i1 %96, label %52, label %54

97:                                               ; preds = %52, %97
  %98 = phi i64 [ %114, %97 ], [ 1, %52 ]
  %99 = trunc i64 %98 to i32
  %100 = sub nsw i32 0, %99
  %101 = and i32 %99, %100
  %102 = getelementptr inbounds [4 x i64], [4 x i64]* %41, i64 %98, i64 0
  %103 = getelementptr inbounds [4 x i64], [4 x i64]* %44, i64 %98, i64 0
  %104 = getelementptr inbounds [4 x i64], [4 x i64]* %47, i64 %98, i64 0
  %105 = xor i32 %101, %99
  %106 = zext i32 %105 to i64
  %107 = getelementptr inbounds [4 x i64], [4 x i64]* %41, i64 %106, i64 0
  %108 = getelementptr inbounds [4 x i64], [4 x i64]* %44, i64 %106, i64 0
  %109 = getelementptr inbounds [4 x i64], [4 x i64]* %47, i64 %106, i64 0
  %110 = zext i32 %101 to i64
  %111 = getelementptr inbounds [4 x i64], [4 x i64]* %41, i64 %110, i64 0
  %112 = getelementptr inbounds [4 x i64], [4 x i64]* %44, i64 %110, i64 0
  %113 = getelementptr inbounds [4 x i64], [4 x i64]* %47, i64 %110, i64 0
  tail call fastcc void @1(i64* nonnull %102, i64* nonnull %103, i64* nonnull %104, i64* %107, i64* %108, i64* %109, i64* %111, i64* %112, i64* %113)
  %114 = add nuw i64 %98, 1
  %115 = icmp eq i64 %114, %38
  br i1 %115, label %116, label %97

116:                                              ; preds = %97, %52
  %117 = getelementptr inbounds [4 x i64], [4 x i64]* %21, i64 %37
  %118 = getelementptr inbounds [4 x i64], [4 x i64]* %22, i64 %37
  %119 = getelementptr inbounds [4 x i64], [4 x i64]* %23, i64 %37
  %120 = add nuw i64 %24, 1
  %121 = load i64, i64* %8, align 8
  %122 = icmp ult i64 %120, %121
  br i1 %122, label %19, label %123

123:                                              ; preds = %116, %5
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jsmul_table_nistp224_inner(i64* nocapture %0, i64* nocapture %1, i64* nocapture %2, %0* nocapture readnone %3, %4* nocapture readonly %4, [1 x %1]* readonly %5, i64 %6) local_unnamed_addr #0 {
  %8 = alloca [4 x i64], align 16
  %9 = alloca [4 x i64], align 16
  %10 = alloca [4 x i64], align 16
  %11 = bitcast [4 x i64]* %8 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %11) #7
  %12 = bitcast [4 x i64]* %9 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %12) #7
  %13 = bitcast [4 x i64]* %10 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %13) #7
  %14 = getelementptr inbounds %4, %4* %4, i64 0, i32 1
  %15 = load i64, i64* %14, align 8
  %16 = getelementptr inbounds %4, %4* %4, i64 0, i32 3
  %17 = load i64, i64* %16, align 8
  %18 = getelementptr inbounds %4, %4* %4, i64 0, i32 2
  %19 = load i64, i64* %18, align 8
  %20 = add i64 %17, -1
  %21 = getelementptr inbounds %4, %4* %4, i64 0, i32 4
  %22 = load [4 x i64]**, [4 x i64]*** %21, align 8
  %23 = getelementptr inbounds %4, %4* %4, i64 0, i32 5
  %24 = load [4 x i64]**, [4 x i64]*** %23, align 8
  %25 = getelementptr inbounds %4, %4* %4, i64 0, i32 6
  %26 = load [4 x i64]**, [4 x i64]*** %25, align 8
  %27 = getelementptr inbounds [4 x i64], [4 x i64]* %8, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %11, i8 0, i64 32, i1 false)
  %28 = getelementptr inbounds [4 x i64], [4 x i64]* %9, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %12, i8 0, i64 32, i1 false)
  %29 = getelementptr inbounds [4 x i64], [4 x i64]* %10, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %13, i8 0, i64 32, i1 false)
  %30 = trunc i64 %6 to i32
  %31 = add i32 %30, -1
  %32 = icmp sgt i32 %31, -1
  br i1 %32, label %33, label %150

33:                                               ; preds = %7
  %34 = mul i64 %20, %19
  %35 = sub i64 %15, %34
  %36 = icmp eq i64 %17, 0
  %37 = trunc i64 %35 to i32
  %38 = add i32 %37, -1
  %39 = icmp sgt i32 %38, -1
  %40 = trunc i64 %19 to i32
  %41 = add i32 %40, -1
  %42 = icmp sgt i32 %41, -1
  %43 = sext i32 %38 to i64
  %44 = sext i32 %41 to i64
  %45 = sext i32 %31 to i64
  %46 = xor i1 %39, true
  br label %51

47:                                               ; preds = %119, %86, %51
  %48 = add i32 %53, -1
  %49 = icmp sgt i32 %48, -1
  %50 = add nsw i64 %52, -1
  br i1 %49, label %51, label %134

51:                                               ; preds = %33, %47
  %52 = phi i64 [ %45, %33 ], [ %50, %47 ]
  %53 = phi i32 [ %31, %33 ], [ %48, %47 ]
  call fastcc void @0(i64* nonnull %27, i64* nonnull %28, i64* nonnull %29, i64* nonnull %27, i64* nonnull %28, i64* nonnull %29)
  br i1 %36, label %47, label %54

54:                                               ; preds = %51
  br i1 %42, label %55, label %101

55:                                               ; preds = %54, %86
  %56 = phi i64 [ %98, %86 ], [ 0, %54 ]
  %57 = phi [1 x %1]* [ %99, %86 ], [ %5, %54 ]
  %58 = icmp eq i64 %56, %20
  br i1 %58, label %72, label %59

59:                                               ; preds = %55, %59
  %60 = phi i64 [ %71, %59 ], [ %44, %55 ]
  %61 = phi i32 [ %69, %59 ], [ %41, %55 ]
  %62 = phi i32 [ %68, %59 ], [ 0, %55 ]
  %63 = shl i32 %62, 1
  %64 = getelementptr inbounds [1 x %1], [1 x %1]* %57, i64 %60, i64 0
  %65 = call i32 @__gmpz_tstbit(%1* %64, i64 %52) #6
  %66 = icmp ne i32 %65, 0
  %67 = zext i1 %66 to i32
  %68 = or i32 %63, %67
  %69 = add i32 %61, -1
  %70 = icmp sgt i32 %69, -1
  %71 = add nsw i64 %60, -1
  br i1 %70, label %59, label %86

72:                                               ; preds = %55
  br i1 %39, label %73, label %86

73:                                               ; preds = %72, %73
  %74 = phi i64 [ %85, %73 ], [ %43, %72 ]
  %75 = phi i32 [ %83, %73 ], [ %38, %72 ]
  %76 = phi i32 [ %82, %73 ], [ 0, %72 ]
  %77 = shl i32 %76, 1
  %78 = getelementptr inbounds [1 x %1], [1 x %1]* %57, i64 %74, i64 0
  %79 = call i32 @__gmpz_tstbit(%1* %78, i64 %52) #6
  %80 = icmp ne i32 %79, 0
  %81 = zext i1 %80 to i32
  %82 = or i32 %77, %81
  %83 = add i32 %75, -1
  %84 = icmp sgt i32 %83, -1
  %85 = add nsw i64 %74, -1
  br i1 %84, label %73, label %86

86:                                               ; preds = %59, %73, %72
  %87 = phi i32 [ 0, %72 ], [ %82, %73 ], [ %68, %59 ]
  %88 = getelementptr inbounds [4 x i64]*, [4 x i64]** %22, i64 %56
  %89 = load [4 x i64]*, [4 x i64]** %88, align 8
  %90 = sext i32 %87 to i64
  %91 = getelementptr inbounds [4 x i64], [4 x i64]* %89, i64 %90, i64 0
  %92 = getelementptr inbounds [4 x i64]*, [4 x i64]** %24, i64 %56
  %93 = load [4 x i64]*, [4 x i64]** %92, align 8
  %94 = getelementptr inbounds [4 x i64], [4 x i64]* %93, i64 %90, i64 0
  %95 = getelementptr inbounds [4 x i64]*, [4 x i64]** %26, i64 %56
  %96 = load [4 x i64]*, [4 x i64]** %95, align 8
  %97 = getelementptr inbounds [4 x i64], [4 x i64]* %96, i64 %90, i64 0
  call fastcc void @1(i64* nonnull %27, i64* nonnull %28, i64* nonnull %29, i64* nonnull %27, i64* nonnull %28, i64* nonnull %29, i64* %91, i64* %94, i64* %97)
  %98 = add nuw i64 %56, 1
  %99 = getelementptr inbounds [1 x %1], [1 x %1]* %57, i64 %19
  %100 = icmp eq i64 %98, %17
  br i1 %100, label %47, label %55

101:                                              ; preds = %54, %119
  %102 = phi i64 [ %131, %119 ], [ 0, %54 ]
  %103 = phi [1 x %1]* [ %132, %119 ], [ %5, %54 ]
  %104 = icmp ne i64 %102, %20
  %105 = or i1 %104, %46
  br i1 %105, label %119, label %106

106:                                              ; preds = %101, %106
  %107 = phi i64 [ %118, %106 ], [ %43, %101 ]
  %108 = phi i32 [ %116, %106 ], [ %38, %101 ]
  %109 = phi i32 [ %115, %106 ], [ 0, %101 ]
  %110 = shl i32 %109, 1
  %111 = getelementptr inbounds [1 x %1], [1 x %1]* %103, i64 %107, i64 0
  %112 = call i32 @__gmpz_tstbit(%1* %111, i64 %52) #6
  %113 = icmp ne i32 %112, 0
  %114 = zext i1 %113 to i32
  %115 = or i32 %110, %114
  %116 = add i32 %108, -1
  %117 = icmp sgt i32 %116, -1
  %118 = add nsw i64 %107, -1
  br i1 %117, label %106, label %119

119:                                              ; preds = %106, %101
  %120 = phi i32 [ 0, %101 ], [ %115, %106 ]
  %121 = getelementptr inbounds [4 x i64]*, [4 x i64]** %22, i64 %102
  %122 = load [4 x i64]*, [4 x i64]** %121, align 8
  %123 = sext i32 %120 to i64
  %124 = getelementptr inbounds [4 x i64], [4 x i64]* %122, i64 %123, i64 0
  %125 = getelementptr inbounds [4 x i64]*, [4 x i64]** %24, i64 %102
  %126 = load [4 x i64]*, [4 x i64]** %125, align 8
  %127 = getelementptr inbounds [4 x i64], [4 x i64]* %126, i64 %123, i64 0
  %128 = getelementptr inbounds [4 x i64]*, [4 x i64]** %26, i64 %102
  %129 = load [4 x i64]*, [4 x i64]** %128, align 8
  %130 = getelementptr inbounds [4 x i64], [4 x i64]* %129, i64 %123, i64 0
  call fastcc void @1(i64* nonnull %27, i64* nonnull %28, i64* nonnull %29, i64* nonnull %27, i64* nonnull %28, i64* nonnull %29, i64* %124, i64* %127, i64* %130)
  %131 = add nuw i64 %102, 1
  %132 = getelementptr inbounds [1 x %1], [1 x %1]* %103, i64 %19
  %133 = icmp eq i64 %131, %17
  br i1 %133, label %47, label %101

134:                                              ; preds = %47
  %135 = bitcast [4 x i64]* %8 to <2 x i64>*
  %136 = load <2 x i64>, <2 x i64>* %135, align 16
  %137 = getelementptr inbounds [4 x i64], [4 x i64]* %8, i64 0, i64 2
  %138 = bitcast i64* %137 to <2 x i64>*
  %139 = load <2 x i64>, <2 x i64>* %138, align 16
  %140 = bitcast [4 x i64]* %9 to <2 x i64>*
  %141 = load <2 x i64>, <2 x i64>* %140, align 16
  %142 = getelementptr inbounds [4 x i64], [4 x i64]* %9, i64 0, i64 2
  %143 = bitcast i64* %142 to <2 x i64>*
  %144 = load <2 x i64>, <2 x i64>* %143, align 16
  %145 = bitcast [4 x i64]* %10 to <2 x i64>*
  %146 = load <2 x i64>, <2 x i64>* %145, align 16
  %147 = getelementptr inbounds [4 x i64], [4 x i64]* %10, i64 0, i64 2
  %148 = bitcast i64* %147 to <2 x i64>*
  %149 = load <2 x i64>, <2 x i64>* %148, align 16
  br label %150

150:                                              ; preds = %7, %134
  %151 = phi <2 x i64> [ %136, %134 ], [ zeroinitializer, %7 ]
  %152 = phi <2 x i64> [ %139, %134 ], [ zeroinitializer, %7 ]
  %153 = phi <2 x i64> [ %141, %134 ], [ zeroinitializer, %7 ]
  %154 = phi <2 x i64> [ %144, %134 ], [ zeroinitializer, %7 ]
  %155 = phi <2 x i64> [ %146, %134 ], [ zeroinitializer, %7 ]
  %156 = phi <2 x i64> [ %149, %134 ], [ zeroinitializer, %7 ]
  %157 = bitcast i64* %0 to <2 x i64>*
  store <2 x i64> %151, <2 x i64>* %157, align 8
  %158 = getelementptr inbounds i64, i64* %0, i64 2
  %159 = bitcast i64* %158 to <2 x i64>*
  store <2 x i64> %152, <2 x i64>* %159, align 8
  %160 = bitcast i64* %1 to <2 x i64>*
  store <2 x i64> %153, <2 x i64>* %160, align 8
  %161 = getelementptr inbounds i64, i64* %1, i64 2
  %162 = bitcast i64* %161 to <2 x i64>*
  store <2 x i64> %154, <2 x i64>* %162, align 8
  %163 = bitcast i64* %2 to <2 x i64>*
  store <2 x i64> %155, <2 x i64>* %163, align 8
  %164 = getelementptr inbounds i64, i64* %2, i64 2
  %165 = bitcast i64* %164 to <2 x i64>*
  store <2 x i64> %156, <2 x i64>* %165, align 8
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %13) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %12) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %11) #7
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jsmul_block_batch_nistp224_inner(i64* nocapture %0, i64* nocapture %1, i64* %2, %0* nocapture readnone %3, [4 x i64]* nocapture readonly %4, [4 x i64]* nocapture readonly %5, [4 x i64]* nocapture readonly %6, [1 x %1]* readonly %7, i64 %8, i64 %9, i64 %10, i64 %11) local_unnamed_addr #0 {
  %13 = alloca [1 x %4], align 16
  %14 = alloca [4 x i64], align 16
  %15 = alloca [4 x i64], align 16
  %16 = alloca [4 x i64], align 16
  %17 = bitcast [1 x %4]* %13 to i8*
  call void @llvm.lifetime.start.p0i8(i64 56, i8* nonnull %17) #7
  %18 = bitcast [4 x i64]* %14 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %18) #7
  %19 = bitcast [4 x i64]* %15 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %19) #7
  %20 = bitcast [4 x i64]* %16 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %20) #7
  %21 = icmp ult i64 %8, %10
  %22 = select i1 %21, i64 %8, i64 %10
  %23 = getelementptr inbounds [1 x %4], [1 x %4]* %13, i64 0, i64 0
  call void @vec_jsmul_init_nistp224_inner(%4* nonnull %23, %0* undef, i64 %22, i64 %9)
  %24 = bitcast i64* %0 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 8 %24, i8 0, i64 32, i1 false)
  %25 = bitcast i64* %1 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 8 %25, i8 0, i64 32, i1 false)
  %26 = bitcast i64* %2 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 8 %26, i8 0, i64 32, i1 false)
  %27 = icmp eq i64 %8, 0
  br i1 %27, label %51, label %28

28:                                               ; preds = %12
  %29 = getelementptr inbounds [4 x i64], [4 x i64]* %14, i64 0, i64 0
  %30 = getelementptr inbounds [4 x i64], [4 x i64]* %15, i64 0, i64 0
  %31 = getelementptr inbounds [4 x i64], [4 x i64]* %16, i64 0, i64 0
  br label %32

32:                                               ; preds = %28, %43
  %33 = phi i64 [ 0, %28 ], [ %49, %43 ]
  %34 = phi [4 x i64]* [ %4, %28 ], [ %45, %43 ]
  %35 = phi i64 [ %22, %28 ], [ %44, %43 ]
  %36 = phi [4 x i64]* [ %5, %28 ], [ %46, %43 ]
  %37 = phi [4 x i64]* [ %6, %28 ], [ %47, %43 ]
  %38 = phi [1 x %1]* [ %7, %28 ], [ %48, %43 ]
  %39 = sub i64 %8, %33
  %40 = icmp ult i64 %39, %35
  br i1 %40, label %41, label %43

41:                                               ; preds = %32
  %42 = call i32 @vec_jsmul_clear_nistp224_inner(%4* nonnull %23)
  call void @vec_jsmul_init_nistp224_inner(%4* nonnull %23, %0* undef, i64 %39, i64 %9)
  br label %43

43:                                               ; preds = %41, %32
  %44 = phi i64 [ %39, %41 ], [ %35, %32 ]
  call void @vec_jsmul_precomp_nistp224_inner(%4* nonnull %23, %0* undef, [4 x i64]* %34, [4 x i64]* %36, [4 x i64]* %37)
  call void @vec_jsmul_table_nistp224_inner(i64* nonnull %29, i64* nonnull %30, i64* nonnull %31, %0* undef, %4* nonnull %23, [1 x %1]* %38, i64 %11)
  call fastcc void @1(i64* %0, i64* %1, i64* %2, i64* %0, i64* %1, i64* %2, i64* nonnull %29, i64* nonnull %30, i64* nonnull %31)
  %45 = getelementptr inbounds [4 x i64], [4 x i64]* %34, i64 %44
  %46 = getelementptr inbounds [4 x i64], [4 x i64]* %36, i64 %44
  %47 = getelementptr inbounds [4 x i64], [4 x i64]* %37, i64 %44
  %48 = getelementptr inbounds [1 x %1], [1 x %1]* %38, i64 %44
  %49 = add i64 %44, %33
  %50 = icmp ult i64 %49, %8
  br i1 %50, label %32, label %51

51:                                               ; preds = %43, %12
  %52 = call i32 @vec_jsmul_clear_nistp224_inner(%4* nonnull %23)
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %20) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %19) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %18) #7
  call void @llvm.lifetime.end.p0i8(i64 56, i8* nonnull %17) #7
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jsmul_nistp224_inner(i64* nocapture %0, i64* nocapture %1, i64* %2, %0* nocapture readnone %3, [4 x i64]* nocapture readonly %4, [4 x i64]* nocapture readonly %5, [4 x i64]* nocapture readonly %6, [1 x %1]* %7, i64 %8) local_unnamed_addr #0 {
  %10 = icmp eq i64 %8, 0
  br i1 %10, label %20, label %11

11:                                               ; preds = %9, %11
  %12 = phi i64 [ %18, %11 ], [ 0, %9 ]
  %13 = phi i64 [ %17, %11 ], [ 0, %9 ]
  %14 = getelementptr inbounds [1 x %1], [1 x %1]* %7, i64 %12, i64 0
  %15 = tail call i64 @__gmpz_sizeinbase(%1* %14, i32 2) #6
  %16 = icmp ugt i64 %15, %13
  %17 = select i1 %16, i64 %15, i64 %13
  %18 = add nuw i64 %12, 1
  %19 = icmp eq i64 %18, %8
  br i1 %19, label %20, label %11

20:                                               ; preds = %11, %9
  %21 = phi i64 [ 0, %9 ], [ %17, %11 ]
  %22 = trunc i64 %21 to i32
  %23 = tail call i32 @vec_smul_block_width(i32 %22, i32 100) #7
  %24 = sext i32 %23 to i64
  tail call void @vec_jsmul_block_batch_nistp224_inner(i64* %0, i64* %1, i64* %2, %0* undef, [4 x i64]* %4, [4 x i64]* %5, [4 x i64]* %6, [1 x %1]* %7, i64 %8, i64 %24, i64 100, i64 %21)
  ret void
}

declare dso_local i32 @vec_smul_block_width(i32, i32) local_unnamed_addr #5

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jfmul_init_nistp224_inner(%5* nocapture %0, %0* %1, i64 %2) local_unnamed_addr #0 {
  %4 = getelementptr inbounds %0, %0* %1, i64 0, i32 6, i64 0
  %5 = tail call i64 @__gmpz_sizeinbase(%1* nonnull %4, i32 2) #6
  %6 = trunc i64 %5 to i32
  %7 = trunc i64 %2 to i32
  %8 = tail call i32 @vec_fmul_block_width(i32 %6, i32 %7) #7
  %9 = getelementptr inbounds %5, %5* %0, i64 0, i32 0, i64 0
  %10 = sext i32 %8 to i64
  tail call void @vec_jsmul_init_nistp224_inner(%4* %9, %0* undef, i64 %10, i64 %10)
  %11 = add nsw i32 %8, -1
  %12 = add nsw i32 %11, %6
  %13 = sdiv i32 %12, %8
  %14 = sext i32 %13 to i64
  %15 = getelementptr inbounds %5, %5* %0, i64 0, i32 1
  store i64 %14, i64* %15, align 8
  ret void
}

declare dso_local i32 @vec_fmul_block_width(i32, i32) local_unnamed_addr #5

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jfmul_clear_free_nistp224_inner(%5* nocapture %0) local_unnamed_addr #0 {
  %2 = getelementptr inbounds %5, %5* %0, i64 0, i32 0, i64 0
  %3 = tail call i32 @vec_jsmul_clear_nistp224_inner(%4* %2)
  %4 = bitcast %5* %0 to i8*
  tail call void @free(i8* %4) #7
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jfmul_prcmp_nistp224_inner(%0* nocapture readnone %0, %5* nocapture readonly %1, i64* nocapture readonly %2, i64* nocapture readonly %3, i64* nocapture readonly %4) local_unnamed_addr #0 {
  %6 = getelementptr inbounds %5, %5* %1, i64 0, i32 0, i64 0
  %7 = getelementptr inbounds %5, %5* %1, i64 0, i32 0, i64 0, i32 2
  %8 = load i64, i64* %7, align 8
  %9 = shl i64 %8, 5
  %10 = tail call noalias i8* @malloc(i64 %9) #7
  %11 = bitcast i8* %10 to [4 x i64]*
  %12 = tail call noalias i8* @malloc(i64 %9) #7
  %13 = bitcast i8* %12 to [4 x i64]*
  %14 = tail call noalias i8* @malloc(i64 %9) #7
  %15 = bitcast i8* %14 to [4 x i64]*
  %16 = bitcast i64* %2 to <2 x i64>*
  %17 = load <2 x i64>, <2 x i64>* %16, align 8
  %18 = bitcast i8* %10 to <2 x i64>*
  store <2 x i64> %17, <2 x i64>* %18, align 8
  %19 = getelementptr inbounds i64, i64* %2, i64 2
  %20 = getelementptr inbounds i8, i8* %10, i64 16
  %21 = bitcast i64* %19 to <2 x i64>*
  %22 = load <2 x i64>, <2 x i64>* %21, align 8
  %23 = bitcast i8* %20 to <2 x i64>*
  store <2 x i64> %22, <2 x i64>* %23, align 8
  %24 = bitcast i64* %3 to <2 x i64>*
  %25 = load <2 x i64>, <2 x i64>* %24, align 8
  %26 = bitcast i8* %12 to <2 x i64>*
  store <2 x i64> %25, <2 x i64>* %26, align 8
  %27 = getelementptr inbounds i64, i64* %3, i64 2
  %28 = getelementptr inbounds i8, i8* %12, i64 16
  %29 = bitcast i64* %27 to <2 x i64>*
  %30 = load <2 x i64>, <2 x i64>* %29, align 8
  %31 = bitcast i8* %28 to <2 x i64>*
  store <2 x i64> %30, <2 x i64>* %31, align 8
  %32 = bitcast i64* %4 to <2 x i64>*
  %33 = load <2 x i64>, <2 x i64>* %32, align 8
  %34 = bitcast i8* %14 to <2 x i64>*
  store <2 x i64> %33, <2 x i64>* %34, align 8
  %35 = getelementptr inbounds i64, i64* %4, i64 2
  %36 = getelementptr inbounds i8, i8* %14, i64 16
  %37 = bitcast i64* %35 to <2 x i64>*
  %38 = load <2 x i64>, <2 x i64>* %37, align 8
  %39 = bitcast i8* %36 to <2 x i64>*
  store <2 x i64> %38, <2 x i64>* %39, align 8
  %40 = icmp ugt i64 %8, 1
  br i1 %40, label %41, label %88

41:                                               ; preds = %5
  %42 = getelementptr inbounds %5, %5* %1, i64 0, i32 1
  %43 = load i64, i64* %42, align 8
  br label %44

44:                                               ; preds = %84, %41
  %45 = phi i64 [ %43, %41 ], [ %85, %84 ]
  %46 = phi i64 [ 1, %41 ], [ %86, %84 ]
  %47 = getelementptr inbounds [4 x i64], [4 x i64]* %11, i64 %46, i64 0
  %48 = add i64 %46, -1
  %49 = getelementptr inbounds [4 x i64], [4 x i64]* %11, i64 %48, i64 0
  %50 = bitcast i64* %49 to <2 x i64>*
  %51 = load <2 x i64>, <2 x i64>* %50, align 8
  %52 = bitcast i64* %47 to <2 x i64>*
  store <2 x i64> %51, <2 x i64>* %52, align 8
  %53 = getelementptr inbounds [4 x i64], [4 x i64]* %11, i64 %48, i64 2
  %54 = getelementptr inbounds [4 x i64], [4 x i64]* %11, i64 %46, i64 2
  %55 = bitcast i64* %53 to <2 x i64>*
  %56 = load <2 x i64>, <2 x i64>* %55, align 8
  %57 = bitcast i64* %54 to <2 x i64>*
  store <2 x i64> %56, <2 x i64>* %57, align 8
  %58 = getelementptr inbounds [4 x i64], [4 x i64]* %13, i64 %46, i64 0
  %59 = getelementptr inbounds [4 x i64], [4 x i64]* %13, i64 %48, i64 0
  %60 = bitcast i64* %59 to <2 x i64>*
  %61 = load <2 x i64>, <2 x i64>* %60, align 8
  %62 = bitcast i64* %58 to <2 x i64>*
  store <2 x i64> %61, <2 x i64>* %62, align 8
  %63 = getelementptr inbounds [4 x i64], [4 x i64]* %13, i64 %48, i64 2
  %64 = getelementptr inbounds [4 x i64], [4 x i64]* %13, i64 %46, i64 2
  %65 = bitcast i64* %63 to <2 x i64>*
  %66 = load <2 x i64>, <2 x i64>* %65, align 8
  %67 = bitcast i64* %64 to <2 x i64>*
  store <2 x i64> %66, <2 x i64>* %67, align 8
  %68 = getelementptr inbounds [4 x i64], [4 x i64]* %15, i64 %46, i64 0
  %69 = getelementptr inbounds [4 x i64], [4 x i64]* %15, i64 %48, i64 0
  %70 = bitcast i64* %69 to <2 x i64>*
  %71 = load <2 x i64>, <2 x i64>* %70, align 8
  %72 = bitcast i64* %68 to <2 x i64>*
  store <2 x i64> %71, <2 x i64>* %72, align 8
  %73 = getelementptr inbounds [4 x i64], [4 x i64]* %15, i64 %48, i64 2
  %74 = getelementptr inbounds [4 x i64], [4 x i64]* %15, i64 %46, i64 2
  %75 = bitcast i64* %73 to <2 x i64>*
  %76 = load <2 x i64>, <2 x i64>* %75, align 8
  %77 = bitcast i64* %74 to <2 x i64>*
  store <2 x i64> %76, <2 x i64>* %77, align 8
  %78 = icmp eq i64 %45, 0
  br i1 %78, label %84, label %79

79:                                               ; preds = %44, %79
  %80 = phi i64 [ %81, %79 ], [ 0, %44 ]
  tail call fastcc void @0(i64* nonnull %47, i64* nonnull %58, i64* nonnull %68, i64* nonnull %47, i64* nonnull %58, i64* nonnull %68)
  %81 = add nuw i64 %80, 1
  %82 = load i64, i64* %42, align 8
  %83 = icmp ult i64 %81, %82
  br i1 %83, label %79, label %84

84:                                               ; preds = %79, %44
  %85 = phi i64 [ 0, %44 ], [ %82, %79 ]
  %86 = add nuw i64 %46, 1
  %87 = icmp eq i64 %86, %8
  br i1 %87, label %88, label %44

88:                                               ; preds = %84, %5
  tail call void @vec_jsmul_precomp_nistp224_inner(%4* %6, %0* undef, [4 x i64]* %11, [4 x i64]* %13, [4 x i64]* %15)
  tail call void @free(i8* %10) #7
  tail call void @free(i8* %12) #7
  tail call void @free(i8* %14) #7
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jfmul_cmp_nistp224_inner(i64* nocapture %0, i64* nocapture %1, i64* nocapture %2, %0* nocapture readnone %3, %5* nocapture readonly %4, %1* %5) local_unnamed_addr #0 {
  %7 = alloca [1 x %1], align 16
  %8 = getelementptr inbounds %5, %5* %4, i64 0, i32 0, i64 0
  %9 = getelementptr inbounds %5, %5* %4, i64 0, i32 0, i64 0, i32 2
  %10 = load i64, i64* %9, align 8
  %11 = bitcast [1 x %1]* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 16, i8* nonnull %11) #7
  %12 = getelementptr inbounds [1 x %1], [1 x %1]* %7, i64 0, i64 0
  call void @__gmpz_init(%1* nonnull %12) #7
  %13 = call [1 x %1]* @vec_array_alloc_init(i64 %10) #7
  call void @__gmpz_set(%1* nonnull %12, %1* %5) #7
  %14 = icmp eq i64 %10, 0
  %15 = getelementptr inbounds %5, %5* %4, i64 0, i32 1
  br i1 %14, label %23, label %16

16:                                               ; preds = %6, %16
  %17 = phi i64 [ %21, %16 ], [ 0, %6 ]
  %18 = getelementptr inbounds [1 x %1], [1 x %1]* %13, i64 %17, i64 0
  %19 = load i64, i64* %15, align 8
  call void @__gmpz_tdiv_r_2exp(%1* %18, %1* nonnull %12, i64 %19) #7
  %20 = load i64, i64* %15, align 8
  call void @__gmpz_tdiv_q_2exp(%1* nonnull %12, %1* nonnull %12, i64 %20) #7
  %21 = add nuw i64 %17, 1
  %22 = icmp eq i64 %21, %10
  br i1 %22, label %23, label %16

23:                                               ; preds = %16, %6
  %24 = load i64, i64* %15, align 8
  call void @vec_jsmul_table_nistp224_inner(i64* %0, i64* %1, i64* %2, %0* undef, %4* %8, [1 x %1]* %13, i64 %24)
  call void @vec_array_clear_free([1 x %1]* %13, i64 %10) #7
  call void @__gmpz_clear(%1* nonnull %12) #7
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %11) #7
  ret void
}

declare dso_local void @__gmpz_init(%1*) local_unnamed_addr #5

declare dso_local [1 x %1]* @vec_array_alloc_init(i64) local_unnamed_addr #5

declare dso_local void @__gmpz_set(%1*, %1*) local_unnamed_addr #5

declare dso_local void @__gmpz_tdiv_r_2exp(%1*, %1*, i64) local_unnamed_addr #5

declare dso_local void @__gmpz_tdiv_q_2exp(%1*, %1*, i64) local_unnamed_addr #5

declare dso_local void @vec_array_clear_free([1 x %1]*, i64) local_unnamed_addr #5

declare dso_local void @__gmpz_clear(%1*) local_unnamed_addr #5

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jmulsw_nistp224(%1* %0, %1* %1, %1* %2, %0* nocapture readnone %3, %1* %4, %1* %5, %1* %6, %1* %7) local_unnamed_addr #0 {
  %9 = alloca [4 x i64], align 16
  %10 = alloca [4 x i64], align 16
  %11 = alloca [4 x i64], align 16
  %12 = alloca [4 x i64], align 16
  %13 = alloca [4 x i64], align 16
  %14 = alloca [4 x i64], align 16
  %15 = bitcast [4 x i64]* %9 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %15) #7
  %16 = bitcast [4 x i64]* %10 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %16) #7
  %17 = bitcast [4 x i64]* %11 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %17) #7
  %18 = bitcast [4 x i64]* %12 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %18) #7
  %19 = bitcast [4 x i64]* %13 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %19) #7
  %20 = bitcast [4 x i64]* %14 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %20) #7
  %21 = getelementptr inbounds [4 x i64], [4 x i64]* %9, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %15, i8 0, i64 32, i1 false) #7
  %22 = call i8* @__gmpz_export(i8* nonnull %15, i64* null, i32 -1, i64 8, i32 0, i64 8, %1* %4) #7
  %23 = getelementptr inbounds [4 x i64], [4 x i64]* %10, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %16, i8 0, i64 32, i1 false) #7
  %24 = call i8* @__gmpz_export(i8* nonnull %16, i64* null, i32 -1, i64 8, i32 0, i64 8, %1* %5) #7
  %25 = getelementptr inbounds [4 x i64], [4 x i64]* %11, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %17, i8 0, i64 32, i1 false) #7
  %26 = call i8* @__gmpz_export(i8* nonnull %17, i64* null, i32 -1, i64 8, i32 0, i64 8, %1* %6) #7
  %27 = getelementptr inbounds [4 x i64], [4 x i64]* %12, i64 0, i64 0
  %28 = getelementptr inbounds [4 x i64], [4 x i64]* %13, i64 0, i64 0
  %29 = getelementptr inbounds [4 x i64], [4 x i64]* %14, i64 0, i64 0
  call void @vec_jmulsw_nistp224_inner(i64* nonnull %27, i64* nonnull %28, i64* nonnull %29, %0* undef, i64* nonnull %21, i64* nonnull %23, i64* nonnull %25, %1* %7)
  %30 = load i64, i64* %27, align 16
  %31 = getelementptr inbounds [4 x i64], [4 x i64]* %12, i64 0, i64 1
  %32 = load i64, i64* %31, align 8
  %33 = or i64 %32, %30
  %34 = getelementptr inbounds [4 x i64], [4 x i64]* %12, i64 0, i64 2
  %35 = load i64, i64* %34, align 16
  %36 = or i64 %33, %35
  %37 = getelementptr inbounds [4 x i64], [4 x i64]* %12, i64 0, i64 3
  %38 = load i64, i64* %37, align 8
  %39 = or i64 %36, %38
  %40 = add nsw i64 %39, -1
  %41 = xor i64 %35, 72057594037927935
  %42 = insertelement <2 x i64> undef, i64 %30, i32 0
  %43 = shufflevector <2 x i64> %42, <2 x i64> undef, <2 x i32> zeroinitializer
  %44 = xor <2 x i64> %43, <i64 1, i64 2>
  %45 = insertelement <2 x i64> undef, i64 %32, i32 0
  %46 = shufflevector <2 x i64> %45, <2 x i64> undef, <2 x i32> zeroinitializer
  %47 = xor <2 x i64> %46, <i64 72056494526300160, i64 72055395014672384>
  %48 = or <2 x i64> %47, %44
  %49 = insertelement <2 x i64> undef, i64 %41, i32 0
  %50 = shufflevector <2 x i64> %49, <2 x i64> undef, <2 x i32> zeroinitializer
  %51 = or <2 x i64> %48, %50
  %52 = insertelement <2 x i64> undef, i64 %38, i32 0
  %53 = shufflevector <2 x i64> %52, <2 x i64> undef, <2 x i32> zeroinitializer
  %54 = xor <2 x i64> %53, <i64 72057594037927935, i64 144115188075855871>
  %55 = or <2 x i64> %51, %54
  %56 = add nsw <2 x i64> %55, <i64 -1, i64 -1>
  %57 = extractelement <2 x i64> %56, i32 0
  %58 = or i64 %57, %40
  %59 = extractelement <2 x i64> %56, i32 1
  %60 = or i64 %58, %59
  %61 = icmp sgt i64 %60, -1
  br i1 %61, label %63, label %62

62:                                               ; preds = %8
  call void @__gmpz_set_si(%1* %0, i64 0) #7
  br label %64

63:                                               ; preds = %8
  call void @__gmpz_import(%1* %0, i64 4, i32 -1, i64 8, i32 0, i64 8, i8* nonnull %18) #7
  br label %64

64:                                               ; preds = %62, %63
  %65 = load i64, i64* %28, align 16
  %66 = getelementptr inbounds [4 x i64], [4 x i64]* %13, i64 0, i64 1
  %67 = load i64, i64* %66, align 8
  %68 = or i64 %67, %65
  %69 = getelementptr inbounds [4 x i64], [4 x i64]* %13, i64 0, i64 2
  %70 = load i64, i64* %69, align 16
  %71 = or i64 %68, %70
  %72 = getelementptr inbounds [4 x i64], [4 x i64]* %13, i64 0, i64 3
  %73 = load i64, i64* %72, align 8
  %74 = or i64 %71, %73
  %75 = add nsw i64 %74, -1
  %76 = xor i64 %70, 72057594037927935
  %77 = insertelement <2 x i64> undef, i64 %65, i32 0
  %78 = shufflevector <2 x i64> %77, <2 x i64> undef, <2 x i32> zeroinitializer
  %79 = xor <2 x i64> %78, <i64 1, i64 2>
  %80 = insertelement <2 x i64> undef, i64 %67, i32 0
  %81 = shufflevector <2 x i64> %80, <2 x i64> undef, <2 x i32> zeroinitializer
  %82 = xor <2 x i64> %81, <i64 72056494526300160, i64 72055395014672384>
  %83 = or <2 x i64> %82, %79
  %84 = insertelement <2 x i64> undef, i64 %76, i32 0
  %85 = shufflevector <2 x i64> %84, <2 x i64> undef, <2 x i32> zeroinitializer
  %86 = or <2 x i64> %83, %85
  %87 = insertelement <2 x i64> undef, i64 %73, i32 0
  %88 = shufflevector <2 x i64> %87, <2 x i64> undef, <2 x i32> zeroinitializer
  %89 = xor <2 x i64> %88, <i64 72057594037927935, i64 144115188075855871>
  %90 = or <2 x i64> %86, %89
  %91 = add nsw <2 x i64> %90, <i64 -1, i64 -1>
  %92 = extractelement <2 x i64> %91, i32 0
  %93 = or i64 %92, %75
  %94 = extractelement <2 x i64> %91, i32 1
  %95 = or i64 %93, %94
  %96 = icmp sgt i64 %95, -1
  br i1 %96, label %98, label %97

97:                                               ; preds = %64
  call void @__gmpz_set_si(%1* %1, i64 0) #7
  br label %99

98:                                               ; preds = %64
  call void @__gmpz_import(%1* %1, i64 4, i32 -1, i64 8, i32 0, i64 8, i8* nonnull %19) #7
  br label %99

99:                                               ; preds = %97, %98
  %100 = load i64, i64* %29, align 16
  %101 = getelementptr inbounds [4 x i64], [4 x i64]* %14, i64 0, i64 1
  %102 = load i64, i64* %101, align 8
  %103 = or i64 %102, %100
  %104 = getelementptr inbounds [4 x i64], [4 x i64]* %14, i64 0, i64 2
  %105 = load i64, i64* %104, align 16
  %106 = or i64 %103, %105
  %107 = getelementptr inbounds [4 x i64], [4 x i64]* %14, i64 0, i64 3
  %108 = load i64, i64* %107, align 8
  %109 = or i64 %106, %108
  %110 = add nsw i64 %109, -1
  %111 = xor i64 %105, 72057594037927935
  %112 = insertelement <2 x i64> undef, i64 %100, i32 0
  %113 = shufflevector <2 x i64> %112, <2 x i64> undef, <2 x i32> zeroinitializer
  %114 = xor <2 x i64> %113, <i64 1, i64 2>
  %115 = insertelement <2 x i64> undef, i64 %102, i32 0
  %116 = shufflevector <2 x i64> %115, <2 x i64> undef, <2 x i32> zeroinitializer
  %117 = xor <2 x i64> %116, <i64 72056494526300160, i64 72055395014672384>
  %118 = or <2 x i64> %117, %114
  %119 = insertelement <2 x i64> undef, i64 %111, i32 0
  %120 = shufflevector <2 x i64> %119, <2 x i64> undef, <2 x i32> zeroinitializer
  %121 = or <2 x i64> %118, %120
  %122 = insertelement <2 x i64> undef, i64 %108, i32 0
  %123 = shufflevector <2 x i64> %122, <2 x i64> undef, <2 x i32> zeroinitializer
  %124 = xor <2 x i64> %123, <i64 72057594037927935, i64 144115188075855871>
  %125 = or <2 x i64> %121, %124
  %126 = add nsw <2 x i64> %125, <i64 -1, i64 -1>
  %127 = extractelement <2 x i64> %126, i32 0
  %128 = or i64 %127, %110
  %129 = extractelement <2 x i64> %126, i32 1
  %130 = or i64 %128, %129
  %131 = icmp sgt i64 %130, -1
  br i1 %131, label %133, label %132

132:                                              ; preds = %99
  call void @__gmpz_set_si(%1* %2, i64 0) #7
  br label %134

133:                                              ; preds = %99
  call void @__gmpz_import(%1* %2, i64 4, i32 -1, i64 8, i32 0, i64 8, i8* nonnull %20) #7
  br label %134

134:                                              ; preds = %132, %133
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %20) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %19) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %18) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %17) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %16) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %15) #7
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jsmul_nistp224(%1* %0, %1* %1, %1* %2, %0* nocapture readnone %3, [1 x %1]* %4, [1 x %1]* %5, [1 x %1]* %6, [1 x %1]* %7, i64 %8) local_unnamed_addr #0 {
  %10 = alloca [4 x i64], align 16
  %11 = alloca [4 x i64], align 16
  %12 = alloca [4 x i64], align 16
  %13 = bitcast [4 x i64]* %10 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %13) #7
  %14 = bitcast [4 x i64]* %11 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %14) #7
  %15 = bitcast [4 x i64]* %12 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %15) #7
  %16 = shl i64 %8, 5
  %17 = tail call noalias i8* @malloc(i64 %16) #7
  %18 = bitcast i8* %17 to [4 x i64]*
  %19 = icmp eq i64 %8, 0
  br i1 %19, label %50, label %20

20:                                               ; preds = %9, %20
  %21 = phi i64 [ %26, %20 ], [ 0, %9 ]
  %22 = getelementptr inbounds [4 x i64], [4 x i64]* %18, i64 %21, i64 0
  %23 = getelementptr inbounds [1 x %1], [1 x %1]* %4, i64 %21, i64 0
  %24 = bitcast i64* %22 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 8 %24, i8 0, i64 32, i1 false) #7
  %25 = tail call i8* @__gmpz_export(i8* %24, i64* null, i32 -1, i64 8, i32 0, i64 8, %1* %23) #7
  %26 = add nuw i64 %21, 1
  %27 = icmp eq i64 %26, %8
  br i1 %27, label %28, label %20

28:                                               ; preds = %20
  %29 = tail call noalias i8* @malloc(i64 %16) #7
  %30 = bitcast i8* %29 to [4 x i64]*
  br label %31

31:                                               ; preds = %28, %31
  %32 = phi i64 [ %37, %31 ], [ 0, %28 ]
  %33 = getelementptr inbounds [4 x i64], [4 x i64]* %30, i64 %32, i64 0
  %34 = getelementptr inbounds [1 x %1], [1 x %1]* %5, i64 %32, i64 0
  %35 = bitcast i64* %33 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 8 %35, i8 0, i64 32, i1 false) #7
  %36 = tail call i8* @__gmpz_export(i8* %35, i64* null, i32 -1, i64 8, i32 0, i64 8, %1* %34) #7
  %37 = add nuw i64 %32, 1
  %38 = icmp eq i64 %37, %8
  br i1 %38, label %39, label %31

39:                                               ; preds = %31
  %40 = tail call noalias i8* @malloc(i64 %16) #7
  %41 = bitcast i8* %40 to [4 x i64]*
  br label %42

42:                                               ; preds = %39, %42
  %43 = phi i64 [ %48, %42 ], [ 0, %39 ]
  %44 = getelementptr inbounds [4 x i64], [4 x i64]* %41, i64 %43, i64 0
  %45 = getelementptr inbounds [1 x %1], [1 x %1]* %6, i64 %43, i64 0
  %46 = bitcast i64* %44 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 8 %46, i8 0, i64 32, i1 false) #7
  %47 = tail call i8* @__gmpz_export(i8* %46, i64* null, i32 -1, i64 8, i32 0, i64 8, %1* %45) #7
  %48 = add nuw i64 %43, 1
  %49 = icmp eq i64 %48, %8
  br i1 %49, label %58, label %42

50:                                               ; preds = %9
  %51 = tail call noalias i8* @malloc(i64 %16) #7
  %52 = bitcast i8* %51 to [4 x i64]*
  %53 = tail call noalias i8* @malloc(i64 %16) #7
  %54 = bitcast i8* %53 to [4 x i64]*
  %55 = getelementptr inbounds [4 x i64], [4 x i64]* %10, i64 0, i64 0
  %56 = getelementptr inbounds [4 x i64], [4 x i64]* %11, i64 0, i64 0
  %57 = getelementptr inbounds [4 x i64], [4 x i64]* %12, i64 0, i64 0
  br label %71

58:                                               ; preds = %42
  %59 = getelementptr inbounds [4 x i64], [4 x i64]* %10, i64 0, i64 0
  %60 = getelementptr inbounds [4 x i64], [4 x i64]* %11, i64 0, i64 0
  %61 = getelementptr inbounds [4 x i64], [4 x i64]* %12, i64 0, i64 0
  br label %62

62:                                               ; preds = %58, %62
  %63 = phi i64 [ %69, %62 ], [ 0, %58 ]
  %64 = phi i64 [ %68, %62 ], [ 0, %58 ]
  %65 = getelementptr inbounds [1 x %1], [1 x %1]* %7, i64 %63, i64 0
  %66 = tail call i64 @__gmpz_sizeinbase(%1* %65, i32 2) #6
  %67 = icmp ugt i64 %66, %64
  %68 = select i1 %67, i64 %66, i64 %64
  %69 = add nuw i64 %63, 1
  %70 = icmp eq i64 %69, %8
  br i1 %70, label %71, label %62

71:                                               ; preds = %62, %50
  %72 = phi i64* [ %57, %50 ], [ %61, %62 ]
  %73 = phi i64* [ %56, %50 ], [ %60, %62 ]
  %74 = phi i64* [ %55, %50 ], [ %59, %62 ]
  %75 = phi [4 x i64]* [ %52, %50 ], [ %30, %62 ]
  %76 = phi i8* [ %51, %50 ], [ %29, %62 ]
  %77 = phi i8* [ %53, %50 ], [ %40, %62 ]
  %78 = phi [4 x i64]* [ %54, %50 ], [ %41, %62 ]
  %79 = phi i64 [ 0, %50 ], [ %68, %62 ]
  %80 = trunc i64 %79 to i32
  %81 = tail call i32 @vec_smul_block_width(i32 %80, i32 100) #7
  %82 = sext i32 %81 to i64
  call void @vec_jsmul_block_batch_nistp224_inner(i64* %74, i64* %73, i64* %72, %0* undef, [4 x i64]* %18, [4 x i64]* %75, [4 x i64]* %78, [1 x %1]* %7, i64 %8, i64 %82, i64 100, i64 %79) #7
  %83 = load i64, i64* %74, align 8
  %84 = getelementptr inbounds [4 x i64], [4 x i64]* %10, i64 0, i64 1
  %85 = load i64, i64* %84, align 8
  %86 = or i64 %85, %83
  %87 = getelementptr inbounds [4 x i64], [4 x i64]* %10, i64 0, i64 2
  %88 = load i64, i64* %87, align 16
  %89 = or i64 %86, %88
  %90 = getelementptr inbounds [4 x i64], [4 x i64]* %10, i64 0, i64 3
  %91 = load i64, i64* %90, align 8
  %92 = or i64 %89, %91
  %93 = add nsw i64 %92, -1
  %94 = xor i64 %88, 72057594037927935
  %95 = insertelement <2 x i64> undef, i64 %83, i32 0
  %96 = shufflevector <2 x i64> %95, <2 x i64> undef, <2 x i32> zeroinitializer
  %97 = xor <2 x i64> %96, <i64 1, i64 2>
  %98 = insertelement <2 x i64> undef, i64 %85, i32 0
  %99 = shufflevector <2 x i64> %98, <2 x i64> undef, <2 x i32> zeroinitializer
  %100 = xor <2 x i64> %99, <i64 72056494526300160, i64 72055395014672384>
  %101 = or <2 x i64> %100, %97
  %102 = insertelement <2 x i64> undef, i64 %94, i32 0
  %103 = shufflevector <2 x i64> %102, <2 x i64> undef, <2 x i32> zeroinitializer
  %104 = or <2 x i64> %101, %103
  %105 = insertelement <2 x i64> undef, i64 %91, i32 0
  %106 = shufflevector <2 x i64> %105, <2 x i64> undef, <2 x i32> zeroinitializer
  %107 = xor <2 x i64> %106, <i64 72057594037927935, i64 144115188075855871>
  %108 = or <2 x i64> %104, %107
  %109 = add nsw <2 x i64> %108, <i64 -1, i64 -1>
  %110 = extractelement <2 x i64> %109, i32 0
  %111 = or i64 %110, %93
  %112 = extractelement <2 x i64> %109, i32 1
  %113 = or i64 %111, %112
  %114 = icmp sgt i64 %113, -1
  br i1 %114, label %116, label %115

115:                                              ; preds = %71
  call void @__gmpz_set_si(%1* %0, i64 0) #7
  br label %118

116:                                              ; preds = %71
  %117 = bitcast i64* %74 to i8*
  call void @__gmpz_import(%1* %0, i64 4, i32 -1, i64 8, i32 0, i64 8, i8* %117) #7
  br label %118

118:                                              ; preds = %115, %116
  %119 = load i64, i64* %73, align 8
  %120 = getelementptr inbounds [4 x i64], [4 x i64]* %11, i64 0, i64 1
  %121 = load i64, i64* %120, align 8
  %122 = or i64 %121, %119
  %123 = getelementptr inbounds [4 x i64], [4 x i64]* %11, i64 0, i64 2
  %124 = load i64, i64* %123, align 16
  %125 = or i64 %122, %124
  %126 = getelementptr inbounds [4 x i64], [4 x i64]* %11, i64 0, i64 3
  %127 = load i64, i64* %126, align 8
  %128 = or i64 %125, %127
  %129 = add nsw i64 %128, -1
  %130 = xor i64 %124, 72057594037927935
  %131 = insertelement <2 x i64> undef, i64 %119, i32 0
  %132 = shufflevector <2 x i64> %131, <2 x i64> undef, <2 x i32> zeroinitializer
  %133 = xor <2 x i64> %132, <i64 1, i64 2>
  %134 = insertelement <2 x i64> undef, i64 %121, i32 0
  %135 = shufflevector <2 x i64> %134, <2 x i64> undef, <2 x i32> zeroinitializer
  %136 = xor <2 x i64> %135, <i64 72056494526300160, i64 72055395014672384>
  %137 = or <2 x i64> %136, %133
  %138 = insertelement <2 x i64> undef, i64 %130, i32 0
  %139 = shufflevector <2 x i64> %138, <2 x i64> undef, <2 x i32> zeroinitializer
  %140 = or <2 x i64> %137, %139
  %141 = insertelement <2 x i64> undef, i64 %127, i32 0
  %142 = shufflevector <2 x i64> %141, <2 x i64> undef, <2 x i32> zeroinitializer
  %143 = xor <2 x i64> %142, <i64 72057594037927935, i64 144115188075855871>
  %144 = or <2 x i64> %140, %143
  %145 = add nsw <2 x i64> %144, <i64 -1, i64 -1>
  %146 = extractelement <2 x i64> %145, i32 0
  %147 = or i64 %146, %129
  %148 = extractelement <2 x i64> %145, i32 1
  %149 = or i64 %147, %148
  %150 = icmp sgt i64 %149, -1
  br i1 %150, label %152, label %151

151:                                              ; preds = %118
  call void @__gmpz_set_si(%1* %1, i64 0) #7
  br label %154

152:                                              ; preds = %118
  %153 = bitcast i64* %73 to i8*
  call void @__gmpz_import(%1* %1, i64 4, i32 -1, i64 8, i32 0, i64 8, i8* %153) #7
  br label %154

154:                                              ; preds = %151, %152
  %155 = load i64, i64* %72, align 8
  %156 = getelementptr inbounds [4 x i64], [4 x i64]* %12, i64 0, i64 1
  %157 = load i64, i64* %156, align 8
  %158 = or i64 %157, %155
  %159 = getelementptr inbounds [4 x i64], [4 x i64]* %12, i64 0, i64 2
  %160 = load i64, i64* %159, align 16
  %161 = or i64 %158, %160
  %162 = getelementptr inbounds [4 x i64], [4 x i64]* %12, i64 0, i64 3
  %163 = load i64, i64* %162, align 8
  %164 = or i64 %161, %163
  %165 = add nsw i64 %164, -1
  %166 = xor i64 %160, 72057594037927935
  %167 = insertelement <2 x i64> undef, i64 %155, i32 0
  %168 = shufflevector <2 x i64> %167, <2 x i64> undef, <2 x i32> zeroinitializer
  %169 = xor <2 x i64> %168, <i64 1, i64 2>
  %170 = insertelement <2 x i64> undef, i64 %157, i32 0
  %171 = shufflevector <2 x i64> %170, <2 x i64> undef, <2 x i32> zeroinitializer
  %172 = xor <2 x i64> %171, <i64 72056494526300160, i64 72055395014672384>
  %173 = or <2 x i64> %172, %169
  %174 = insertelement <2 x i64> undef, i64 %166, i32 0
  %175 = shufflevector <2 x i64> %174, <2 x i64> undef, <2 x i32> zeroinitializer
  %176 = or <2 x i64> %173, %175
  %177 = insertelement <2 x i64> undef, i64 %163, i32 0
  %178 = shufflevector <2 x i64> %177, <2 x i64> undef, <2 x i32> zeroinitializer
  %179 = xor <2 x i64> %178, <i64 72057594037927935, i64 144115188075855871>
  %180 = or <2 x i64> %176, %179
  %181 = add nsw <2 x i64> %180, <i64 -1, i64 -1>
  %182 = extractelement <2 x i64> %181, i32 0
  %183 = or i64 %182, %165
  %184 = extractelement <2 x i64> %181, i32 1
  %185 = or i64 %183, %184
  %186 = icmp sgt i64 %185, -1
  br i1 %186, label %188, label %187

187:                                              ; preds = %154
  call void @__gmpz_set_si(%1* %2, i64 0) #7
  br label %190

188:                                              ; preds = %154
  %189 = bitcast i64* %72 to i8*
  call void @__gmpz_import(%1* %2, i64 4, i32 -1, i64 8, i32 0, i64 8, i8* %189) #7
  br label %190

190:                                              ; preds = %187, %188
  call void @free(i8* %17) #7
  call void @free(i8* %76) #7
  call void @free(i8* %77) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %15) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %14) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %13) #7
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local noalias %3* @vec_jfmul_precomp_nistp224(%0* %0, %1* %1, %1* %2, %1* %3, i64 %4) local_unnamed_addr #0 {
  %6 = alloca [4 x i64], align 16
  %7 = alloca [4 x i64], align 16
  %8 = alloca [4 x i64], align 16
  %9 = bitcast [4 x i64]* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %9) #7
  %10 = bitcast [4 x i64]* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %10) #7
  %11 = bitcast [4 x i64]* %8 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %11) #7
  %12 = tail call noalias i8* @malloc(i64 64) #7
  %13 = bitcast i8* %12 to %5*
  %14 = bitcast i8* %12 to %3*
  %15 = getelementptr inbounds [4 x i64], [4 x i64]* %6, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %9, i8 0, i64 32, i1 false) #7
  %16 = call i8* @__gmpz_export(i8* nonnull %9, i64* null, i32 -1, i64 8, i32 0, i64 8, %1* %1) #7
  %17 = getelementptr inbounds [4 x i64], [4 x i64]* %7, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %10, i8 0, i64 32, i1 false) #7
  %18 = call i8* @__gmpz_export(i8* nonnull %10, i64* null, i32 -1, i64 8, i32 0, i64 8, %1* %2) #7
  %19 = getelementptr inbounds [4 x i64], [4 x i64]* %8, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %11, i8 0, i64 32, i1 false) #7
  %20 = call i8* @__gmpz_export(i8* nonnull %11, i64* null, i32 -1, i64 8, i32 0, i64 8, %1* %3) #7
  %21 = getelementptr inbounds %0, %0* %0, i64 0, i32 6, i64 0
  %22 = call i64 @__gmpz_sizeinbase(%1* nonnull %21, i32 2) #6
  %23 = trunc i64 %22 to i32
  %24 = trunc i64 %4 to i32
  %25 = call i32 @vec_fmul_block_width(i32 %23, i32 %24) #7
  %26 = bitcast i8* %12 to %4*
  %27 = sext i32 %25 to i64
  call void @vec_jsmul_init_nistp224_inner(%4* %26, %0* undef, i64 %27, i64 %27) #7
  %28 = add nsw i32 %25, -1
  %29 = add nsw i32 %28, %23
  %30 = sdiv i32 %29, %25
  %31 = sext i32 %30 to i64
  %32 = getelementptr inbounds i8, i8* %12, i64 56
  %33 = bitcast i8* %32 to i64*
  store i64 %31, i64* %33, align 8
  call void @vec_jfmul_prcmp_nistp224_inner(%0* undef, %5* %13, i64* nonnull %15, i64* nonnull %17, i64* nonnull %19)
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %11) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %10) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %9) #7
  ret %3* %14
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jfmul_nistp224(%1* %0, %1* %1, %1* %2, %0* nocapture readnone %3, %3* nocapture readonly %4, %1* %5) local_unnamed_addr #0 {
  %7 = alloca [4 x i64], align 16
  %8 = alloca [4 x i64], align 16
  %9 = alloca [4 x i64], align 16
  %10 = bitcast [4 x i64]* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %10) #7
  %11 = bitcast [4 x i64]* %8 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %11) #7
  %12 = bitcast [4 x i64]* %9 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %12) #7
  %13 = getelementptr inbounds [4 x i64], [4 x i64]* %7, i64 0, i64 0
  %14 = getelementptr inbounds [4 x i64], [4 x i64]* %8, i64 0, i64 0
  %15 = getelementptr inbounds [4 x i64], [4 x i64]* %9, i64 0, i64 0
  %16 = bitcast %3* %4 to %5*
  call void @vec_jfmul_cmp_nistp224_inner(i64* nonnull %13, i64* nonnull %14, i64* nonnull %15, %0* undef, %5* %16, %1* %5)
  %17 = load i64, i64* %13, align 16
  %18 = getelementptr inbounds [4 x i64], [4 x i64]* %7, i64 0, i64 1
  %19 = load i64, i64* %18, align 8
  %20 = or i64 %19, %17
  %21 = getelementptr inbounds [4 x i64], [4 x i64]* %7, i64 0, i64 2
  %22 = load i64, i64* %21, align 16
  %23 = or i64 %20, %22
  %24 = getelementptr inbounds [4 x i64], [4 x i64]* %7, i64 0, i64 3
  %25 = load i64, i64* %24, align 8
  %26 = or i64 %23, %25
  %27 = add nsw i64 %26, -1
  %28 = xor i64 %22, 72057594037927935
  %29 = insertelement <2 x i64> undef, i64 %17, i32 0
  %30 = shufflevector <2 x i64> %29, <2 x i64> undef, <2 x i32> zeroinitializer
  %31 = xor <2 x i64> %30, <i64 1, i64 2>
  %32 = insertelement <2 x i64> undef, i64 %19, i32 0
  %33 = shufflevector <2 x i64> %32, <2 x i64> undef, <2 x i32> zeroinitializer
  %34 = xor <2 x i64> %33, <i64 72056494526300160, i64 72055395014672384>
  %35 = or <2 x i64> %34, %31
  %36 = insertelement <2 x i64> undef, i64 %28, i32 0
  %37 = shufflevector <2 x i64> %36, <2 x i64> undef, <2 x i32> zeroinitializer
  %38 = or <2 x i64> %35, %37
  %39 = insertelement <2 x i64> undef, i64 %25, i32 0
  %40 = shufflevector <2 x i64> %39, <2 x i64> undef, <2 x i32> zeroinitializer
  %41 = xor <2 x i64> %40, <i64 72057594037927935, i64 144115188075855871>
  %42 = or <2 x i64> %38, %41
  %43 = add nsw <2 x i64> %42, <i64 -1, i64 -1>
  %44 = extractelement <2 x i64> %43, i32 0
  %45 = or i64 %44, %27
  %46 = extractelement <2 x i64> %43, i32 1
  %47 = or i64 %45, %46
  %48 = icmp sgt i64 %47, -1
  br i1 %48, label %50, label %49

49:                                               ; preds = %6
  tail call void @__gmpz_set_si(%1* %0, i64 0) #7
  br label %51

50:                                               ; preds = %6
  call void @__gmpz_import(%1* %0, i64 4, i32 -1, i64 8, i32 0, i64 8, i8* nonnull %10) #7
  br label %51

51:                                               ; preds = %49, %50
  %52 = load i64, i64* %14, align 16
  %53 = getelementptr inbounds [4 x i64], [4 x i64]* %8, i64 0, i64 1
  %54 = load i64, i64* %53, align 8
  %55 = or i64 %54, %52
  %56 = getelementptr inbounds [4 x i64], [4 x i64]* %8, i64 0, i64 2
  %57 = load i64, i64* %56, align 16
  %58 = or i64 %55, %57
  %59 = getelementptr inbounds [4 x i64], [4 x i64]* %8, i64 0, i64 3
  %60 = load i64, i64* %59, align 8
  %61 = or i64 %58, %60
  %62 = add nsw i64 %61, -1
  %63 = xor i64 %57, 72057594037927935
  %64 = insertelement <2 x i64> undef, i64 %52, i32 0
  %65 = shufflevector <2 x i64> %64, <2 x i64> undef, <2 x i32> zeroinitializer
  %66 = xor <2 x i64> %65, <i64 1, i64 2>
  %67 = insertelement <2 x i64> undef, i64 %54, i32 0
  %68 = shufflevector <2 x i64> %67, <2 x i64> undef, <2 x i32> zeroinitializer
  %69 = xor <2 x i64> %68, <i64 72056494526300160, i64 72055395014672384>
  %70 = or <2 x i64> %69, %66
  %71 = insertelement <2 x i64> undef, i64 %63, i32 0
  %72 = shufflevector <2 x i64> %71, <2 x i64> undef, <2 x i32> zeroinitializer
  %73 = or <2 x i64> %70, %72
  %74 = insertelement <2 x i64> undef, i64 %60, i32 0
  %75 = shufflevector <2 x i64> %74, <2 x i64> undef, <2 x i32> zeroinitializer
  %76 = xor <2 x i64> %75, <i64 72057594037927935, i64 144115188075855871>
  %77 = or <2 x i64> %73, %76
  %78 = add nsw <2 x i64> %77, <i64 -1, i64 -1>
  %79 = extractelement <2 x i64> %78, i32 0
  %80 = or i64 %79, %62
  %81 = extractelement <2 x i64> %78, i32 1
  %82 = or i64 %80, %81
  %83 = icmp sgt i64 %82, -1
  br i1 %83, label %85, label %84

84:                                               ; preds = %51
  call void @__gmpz_set_si(%1* %1, i64 0) #7
  br label %86

85:                                               ; preds = %51
  call void @__gmpz_import(%1* %1, i64 4, i32 -1, i64 8, i32 0, i64 8, i8* nonnull %11) #7
  br label %86

86:                                               ; preds = %84, %85
  %87 = load i64, i64* %15, align 16
  %88 = getelementptr inbounds [4 x i64], [4 x i64]* %9, i64 0, i64 1
  %89 = load i64, i64* %88, align 8
  %90 = or i64 %89, %87
  %91 = getelementptr inbounds [4 x i64], [4 x i64]* %9, i64 0, i64 2
  %92 = load i64, i64* %91, align 16
  %93 = or i64 %90, %92
  %94 = getelementptr inbounds [4 x i64], [4 x i64]* %9, i64 0, i64 3
  %95 = load i64, i64* %94, align 8
  %96 = or i64 %93, %95
  %97 = add nsw i64 %96, -1
  %98 = xor i64 %92, 72057594037927935
  %99 = insertelement <2 x i64> undef, i64 %87, i32 0
  %100 = shufflevector <2 x i64> %99, <2 x i64> undef, <2 x i32> zeroinitializer
  %101 = xor <2 x i64> %100, <i64 1, i64 2>
  %102 = insertelement <2 x i64> undef, i64 %89, i32 0
  %103 = shufflevector <2 x i64> %102, <2 x i64> undef, <2 x i32> zeroinitializer
  %104 = xor <2 x i64> %103, <i64 72056494526300160, i64 72055395014672384>
  %105 = or <2 x i64> %104, %101
  %106 = insertelement <2 x i64> undef, i64 %98, i32 0
  %107 = shufflevector <2 x i64> %106, <2 x i64> undef, <2 x i32> zeroinitializer
  %108 = or <2 x i64> %105, %107
  %109 = insertelement <2 x i64> undef, i64 %95, i32 0
  %110 = shufflevector <2 x i64> %109, <2 x i64> undef, <2 x i32> zeroinitializer
  %111 = xor <2 x i64> %110, <i64 72057594037927935, i64 144115188075855871>
  %112 = or <2 x i64> %108, %111
  %113 = add nsw <2 x i64> %112, <i64 -1, i64 -1>
  %114 = extractelement <2 x i64> %113, i32 0
  %115 = or i64 %114, %97
  %116 = extractelement <2 x i64> %113, i32 1
  %117 = or i64 %115, %116
  %118 = icmp sgt i64 %117, -1
  br i1 %118, label %120, label %119

119:                                              ; preds = %86
  call void @__gmpz_set_si(%1* %2, i64 0) #7
  br label %121

120:                                              ; preds = %86
  call void @__gmpz_import(%1* %2, i64 4, i32 -1, i64 8, i32 0, i64 8, i8* nonnull %12) #7
  br label %121

121:                                              ; preds = %119, %120
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %12) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %11) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %10) #7
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jfmul_free_nistp224(%3* nocapture %0) local_unnamed_addr #0 {
  %2 = bitcast %3* %0 to %4*
  %3 = tail call i32 @vec_jsmul_clear_nistp224_inner(%4* %2) #7
  %4 = bitcast %3* %0 to i8*
  tail call void @free(i8* %4) #7
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jdbl_nistp224(%2* nocapture readnone %0, %1* %1, %1* %2, %1* %3, %0* nocapture readnone %4, %1* %5, %1* %6, %1* %7) local_unnamed_addr #0 {
  %9 = alloca [4 x i64], align 16
  %10 = alloca [4 x i64], align 16
  %11 = alloca [4 x i64], align 16
  %12 = alloca [4 x i64], align 16
  %13 = alloca [4 x i64], align 16
  %14 = alloca [4 x i64], align 16
  %15 = bitcast [4 x i64]* %9 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %15) #7
  %16 = bitcast [4 x i64]* %10 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %16) #7
  %17 = bitcast [4 x i64]* %11 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %17) #7
  %18 = bitcast [4 x i64]* %12 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %18) #7
  %19 = bitcast [4 x i64]* %13 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %19) #7
  %20 = bitcast [4 x i64]* %14 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %20) #7
  %21 = getelementptr inbounds [4 x i64], [4 x i64]* %9, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %15, i8 0, i64 32, i1 false) #7
  %22 = call i8* @__gmpz_export(i8* nonnull %15, i64* null, i32 -1, i64 8, i32 0, i64 8, %1* %5) #7
  %23 = getelementptr inbounds [4 x i64], [4 x i64]* %10, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %16, i8 0, i64 32, i1 false) #7
  %24 = call i8* @__gmpz_export(i8* nonnull %16, i64* null, i32 -1, i64 8, i32 0, i64 8, %1* %6) #7
  %25 = getelementptr inbounds [4 x i64], [4 x i64]* %11, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %17, i8 0, i64 32, i1 false) #7
  %26 = call i8* @__gmpz_export(i8* nonnull %17, i64* null, i32 -1, i64 8, i32 0, i64 8, %1* %7) #7
  %27 = getelementptr inbounds [4 x i64], [4 x i64]* %12, i64 0, i64 0
  %28 = getelementptr inbounds [4 x i64], [4 x i64]* %13, i64 0, i64 0
  %29 = getelementptr inbounds [4 x i64], [4 x i64]* %14, i64 0, i64 0
  call fastcc void @0(i64* nonnull %27, i64* nonnull %28, i64* nonnull %29, i64* nonnull %21, i64* nonnull %23, i64* nonnull %25)
  %30 = load i64, i64* %27, align 16
  %31 = getelementptr inbounds [4 x i64], [4 x i64]* %12, i64 0, i64 1
  %32 = load i64, i64* %31, align 8
  %33 = or i64 %32, %30
  %34 = getelementptr inbounds [4 x i64], [4 x i64]* %12, i64 0, i64 2
  %35 = load i64, i64* %34, align 16
  %36 = or i64 %33, %35
  %37 = getelementptr inbounds [4 x i64], [4 x i64]* %12, i64 0, i64 3
  %38 = load i64, i64* %37, align 8
  %39 = or i64 %36, %38
  %40 = add nsw i64 %39, -1
  %41 = xor i64 %35, 72057594037927935
  %42 = insertelement <2 x i64> undef, i64 %30, i32 0
  %43 = shufflevector <2 x i64> %42, <2 x i64> undef, <2 x i32> zeroinitializer
  %44 = xor <2 x i64> %43, <i64 1, i64 2>
  %45 = insertelement <2 x i64> undef, i64 %32, i32 0
  %46 = shufflevector <2 x i64> %45, <2 x i64> undef, <2 x i32> zeroinitializer
  %47 = xor <2 x i64> %46, <i64 72056494526300160, i64 72055395014672384>
  %48 = or <2 x i64> %47, %44
  %49 = insertelement <2 x i64> undef, i64 %41, i32 0
  %50 = shufflevector <2 x i64> %49, <2 x i64> undef, <2 x i32> zeroinitializer
  %51 = or <2 x i64> %48, %50
  %52 = insertelement <2 x i64> undef, i64 %38, i32 0
  %53 = shufflevector <2 x i64> %52, <2 x i64> undef, <2 x i32> zeroinitializer
  %54 = xor <2 x i64> %53, <i64 72057594037927935, i64 144115188075855871>
  %55 = or <2 x i64> %51, %54
  %56 = add nsw <2 x i64> %55, <i64 -1, i64 -1>
  %57 = extractelement <2 x i64> %56, i32 0
  %58 = or i64 %57, %40
  %59 = extractelement <2 x i64> %56, i32 1
  %60 = or i64 %58, %59
  %61 = icmp sgt i64 %60, -1
  br i1 %61, label %63, label %62

62:                                               ; preds = %8
  call void @__gmpz_set_si(%1* %1, i64 0) #7
  br label %64

63:                                               ; preds = %8
  call void @__gmpz_import(%1* %1, i64 4, i32 -1, i64 8, i32 0, i64 8, i8* nonnull %18) #7
  br label %64

64:                                               ; preds = %62, %63
  %65 = load i64, i64* %28, align 16
  %66 = getelementptr inbounds [4 x i64], [4 x i64]* %13, i64 0, i64 1
  %67 = load i64, i64* %66, align 8
  %68 = or i64 %67, %65
  %69 = getelementptr inbounds [4 x i64], [4 x i64]* %13, i64 0, i64 2
  %70 = load i64, i64* %69, align 16
  %71 = or i64 %68, %70
  %72 = getelementptr inbounds [4 x i64], [4 x i64]* %13, i64 0, i64 3
  %73 = load i64, i64* %72, align 8
  %74 = or i64 %71, %73
  %75 = add nsw i64 %74, -1
  %76 = xor i64 %70, 72057594037927935
  %77 = insertelement <2 x i64> undef, i64 %65, i32 0
  %78 = shufflevector <2 x i64> %77, <2 x i64> undef, <2 x i32> zeroinitializer
  %79 = xor <2 x i64> %78, <i64 1, i64 2>
  %80 = insertelement <2 x i64> undef, i64 %67, i32 0
  %81 = shufflevector <2 x i64> %80, <2 x i64> undef, <2 x i32> zeroinitializer
  %82 = xor <2 x i64> %81, <i64 72056494526300160, i64 72055395014672384>
  %83 = or <2 x i64> %82, %79
  %84 = insertelement <2 x i64> undef, i64 %76, i32 0
  %85 = shufflevector <2 x i64> %84, <2 x i64> undef, <2 x i32> zeroinitializer
  %86 = or <2 x i64> %83, %85
  %87 = insertelement <2 x i64> undef, i64 %73, i32 0
  %88 = shufflevector <2 x i64> %87, <2 x i64> undef, <2 x i32> zeroinitializer
  %89 = xor <2 x i64> %88, <i64 72057594037927935, i64 144115188075855871>
  %90 = or <2 x i64> %86, %89
  %91 = add nsw <2 x i64> %90, <i64 -1, i64 -1>
  %92 = extractelement <2 x i64> %91, i32 0
  %93 = or i64 %92, %75
  %94 = extractelement <2 x i64> %91, i32 1
  %95 = or i64 %93, %94
  %96 = icmp sgt i64 %95, -1
  br i1 %96, label %98, label %97

97:                                               ; preds = %64
  call void @__gmpz_set_si(%1* %2, i64 0) #7
  br label %99

98:                                               ; preds = %64
  call void @__gmpz_import(%1* %2, i64 4, i32 -1, i64 8, i32 0, i64 8, i8* nonnull %19) #7
  br label %99

99:                                               ; preds = %97, %98
  %100 = load i64, i64* %29, align 16
  %101 = getelementptr inbounds [4 x i64], [4 x i64]* %14, i64 0, i64 1
  %102 = load i64, i64* %101, align 8
  %103 = or i64 %102, %100
  %104 = getelementptr inbounds [4 x i64], [4 x i64]* %14, i64 0, i64 2
  %105 = load i64, i64* %104, align 16
  %106 = or i64 %103, %105
  %107 = getelementptr inbounds [4 x i64], [4 x i64]* %14, i64 0, i64 3
  %108 = load i64, i64* %107, align 8
  %109 = or i64 %106, %108
  %110 = add nsw i64 %109, -1
  %111 = xor i64 %105, 72057594037927935
  %112 = insertelement <2 x i64> undef, i64 %100, i32 0
  %113 = shufflevector <2 x i64> %112, <2 x i64> undef, <2 x i32> zeroinitializer
  %114 = xor <2 x i64> %113, <i64 1, i64 2>
  %115 = insertelement <2 x i64> undef, i64 %102, i32 0
  %116 = shufflevector <2 x i64> %115, <2 x i64> undef, <2 x i32> zeroinitializer
  %117 = xor <2 x i64> %116, <i64 72056494526300160, i64 72055395014672384>
  %118 = or <2 x i64> %117, %114
  %119 = insertelement <2 x i64> undef, i64 %111, i32 0
  %120 = shufflevector <2 x i64> %119, <2 x i64> undef, <2 x i32> zeroinitializer
  %121 = or <2 x i64> %118, %120
  %122 = insertelement <2 x i64> undef, i64 %108, i32 0
  %123 = shufflevector <2 x i64> %122, <2 x i64> undef, <2 x i32> zeroinitializer
  %124 = xor <2 x i64> %123, <i64 72057594037927935, i64 144115188075855871>
  %125 = or <2 x i64> %121, %124
  %126 = add nsw <2 x i64> %125, <i64 -1, i64 -1>
  %127 = extractelement <2 x i64> %126, i32 0
  %128 = or i64 %127, %110
  %129 = extractelement <2 x i64> %126, i32 1
  %130 = or i64 %128, %129
  %131 = icmp sgt i64 %130, -1
  br i1 %131, label %133, label %132

132:                                              ; preds = %99
  call void @__gmpz_set_si(%1* %3, i64 0) #7
  br label %134

133:                                              ; preds = %99
  call void @__gmpz_import(%1* %3, i64 4, i32 -1, i64 8, i32 0, i64 8, i8* nonnull %20) #7
  br label %134

134:                                              ; preds = %132, %133
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %20) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %19) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %18) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %17) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %16) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %15) #7
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local void @vec_jadd_nistp224(%2* nocapture readnone %0, %1* %1, %1* %2, %1* %3, %0* nocapture readnone %4, %1* %5, %1* %6, %1* %7, %1* %8, %1* %9, %1* %10) local_unnamed_addr #0 {
  %12 = alloca [4 x i64], align 16
  %13 = alloca [4 x i64], align 16
  %14 = alloca [4 x i64], align 16
  %15 = alloca [4 x i64], align 16
  %16 = alloca [4 x i64], align 16
  %17 = alloca [4 x i64], align 16
  %18 = alloca [4 x i64], align 16
  %19 = alloca [4 x i64], align 16
  %20 = alloca [4 x i64], align 16
  %21 = bitcast [4 x i64]* %12 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %21) #7
  %22 = bitcast [4 x i64]* %13 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %22) #7
  %23 = bitcast [4 x i64]* %14 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %23) #7
  %24 = bitcast [4 x i64]* %15 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %24) #7
  %25 = bitcast [4 x i64]* %16 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %25) #7
  %26 = bitcast [4 x i64]* %17 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %26) #7
  %27 = bitcast [4 x i64]* %18 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %27) #7
  %28 = bitcast [4 x i64]* %19 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %28) #7
  %29 = bitcast [4 x i64]* %20 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %29) #7
  %30 = getelementptr inbounds [4 x i64], [4 x i64]* %12, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %21, i8 0, i64 32, i1 false) #7
  %31 = call i8* @__gmpz_export(i8* nonnull %21, i64* null, i32 -1, i64 8, i32 0, i64 8, %1* %5) #7
  %32 = getelementptr inbounds [4 x i64], [4 x i64]* %13, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %22, i8 0, i64 32, i1 false) #7
  %33 = call i8* @__gmpz_export(i8* nonnull %22, i64* null, i32 -1, i64 8, i32 0, i64 8, %1* %6) #7
  %34 = getelementptr inbounds [4 x i64], [4 x i64]* %14, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %23, i8 0, i64 32, i1 false) #7
  %35 = call i8* @__gmpz_export(i8* nonnull %23, i64* null, i32 -1, i64 8, i32 0, i64 8, %1* %7) #7
  %36 = getelementptr inbounds [4 x i64], [4 x i64]* %15, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %24, i8 0, i64 32, i1 false) #7
  %37 = call i8* @__gmpz_export(i8* nonnull %24, i64* null, i32 -1, i64 8, i32 0, i64 8, %1* %8) #7
  %38 = getelementptr inbounds [4 x i64], [4 x i64]* %16, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %25, i8 0, i64 32, i1 false) #7
  %39 = call i8* @__gmpz_export(i8* nonnull %25, i64* null, i32 -1, i64 8, i32 0, i64 8, %1* %9) #7
  %40 = getelementptr inbounds [4 x i64], [4 x i64]* %17, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %26, i8 0, i64 32, i1 false) #7
  %41 = call i8* @__gmpz_export(i8* nonnull %26, i64* null, i32 -1, i64 8, i32 0, i64 8, %1* %10) #7
  %42 = getelementptr inbounds [4 x i64], [4 x i64]* %18, i64 0, i64 0
  %43 = getelementptr inbounds [4 x i64], [4 x i64]* %19, i64 0, i64 0
  %44 = getelementptr inbounds [4 x i64], [4 x i64]* %20, i64 0, i64 0
  call fastcc void @1(i64* nonnull %42, i64* nonnull %43, i64* nonnull %44, i64* nonnull %30, i64* nonnull %32, i64* nonnull %34, i64* nonnull %36, i64* nonnull %38, i64* nonnull %40)
  %45 = load i64, i64* %42, align 16
  %46 = getelementptr inbounds [4 x i64], [4 x i64]* %18, i64 0, i64 1
  %47 = load i64, i64* %46, align 8
  %48 = or i64 %47, %45
  %49 = getelementptr inbounds [4 x i64], [4 x i64]* %18, i64 0, i64 2
  %50 = load i64, i64* %49, align 16
  %51 = or i64 %48, %50
  %52 = getelementptr inbounds [4 x i64], [4 x i64]* %18, i64 0, i64 3
  %53 = load i64, i64* %52, align 8
  %54 = or i64 %51, %53
  %55 = add nsw i64 %54, -1
  %56 = xor i64 %50, 72057594037927935
  %57 = insertelement <2 x i64> undef, i64 %45, i32 0
  %58 = shufflevector <2 x i64> %57, <2 x i64> undef, <2 x i32> zeroinitializer
  %59 = xor <2 x i64> %58, <i64 1, i64 2>
  %60 = insertelement <2 x i64> undef, i64 %47, i32 0
  %61 = shufflevector <2 x i64> %60, <2 x i64> undef, <2 x i32> zeroinitializer
  %62 = xor <2 x i64> %61, <i64 72056494526300160, i64 72055395014672384>
  %63 = or <2 x i64> %62, %59
  %64 = insertelement <2 x i64> undef, i64 %56, i32 0
  %65 = shufflevector <2 x i64> %64, <2 x i64> undef, <2 x i32> zeroinitializer
  %66 = or <2 x i64> %63, %65
  %67 = insertelement <2 x i64> undef, i64 %53, i32 0
  %68 = shufflevector <2 x i64> %67, <2 x i64> undef, <2 x i32> zeroinitializer
  %69 = xor <2 x i64> %68, <i64 72057594037927935, i64 144115188075855871>
  %70 = or <2 x i64> %66, %69
  %71 = add nsw <2 x i64> %70, <i64 -1, i64 -1>
  %72 = extractelement <2 x i64> %71, i32 0
  %73 = or i64 %72, %55
  %74 = extractelement <2 x i64> %71, i32 1
  %75 = or i64 %73, %74
  %76 = icmp sgt i64 %75, -1
  br i1 %76, label %78, label %77

77:                                               ; preds = %11
  call void @__gmpz_set_si(%1* %1, i64 0) #7
  br label %79

78:                                               ; preds = %11
  call void @__gmpz_import(%1* %1, i64 4, i32 -1, i64 8, i32 0, i64 8, i8* nonnull %27) #7
  br label %79

79:                                               ; preds = %77, %78
  %80 = load i64, i64* %43, align 16
  %81 = getelementptr inbounds [4 x i64], [4 x i64]* %19, i64 0, i64 1
  %82 = load i64, i64* %81, align 8
  %83 = or i64 %82, %80
  %84 = getelementptr inbounds [4 x i64], [4 x i64]* %19, i64 0, i64 2
  %85 = load i64, i64* %84, align 16
  %86 = or i64 %83, %85
  %87 = getelementptr inbounds [4 x i64], [4 x i64]* %19, i64 0, i64 3
  %88 = load i64, i64* %87, align 8
  %89 = or i64 %86, %88
  %90 = add nsw i64 %89, -1
  %91 = xor i64 %85, 72057594037927935
  %92 = insertelement <2 x i64> undef, i64 %80, i32 0
  %93 = shufflevector <2 x i64> %92, <2 x i64> undef, <2 x i32> zeroinitializer
  %94 = xor <2 x i64> %93, <i64 1, i64 2>
  %95 = insertelement <2 x i64> undef, i64 %82, i32 0
  %96 = shufflevector <2 x i64> %95, <2 x i64> undef, <2 x i32> zeroinitializer
  %97 = xor <2 x i64> %96, <i64 72056494526300160, i64 72055395014672384>
  %98 = or <2 x i64> %97, %94
  %99 = insertelement <2 x i64> undef, i64 %91, i32 0
  %100 = shufflevector <2 x i64> %99, <2 x i64> undef, <2 x i32> zeroinitializer
  %101 = or <2 x i64> %98, %100
  %102 = insertelement <2 x i64> undef, i64 %88, i32 0
  %103 = shufflevector <2 x i64> %102, <2 x i64> undef, <2 x i32> zeroinitializer
  %104 = xor <2 x i64> %103, <i64 72057594037927935, i64 144115188075855871>
  %105 = or <2 x i64> %101, %104
  %106 = add nsw <2 x i64> %105, <i64 -1, i64 -1>
  %107 = extractelement <2 x i64> %106, i32 0
  %108 = or i64 %107, %90
  %109 = extractelement <2 x i64> %106, i32 1
  %110 = or i64 %108, %109
  %111 = icmp sgt i64 %110, -1
  br i1 %111, label %113, label %112

112:                                              ; preds = %79
  call void @__gmpz_set_si(%1* %2, i64 0) #7
  br label %114

113:                                              ; preds = %79
  call void @__gmpz_import(%1* %2, i64 4, i32 -1, i64 8, i32 0, i64 8, i8* nonnull %28) #7
  br label %114

114:                                              ; preds = %112, %113
  %115 = load i64, i64* %44, align 16
  %116 = getelementptr inbounds [4 x i64], [4 x i64]* %20, i64 0, i64 1
  %117 = load i64, i64* %116, align 8
  %118 = or i64 %117, %115
  %119 = getelementptr inbounds [4 x i64], [4 x i64]* %20, i64 0, i64 2
  %120 = load i64, i64* %119, align 16
  %121 = or i64 %118, %120
  %122 = getelementptr inbounds [4 x i64], [4 x i64]* %20, i64 0, i64 3
  %123 = load i64, i64* %122, align 8
  %124 = or i64 %121, %123
  %125 = add nsw i64 %124, -1
  %126 = xor i64 %120, 72057594037927935
  %127 = insertelement <2 x i64> undef, i64 %115, i32 0
  %128 = shufflevector <2 x i64> %127, <2 x i64> undef, <2 x i32> zeroinitializer
  %129 = xor <2 x i64> %128, <i64 1, i64 2>
  %130 = insertelement <2 x i64> undef, i64 %117, i32 0
  %131 = shufflevector <2 x i64> %130, <2 x i64> undef, <2 x i32> zeroinitializer
  %132 = xor <2 x i64> %131, <i64 72056494526300160, i64 72055395014672384>
  %133 = or <2 x i64> %132, %129
  %134 = insertelement <2 x i64> undef, i64 %126, i32 0
  %135 = shufflevector <2 x i64> %134, <2 x i64> undef, <2 x i32> zeroinitializer
  %136 = or <2 x i64> %133, %135
  %137 = insertelement <2 x i64> undef, i64 %123, i32 0
  %138 = shufflevector <2 x i64> %137, <2 x i64> undef, <2 x i32> zeroinitializer
  %139 = xor <2 x i64> %138, <i64 72057594037927935, i64 144115188075855871>
  %140 = or <2 x i64> %136, %139
  %141 = add nsw <2 x i64> %140, <i64 -1, i64 -1>
  %142 = extractelement <2 x i64> %141, i32 0
  %143 = or i64 %142, %125
  %144 = extractelement <2 x i64> %141, i32 1
  %145 = or i64 %143, %144
  %146 = icmp sgt i64 %145, -1
  br i1 %146, label %148, label %147

147:                                              ; preds = %114
  call void @__gmpz_set_si(%1* %3, i64 0) #7
  br label %149

148:                                              ; preds = %114
  call void @__gmpz_import(%1* %3, i64 4, i32 -1, i64 8, i32 0, i64 8, i8* nonnull %29) #7
  br label %149

149:                                              ; preds = %147, %148
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %29) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %28) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %27) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %26) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %25) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %24) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %23) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %22) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %21) #7
  ret void
}

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @time_jdbl_nistp224(i32 %0, %1* %1, %1* %2) local_unnamed_addr #0 {
  %4 = alloca [4 x i64], align 16
  %5 = alloca [4 x i64], align 16
  %6 = alloca [4 x i64], align 16
  %7 = alloca [1 x %1], align 16
  %8 = bitcast [4 x i64]* %4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %8) #7
  %9 = bitcast [4 x i64]* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %9) #7
  %10 = bitcast [4 x i64]* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %10) #7
  %11 = bitcast [1 x %1]* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 16, i8* nonnull %11) #7
  %12 = getelementptr inbounds [4 x i64], [4 x i64]* %4, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %8, i8 0, i64 32, i1 false) #7
  %13 = call i8* @__gmpz_export(i8* nonnull %8, i64* null, i32 -1, i64 8, i32 0, i64 8, %1* %1) #7
  %14 = getelementptr inbounds [4 x i64], [4 x i64]* %5, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %9, i8 0, i64 32, i1 false) #7
  %15 = call i8* @__gmpz_export(i8* nonnull %9, i64* null, i32 -1, i64 8, i32 0, i64 8, %1* %2) #7
  %16 = getelementptr inbounds [1 x %1], [1 x %1]* %7, i64 0, i64 0
  call void @__gmpz_init(%1* nonnull %16) #7
  call void @__gmpz_set_ui(%1* nonnull %16, i64 1) #7
  %17 = getelementptr inbounds [4 x i64], [4 x i64]* %6, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %10, i8 0, i64 32, i1 false) #7
  %18 = call i8* @__gmpz_export(i8* nonnull %10, i64* null, i32 -1, i64 8, i32 0, i64 8, %1* nonnull %16) #7
  call void @__gmpz_clear(%1* nonnull %16) #7
  %19 = call i64 @clock() #7
  %20 = shl i64 %19, 32
  %21 = ashr exact i64 %20, 32
  %22 = sext i32 %0 to i64
  br label %23

23:                                               ; preds = %23, %3
  %24 = phi i64 [ 0, %3 ], [ %25, %23 ]
  call fastcc void @0(i64* nonnull %12, i64* nonnull %14, i64* nonnull %17, i64* nonnull %12, i64* nonnull %14, i64* nonnull %17)
  %25 = add nuw nsw i64 %24, 1
  %26 = call i32 @vec_done(i64 %21, i64 %22) #7
  %27 = icmp eq i32 %26, 0
  br i1 %27, label %23, label %28

28:                                               ; preds = %23
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %11) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %10) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %9) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %8) #7
  ret i64 %25
}

declare dso_local void @__gmpz_set_ui(%1*, i64) local_unnamed_addr #5

; Function Attrs: nounwind
declare dso_local i64 @clock() local_unnamed_addr #4

declare dso_local i32 @vec_done(i64, i64) local_unnamed_addr #5

; Function Attrs: nounwind sspstrong uwtable
define dso_local i64 @time_jadd_nistp224(i32 %0, %1* %1, %1* %2) local_unnamed_addr #0 {
  %4 = alloca [4 x i64], align 16
  %5 = alloca [4 x i64], align 16
  %6 = alloca [4 x i64], align 16
  %7 = alloca [4 x i64], align 16
  %8 = alloca [4 x i64], align 16
  %9 = alloca [4 x i64], align 16
  %10 = alloca [1 x %1], align 16
  %11 = bitcast [4 x i64]* %4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %11) #7
  %12 = bitcast [4 x i64]* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %12) #7
  %13 = bitcast [4 x i64]* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %13) #7
  %14 = bitcast [4 x i64]* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %14) #7
  %15 = bitcast [4 x i64]* %8 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %15) #7
  %16 = bitcast [4 x i64]* %9 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %16) #7
  %17 = bitcast [1 x %1]* %10 to i8*
  call void @llvm.lifetime.start.p0i8(i64 16, i8* nonnull %17) #7
  %18 = getelementptr inbounds [4 x i64], [4 x i64]* %7, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %14, i8 0, i64 32, i1 false) #7
  %19 = call i8* @__gmpz_export(i8* nonnull %14, i64* null, i32 -1, i64 8, i32 0, i64 8, %1* %1) #7
  %20 = getelementptr inbounds [4 x i64], [4 x i64]* %8, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %15, i8 0, i64 32, i1 false) #7
  %21 = call i8* @__gmpz_export(i8* nonnull %15, i64* null, i32 -1, i64 8, i32 0, i64 8, %1* %2) #7
  %22 = getelementptr inbounds [1 x %1], [1 x %1]* %10, i64 0, i64 0
  call void @__gmpz_init(%1* nonnull %22) #7
  call void @__gmpz_set_ui(%1* nonnull %22, i64 1) #7
  %23 = getelementptr inbounds [4 x i64], [4 x i64]* %9, i64 0, i64 0
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %16, i8 0, i64 32, i1 false) #7
  %24 = call i8* @__gmpz_export(i8* nonnull %16, i64* null, i32 -1, i64 8, i32 0, i64 8, %1* nonnull %22) #7
  call void @__gmpz_clear(%1* nonnull %22) #7
  %25 = getelementptr inbounds [4 x i64], [4 x i64]* %4, i64 0, i64 0
  %26 = getelementptr inbounds [4 x i64], [4 x i64]* %5, i64 0, i64 0
  %27 = getelementptr inbounds [4 x i64], [4 x i64]* %6, i64 0, i64 0
  call fastcc void @0(i64* nonnull %25, i64* nonnull %26, i64* nonnull %27, i64* nonnull %18, i64* nonnull %20, i64* nonnull %23)
  %28 = call i64 @clock() #7
  %29 = shl i64 %28, 32
  %30 = ashr exact i64 %29, 32
  %31 = sext i32 %0 to i64
  br label %32

32:                                               ; preds = %32, %3
  %33 = phi i64 [ 0, %3 ], [ %34, %32 ]
  call fastcc void @1(i64* nonnull %25, i64* nonnull %26, i64* nonnull %27, i64* nonnull %25, i64* nonnull %26, i64* nonnull %27, i64* nonnull %18, i64* nonnull %20, i64* nonnull %23)
  %34 = add nuw nsw i64 %33, 1
  %35 = call i32 @vec_done(i64 %30, i64 %31) #7
  %36 = icmp eq i32 %35, 0
  br i1 %36, label %32, label %37

37:                                               ; preds = %32
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %17) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %16) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %15) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %14) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %13) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %12) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %11) #7
  ret i64 %34
}

declare dso_local i8* @__gmpz_export(i8*, i64*, i32, i64, i32, i64, %1*) local_unnamed_addr #5

declare dso_local void @__gmpz_set_si(%1*, i64) local_unnamed_addr #5

declare dso_local void @__gmpz_import(%1*, i64, i32, i64, i32, i64, i8*) local_unnamed_addr #5

attributes #0 = { nounwind sspstrong uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "frame-pointer"="none" "less-precise-fpmad"="false" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+fxsr,+mmx,+sse,+sse2,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { argmemonly nounwind willreturn }
attributes #2 = { argmemonly nounwind willreturn writeonly }
attributes #3 = { nounwind readonly "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "frame-pointer"="none" "less-precise-fpmad"="false" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+fxsr,+mmx,+sse,+sse2,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #4 = { nounwind "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "frame-pointer"="none" "less-precise-fpmad"="false" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+fxsr,+mmx,+sse,+sse2,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #5 = { "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "frame-pointer"="none" "less-precise-fpmad"="false" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+fxsr,+mmx,+sse,+sse2,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #6 = { nounwind readonly }
attributes #7 = { nounwind }

!llvm.module.flags = !{!0, !1, !2}
!llvm.ident = !{!3}

!0 = !{i32 2, !"Dwarf Version", i32 4}
!1 = !{i32 2, !"Debug Info Version", i32 3}
!2 = !{i32 1, !"wchar_size", i32 4}
!3 = !{!"clang version 7.0.0 (tags/RELEASE_700/final)"}
